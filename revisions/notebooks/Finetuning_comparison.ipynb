{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef23a78",
   "metadata": {},
   "source": [
    "# Notebook for exploring the heatmap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29930456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "610265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../'\n",
    "paths = {\n",
    "# standard resutls (internal, external)\n",
    "'p1':'results/evaluation_test/plots/',\n",
    "# model pooling / ensembling:\n",
    "'p2': 'results/evaluation_test/prediction_pooled_subsampled/max/plots/',\n",
    "# finetuned models on 10%:\n",
    "'p3': 'results/finetuning/evaluation_test/plots/',\n",
    "# baseline (of finetuning) on 10%:\n",
    "'p4': 'results/finetuning/evaluation_test/baseline/plots/',\n",
    "# finetuned models on 10%:\n",
    "'p5': 'results/finetuning_0_20/evaluation_test/plots/',\n",
    "# baseline (of finetuning) on 10%:\n",
    "'p6': 'results/finetuning_0_20/evaluation_test/baseline/plots/',\n",
    "# finetuning 10% + pooled preds\n",
    "'p7': 'results/finetuning/evaluation_test/prediction_pooled_subsampled/max/plots',\n",
    "}\n",
    "paths = {key: os.path.join(base_dir, val) for key, val in paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07ba4d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p1': '../../results/evaluation_test/plots/',\n",
       " 'p2': '../../results/evaluation_test/prediction_pooled_subsampled/max/plots/',\n",
       " 'p3': '../../results/finetuning/evaluation_test/plots/',\n",
       " 'p4': '../../results/finetuning/evaluation_test/baseline/plots/',\n",
       " 'p5': '../../results/finetuning_0_20/evaluation_test/plots/',\n",
       " 'p6': '../../results/finetuning_0_20/evaluation_test/baseline/plots/',\n",
       " 'p7': '../../results/finetuning/evaluation_test/prediction_pooled_subsampled/max/plots'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6e4d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_paths = {key: os.path.join(path, 'roc_summary_subsampled.csv') for key, path in paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f80a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_paths = {key: os.path.join(path, 'scatter_agg_data.csv') for key, path in paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec992953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../results/finetuning_0_20/evaluation_test/plots/scatter_agg_data.csv\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatter_raw_data.csv\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_aumc_eicu_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_aumc_hirid_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_aumc_mimic_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_eicu_aumc_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_eicu_hirid_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_eicu_mimic_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_hirid_aumc_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_hirid_eicu_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_hirid_mimic_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_mimic_aumc_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_mimic_eicu_pat_thres_80_subsampled.png\r\n",
      "../../results/finetuning_0_20/evaluation_test/plots/scatterplot_mimic_hirid_pat_thres_80_subsampled.png\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../results/finetuning_0_20/evaluation_test/plots/scatter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1cc993a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p1': '../../results/evaluation_test/plots/',\n",
       " 'p2': '../../results/evaluation_test/prediction_pooled_subsampled/max/plots/',\n",
       " 'p3': '../../results/finetuning/evaluation_test/plots/',\n",
       " 'p4': '../../results/finetuning/evaluation_test/baseline/plots/',\n",
       " 'p5': '../../results/finetuning_0_20/evaluation_test/plots/',\n",
       " 'p6': '../../results/finetuning_0_20/evaluation_test/baseline/plots/',\n",
       " 'p7': '../../results/finetuning/evaluation_test/prediction_pooled_subsampled/max/plots'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "364fe85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {key: pd.read_csv(val) for key,val in roc_paths.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9419aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {key: df.drop(columns='Unnamed: 0') for key, df in dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0be8b62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object'),\n",
       " Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object'),\n",
       " Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object'),\n",
       " Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object'),\n",
       " Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object'),\n",
       " Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object'),\n",
       " Index(['model', 'train_dataset', 'eval_dataset', 'auc_mean', 'auc_std'], dtype='object')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df.columns for k, df in dfs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcc436a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['p3']['finetuned'] = True\n",
    "dfs['p1']['finetuned'] = False\n",
    "dfs['p2']['finetuned'] = False\n",
    "dfs['p4']['finetuned'] = False\n",
    "dfs['p5']['finetuned'] = True\n",
    "dfs['p6']['finetuned'] = False\n",
    "dfs['p7']['finetuned'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6f5e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator if it's the finetuning baseline:\n",
    "dfs['p3']['baseline'] = False\n",
    "dfs['p1']['baseline'] = False\n",
    "dfs['p2']['baseline'] = False\n",
    "dfs['p4']['baseline'] = True\n",
    "dfs['p5']['baseline'] = False\n",
    "dfs['p6']['baseline'] = True\n",
    "dfs['p7']['baseline'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51186bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning size: (training size for baseline)\n",
    "dfs['p3']['finetuning_size'] = 0.10\n",
    "dfs['p1']['finetuning_size'] = np.nan\n",
    "dfs['p2']['finetuning_size'] = np.nan\n",
    "dfs['p4']['finetuning_size'] = 0.10\n",
    "dfs['p5']['finetuning_size'] = 0.02\n",
    "dfs['p6']['finetuning_size'] = 0.02\n",
    "dfs['p7']['finetuning_size'] = 0.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f2e6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning size: (training size for baseline)\n",
    "dfs['p3']['pooled'] = False\n",
    "dfs['p1']['pooled'] = False\n",
    "dfs['p2']['pooled'] = True\n",
    "dfs['p4']['pooled'] = False\n",
    "dfs['p5']['pooled'] = False\n",
    "dfs['p6']['pooled'] = False\n",
    "dfs['p7']['pooled'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8edc4344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.917711</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRUModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.856986</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.893829</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lr</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.883245</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mews</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.718061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>mews</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.609294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>news</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.652844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>qsofa</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.565961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>sirs</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.609423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>sofa</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.693076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model train_dataset eval_dataset  auc_mean   auc_std  finetuned  \\\n",
       "0    AttentionModel          aumc         aumc  0.917711  0.003569      False   \n",
       "1          GRUModel          aumc         aumc  0.856986  0.014407      False   \n",
       "2              lgbm          aumc         aumc  0.893829  0.004515      False   \n",
       "3                lr          aumc         aumc  0.883245  0.001715      False   \n",
       "4              mews          aumc         aumc  0.718061  0.000000      False   \n",
       "..              ...           ...          ...       ...       ...        ...   \n",
       "175            mews         mimic        mimic  0.609294  0.000000      False   \n",
       "176            news         mimic        mimic  0.652844  0.000000      False   \n",
       "177           qsofa         mimic        mimic  0.565961  0.000000      False   \n",
       "178            sirs         mimic        mimic  0.609423  0.000000      False   \n",
       "179            sofa         mimic        mimic  0.693076  0.000000      False   \n",
       "\n",
       "     baseline  finetuning_size  pooled  \n",
       "0       False              NaN   False  \n",
       "1       False              NaN   False  \n",
       "2       False              NaN   False  \n",
       "3       False              NaN   False  \n",
       "4       False              NaN   False  \n",
       "..        ...              ...     ...  \n",
       "175     False              NaN   False  \n",
       "176     False              NaN   False  \n",
       "177     False              NaN   False  \n",
       "178     False              NaN   False  \n",
       "179     False              NaN   False  \n",
       "\n",
       "[180 rows x 9 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49cecbe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdfs\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdfs' is not defined"
     ]
    }
   ],
   "source": [
    "sdfs['p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b0f7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53f48b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p1': '../../results/evaluation_test/plots/scatter_agg_data.csv',\n",
       " 'p2': '../../results/evaluation_test/prediction_pooled_subsampled/max/plots/scatter_agg_data.csv',\n",
       " 'p3': '../../results/finetuning/evaluation_test/plots/scatter_agg_data.csv',\n",
       " 'p4': '../../results/finetuning/evaluation_test/baseline/plots/scatter_agg_data.csv',\n",
       " 'p5': '../../results/finetuning_0_20/evaluation_test/plots/scatter_agg_data.csv',\n",
       " 'p6': '../../results/finetuning_0_20/evaluation_test/baseline/plots/scatter_agg_data.csv',\n",
       " 'p7': '../../results/finetuning/evaluation_test/prediction_pooled_subsampled/max/plots/scatter_agg_data.csv'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scatter_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4992cf",
   "metadata": {},
   "source": [
    "## load scatter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87aaa016",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfs = {key: pd.read_csv(val) for key,val in scatter_paths.items()} #scatter dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a83f8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to attn model:\n",
    "keys = ['p1', 'p2']\n",
    "for key in keys:\n",
    "    dfs[key] = dfs[key].query(\"model == 'AttentionModel'\")\n",
    "    sdfs[key] = sdfs[key].query(\"model.str.contains('attn')\") #different versions of attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff1874e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs = {}\n",
    "for k in dfs.keys():\n",
    "    full_dfs[k] = pd.merge(dfs[k], sdfs[k], on=['train_dataset', 'eval_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b4c1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in full_dfs.keys():\n",
    "    full_dfs[k] = full_dfs[k].rename(columns=\n",
    "        {'x_mean': 'earliness_mean', \n",
    "         'x_std': 'earliness_std', \n",
    "         'y_mean': 'precision_mean', \n",
    "         'y_std': 'precision_std' }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6859772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(full_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c288433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AttentionModel'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['model_x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68914967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at the attn model:\n",
    "df = df.query(\"model_x == 'AttentionModel'\")\n",
    "# drop obsolete dataset (too heterogenous / low quality)\n",
    "df = df.query(\"train_dataset != 'emory' & eval_dataset != 'emory'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1de4b861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">p1</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.917711</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.055</td>\n",
       "      <td>0.079844</td>\n",
       "      <td>0.531098</td>\n",
       "      <td>0.022103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.698006</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.755</td>\n",
       "      <td>1.246044</td>\n",
       "      <td>0.244710</td>\n",
       "      <td>0.007552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.220</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>0.270946</td>\n",
       "      <td>0.012086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.235</td>\n",
       "      <td>1.187908</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0.008421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.455</td>\n",
       "      <td>1.525451</td>\n",
       "      <td>0.254022</td>\n",
       "      <td>0.010373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.802502</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.625</td>\n",
       "      <td>1.212693</td>\n",
       "      <td>0.321037</td>\n",
       "      <td>0.010810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.705312</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.680</td>\n",
       "      <td>1.153581</td>\n",
       "      <td>0.252045</td>\n",
       "      <td>0.018139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.115</td>\n",
       "      <td>0.273633</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.806525</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.613239</td>\n",
       "      <td>0.327414</td>\n",
       "      <td>0.035004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.705568</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.918286</td>\n",
       "      <td>0.241868</td>\n",
       "      <td>0.008305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.834322</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.770</td>\n",
       "      <td>0.130384</td>\n",
       "      <td>0.363912</td>\n",
       "      <td>0.008250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.090</td>\n",
       "      <td>0.915253</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.798606</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.825</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>0.049013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.753329</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.590</td>\n",
       "      <td>1.004459</td>\n",
       "      <td>0.283009</td>\n",
       "      <td>0.010858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.694860</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.942802</td>\n",
       "      <td>0.237810</td>\n",
       "      <td>0.009988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.831972</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.236907</td>\n",
       "      <td>0.356650</td>\n",
       "      <td>0.006018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p2</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.835573</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.844911</td>\n",
       "      <td>0.368059</td>\n",
       "      <td>0.029036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.470</td>\n",
       "      <td>1.178638</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.011018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729156</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.282332</td>\n",
       "      <td>0.265123</td>\n",
       "      <td>0.010473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.720870</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.449514</td>\n",
       "      <td>0.252695</td>\n",
       "      <td>0.004614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p3</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1.219375</td>\n",
       "      <td>0.283055</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.792255</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.085</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.873594</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.850</td>\n",
       "      <td>1.014889</td>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.040464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.293611</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.495</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.303076</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.867947</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.140</td>\n",
       "      <td>1.011589</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.035182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.894071</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.546580</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.020819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p4</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.829317</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.334944</td>\n",
       "      <td>0.399529</td>\n",
       "      <td>0.017069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.729738</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.320</td>\n",
       "      <td>0.499187</td>\n",
       "      <td>0.256771</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.787233</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.135</td>\n",
       "      <td>0.266693</td>\n",
       "      <td>0.302023</td>\n",
       "      <td>0.003675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.782670</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.825</td>\n",
       "      <td>0.298957</td>\n",
       "      <td>0.295055</td>\n",
       "      <td>0.004809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p5</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.724866</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.649134</td>\n",
       "      <td>0.252831</td>\n",
       "      <td>0.003548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774966</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.220</td>\n",
       "      <td>0.566348</td>\n",
       "      <td>0.301949</td>\n",
       "      <td>0.005359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.645</td>\n",
       "      <td>1.332807</td>\n",
       "      <td>0.234105</td>\n",
       "      <td>0.015405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.041005</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.195</td>\n",
       "      <td>1.059835</td>\n",
       "      <td>0.303952</td>\n",
       "      <td>0.071046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729016</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.806730</td>\n",
       "      <td>0.264214</td>\n",
       "      <td>0.006479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.724120</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.915</td>\n",
       "      <td>0.277038</td>\n",
       "      <td>0.246401</td>\n",
       "      <td>0.009146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.844473</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.175</td>\n",
       "      <td>0.653357</td>\n",
       "      <td>0.380352</td>\n",
       "      <td>0.021274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.742710</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.690</td>\n",
       "      <td>1.025549</td>\n",
       "      <td>0.263921</td>\n",
       "      <td>0.008179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.737982</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.337722</td>\n",
       "      <td>0.255292</td>\n",
       "      <td>0.008648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.843931</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.865</td>\n",
       "      <td>1.803781</td>\n",
       "      <td>0.386290</td>\n",
       "      <td>0.037804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.780288</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.125</td>\n",
       "      <td>0.257391</td>\n",
       "      <td>0.294014</td>\n",
       "      <td>0.004669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.782912</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.366742</td>\n",
       "      <td>0.305844</td>\n",
       "      <td>0.004670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p6</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.807556</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.575</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.327786</td>\n",
       "      <td>0.011386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.711287</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.350</td>\n",
       "      <td>0.385276</td>\n",
       "      <td>0.255480</td>\n",
       "      <td>0.005198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.763304</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.455</td>\n",
       "      <td>0.156525</td>\n",
       "      <td>0.287224</td>\n",
       "      <td>0.003679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.745647</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.305</td>\n",
       "      <td>0.207214</td>\n",
       "      <td>0.267340</td>\n",
       "      <td>0.003602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p7</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.883985</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.470225</td>\n",
       "      <td>0.479475</td>\n",
       "      <td>0.019430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.687750</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.004886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.779573</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.840</td>\n",
       "      <td>0.370220</td>\n",
       "      <td>0.303068</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.450</td>\n",
       "      <td>0.244310</td>\n",
       "      <td>0.309495</td>\n",
       "      <td>0.006718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p1 0   AttentionModel          aumc         aumc  0.917711  0.003569   \n",
       "   1   AttentionModel          aumc         eicu  0.698006  0.010686   \n",
       "   3   AttentionModel          aumc        hirid  0.733108  0.013438   \n",
       "   4   AttentionModel          aumc        mimic  0.685606  0.013133   \n",
       "   5   AttentionModel          eicu         aumc  0.731821  0.007108   \n",
       "   6   AttentionModel          eicu         eicu  0.802502  0.003536   \n",
       "   8   AttentionModel          eicu        hirid  0.705312  0.026872   \n",
       "   9   AttentionModel          eicu        mimic  0.714940  0.009202   \n",
       "   15  AttentionModel         hirid         aumc  0.806525  0.014401   \n",
       "   16  AttentionModel         hirid         eicu  0.705568  0.009348   \n",
       "   18  AttentionModel         hirid        hirid  0.834322  0.002237   \n",
       "   19  AttentionModel         hirid        mimic  0.648077  0.012512   \n",
       "   20  AttentionModel         mimic         aumc  0.798606  0.022978   \n",
       "   21  AttentionModel         mimic         eicu  0.753329  0.007784   \n",
       "   23  AttentionModel         mimic        hirid  0.694860  0.010384   \n",
       "   24  AttentionModel         mimic        mimic  0.831972  0.003256   \n",
       "p2 0   AttentionModel        pooled         aumc  0.835573  0.010908   \n",
       "   1   AttentionModel        pooled         eicu  0.759670  0.006823   \n",
       "   2   AttentionModel        pooled        hirid  0.729156  0.013789   \n",
       "   3   AttentionModel        pooled        mimic  0.720870  0.005167   \n",
       "p3 0   AttentionModel          aumc         eicu  0.756370  0.007211   \n",
       "   1   AttentionModel          aumc        hirid  0.792255  0.002184   \n",
       "   2   AttentionModel          aumc        mimic  0.786427  0.002008   \n",
       "   3   AttentionModel          eicu         aumc  0.873594  0.016944   \n",
       "   4   AttentionModel          eicu        hirid  0.774104  0.001855   \n",
       "   5   AttentionModel          eicu        mimic  0.789502  0.002433   \n",
       "   6   AttentionModel         hirid         aumc  0.867947  0.010989   \n",
       "   7   AttentionModel         hirid         eicu  0.761983  0.004556   \n",
       "   8   AttentionModel         hirid        mimic  0.794380  0.002116   \n",
       "   9   AttentionModel         mimic         aumc  0.894071  0.003426   \n",
       "   10  AttentionModel         mimic         eicu  0.793121  0.003726   \n",
       "   11  AttentionModel         mimic        hirid  0.797083  0.004061   \n",
       "p4 0   AttentionModel          aumc         aumc  0.829317  0.004322   \n",
       "   1   AttentionModel          eicu         eicu  0.729738  0.005943   \n",
       "   2   AttentionModel         hirid        hirid  0.787233  0.001456   \n",
       "   3   AttentionModel         mimic        mimic  0.782670  0.002296   \n",
       "p5 0   AttentionModel          aumc         eicu  0.724866  0.003635   \n",
       "   1   AttentionModel          aumc        hirid  0.774966  0.002813   \n",
       "   2   AttentionModel          aumc        mimic  0.699866  0.023450   \n",
       "   3   AttentionModel          eicu         aumc  0.773791  0.041005   \n",
       "   4   AttentionModel          eicu        hirid  0.729016  0.010541   \n",
       "   5   AttentionModel          eicu        mimic  0.724120  0.011153   \n",
       "   6   AttentionModel         hirid         aumc  0.844473  0.011185   \n",
       "   7   AttentionModel         hirid         eicu  0.742710  0.006265   \n",
       "   8   AttentionModel         hirid        mimic  0.737982  0.009524   \n",
       "   9   AttentionModel         mimic         aumc  0.843931  0.013971   \n",
       "   10  AttentionModel         mimic         eicu  0.780288  0.002248   \n",
       "   11  AttentionModel         mimic        hirid  0.782912  0.003641   \n",
       "p6 0   AttentionModel          aumc         aumc  0.807556  0.006384   \n",
       "   1   AttentionModel          eicu         eicu  0.711287  0.003618   \n",
       "   2   AttentionModel         hirid        hirid  0.763304  0.001136   \n",
       "   3   AttentionModel         mimic        mimic  0.745647  0.002570   \n",
       "p7 0   AttentionModel        pooled         aumc  0.883985  0.009676   \n",
       "   1   AttentionModel        pooled         eicu  0.770197  0.002738   \n",
       "   2   AttentionModel        pooled        hirid  0.779573  0.001816   \n",
       "   3   AttentionModel        pooled        mimic  0.795438  0.002163   \n",
       "\n",
       "       finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p1 0       False     False              NaN   False  attn             4.055   \n",
       "   1       False     False              NaN   False  attn             3.755   \n",
       "   3       False     False              NaN   False  attn             3.220   \n",
       "   4       False     False              NaN   False  attn             4.235   \n",
       "   5       False     False              NaN   False  attn             3.455   \n",
       "   6       False     False              NaN   False  attn             4.625   \n",
       "   8       False     False              NaN   False  attn             2.680   \n",
       "   9       False     False              NaN   False  attn             4.115   \n",
       "   15      False     False              NaN   False  attn             1.280   \n",
       "   16      False     False              NaN   False  attn             2.930   \n",
       "   18      False     False              NaN   False  attn             2.770   \n",
       "   19      False     False              NaN   False  attn             3.090   \n",
       "   20      False     False              NaN   False  attn             2.825   \n",
       "   21      False     False              NaN   False  attn             3.590   \n",
       "   23      False     False              NaN   False  attn             0.905   \n",
       "   24      False     False              NaN   False  attn             3.365   \n",
       "p2 0       False     False              NaN    True  attn             0.955   \n",
       "   1       False     False              NaN    True  attn             1.470   \n",
       "   2       False     False              NaN    True  attn             1.225   \n",
       "   3       False     False              NaN    True  attn             3.365   \n",
       "p3 0        True     False             0.10   False  attn             3.375   \n",
       "   1        True     False             0.10   False  attn             4.275   \n",
       "   2        True     False             0.10   False  attn             4.085   \n",
       "   3        True     False             0.10   False  attn             2.850   \n",
       "   4        True     False             0.10   False  attn             2.765   \n",
       "   5        True     False             0.10   False  attn             3.495   \n",
       "   6        True     False             0.10   False  attn             3.140   \n",
       "   7        True     False             0.10   False  attn             2.855   \n",
       "   8        True     False             0.10   False  attn             2.675   \n",
       "   9        True     False             0.10   False  attn             1.500   \n",
       "   10       True     False             0.10   False  attn             3.980   \n",
       "   11       True     False             0.10   False  attn             2.405   \n",
       "p4 0       False      True             0.10   False  attn             6.875   \n",
       "   1       False      True             0.10   False  attn             8.320   \n",
       "   2       False      True             0.10   False  attn             4.135   \n",
       "   3       False      True             0.10   False  attn             4.825   \n",
       "p5 0        True     False             0.02   False  attn             5.555   \n",
       "   1        True     False             0.02   False  attn             6.220   \n",
       "   2        True     False             0.02   False  attn             3.645   \n",
       "   3        True     False             0.02   False  attn             3.195   \n",
       "   4        True     False             0.02   False  attn             1.910   \n",
       "   5        True     False             0.02   False  attn             3.915   \n",
       "   6        True     False             0.02   False  attn             2.175   \n",
       "   7        True     False             0.02   False  attn             4.690   \n",
       "   8        True     False             0.02   False  attn             1.555   \n",
       "   9        True     False             0.02   False  attn             2.865   \n",
       "   10       True     False             0.02   False  attn             5.125   \n",
       "   11       True     False             0.02   False  attn             1.245   \n",
       "p6 0       False      True             0.02   False  attn             8.575   \n",
       "   1       False      True             0.02   False  attn             8.350   \n",
       "   2       False      True             0.02   False  attn             5.455   \n",
       "   3       False      True             0.02   False  attn             5.305   \n",
       "p7 0        True     False             0.10    True  attn             2.000   \n",
       "   1        True     False             0.10    True  attn             3.260   \n",
       "   2        True     False             0.10    True  attn             2.840   \n",
       "   3        True     False             0.10    True  attn             3.450   \n",
       "\n",
       "       earliness_std  precision_mean  precision_std  \n",
       "p1 0        0.079844        0.531098       0.022103  \n",
       "   1        1.246044        0.244710       0.007552  \n",
       "   3        1.072147        0.270946       0.012086  \n",
       "   4        1.187908        0.227974       0.008421  \n",
       "   5        1.525451        0.254022       0.010373  \n",
       "   6        1.212693        0.321037       0.010810  \n",
       "   8        1.153581        0.252045       0.018139  \n",
       "   9        0.273633        0.243277       0.004945  \n",
       "   15       0.613239        0.327414       0.035004  \n",
       "   16       0.918286        0.241868       0.008305  \n",
       "   18       0.130384        0.363912       0.008250  \n",
       "   19       0.915253        0.205877       0.007570  \n",
       "   20       1.902383        0.315987       0.049013  \n",
       "   21       1.004459        0.283009       0.010858  \n",
       "   23       0.942802        0.237810       0.009988  \n",
       "   24       0.236907        0.356650       0.006018  \n",
       "p2 0        0.844911        0.368059       0.029036  \n",
       "   1        1.178638        0.285484       0.011018  \n",
       "   2        1.282332        0.265123       0.010473  \n",
       "   3        0.449514        0.252695       0.004614  \n",
       "p3 0        1.219375        0.283055       0.010133  \n",
       "   1        0.269838        0.314837       0.004383  \n",
       "   2        0.271339        0.294975       0.002620  \n",
       "   3        1.014889        0.446497       0.040464  \n",
       "   4        0.621892        0.293611       0.003086  \n",
       "   5        0.361594        0.303076       0.006224  \n",
       "   6        1.011589        0.410842       0.035182  \n",
       "   7        0.733655        0.286573       0.006346  \n",
       "   8        0.446864        0.303278       0.005638  \n",
       "   9        0.546580        0.487837       0.020819  \n",
       "   10       0.758782        0.321017       0.008107  \n",
       "   11       0.361594        0.304273       0.005657  \n",
       "p4 0        0.334944        0.399529       0.017069  \n",
       "   1        0.499187        0.256771       0.003817  \n",
       "   2        0.266693        0.302023       0.003675  \n",
       "   3        0.298957        0.295055       0.004809  \n",
       "p5 0        0.649134        0.252831       0.003548  \n",
       "   1        0.566348        0.301949       0.005359  \n",
       "   2        1.332807        0.234105       0.015405  \n",
       "   3        1.059835        0.303952       0.071046  \n",
       "   4        0.806730        0.264214       0.006479  \n",
       "   5        0.277038        0.246401       0.009146  \n",
       "   6        0.653357        0.380352       0.021274  \n",
       "   7        1.025549        0.263921       0.008179  \n",
       "   8        1.337722        0.255292       0.008648  \n",
       "   9        1.803781        0.386290       0.037804  \n",
       "   10       0.257391        0.294014       0.004669  \n",
       "   11       0.366742        0.305844       0.004670  \n",
       "p6 0        0.729512        0.327786       0.011386  \n",
       "   1        0.385276        0.255480       0.005198  \n",
       "   2        0.156525        0.287224       0.003679  \n",
       "   3        0.207214        0.267340       0.003602  \n",
       "p7 0        1.470225        0.479475       0.019430  \n",
       "   1        0.687750        0.293600       0.004886  \n",
       "   2        0.370220        0.303068       0.002332  \n",
       "   3        0.244310        0.309495       0.006718  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05b5ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df.query(\"train_dataset == eval_dataset & \\\n",
    "        finetuned == False & \\\n",
    "        train_dataset != 'pooled' & \\\n",
    "        baseline == False\"\n",
    ")\n",
    "assert len(df_int) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80e1eeb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_int\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_int' is not defined"
     ]
    }
   ],
   "source": [
    "df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa7f2994",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m auc_int_mean \u001b[38;5;241m=\u001b[39m \u001b[43mdf_int\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m auc_int_std \u001b[38;5;241m=\u001b[39m df_int\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_int' is not defined"
     ]
    }
   ],
   "source": [
    "#auc_int_mean = df_int.mean(axis=0)['auc_mean']\n",
    "#auc_int_std = df_int.std(axis=0)['auc_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "181bcbf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_int_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mauc_int_mean\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_int_mean' is not defined"
     ]
    }
   ],
   "source": [
    "auc_int_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8e22a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "earliness_means = {}\n",
    "earliness_stds = {}\n",
    "precision_means = {}\n",
    "precision_stds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f204498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1228713345.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['int'] = df_int.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1228713345.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['int'] = df_int.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['int'] = df_int.mean()['earliness_mean']\n",
    "earliness_stds['int'] = df_int.std()['earliness_mean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee3bf103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6c2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97280bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_int_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mauc_int_mean\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_int_mean' is not defined"
     ]
    }
   ],
   "source": [
    "auc_int_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc6f8963",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mean pair-wise AUC (no finetuning, no pooling)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m auc_pw_mean \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset != eval_dataset & finetuned == False & train_dataset != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m & baseline == False\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m auc_pw_std \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset != eval_dataset & finetuned == False & train_dataset != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m & baseline == False\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# mean pair-wise AUC (no finetuning, no pooling)\n",
    "#auc_pw_mean = df.query(\"train_dataset != eval_dataset & finetuned == False & train_dataset != 'pooled' & baseline == False\")['auc_mean'].mean()\n",
    "#auc_pw_std = df.query(\"train_dataset != eval_dataset & finetuned == False & train_dataset != 'pooled' & baseline == False\")['auc_mean'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d7d333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pw = df.query(\"train_dataset != eval_dataset & finetuned == False & train_dataset != 'pooled' & baseline == False\")\n",
    "assert len(df_pw) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47db0eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p1</th>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.698006</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.755</td>\n",
       "      <td>1.246044</td>\n",
       "      <td>0.244710</td>\n",
       "      <td>0.007552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.220</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>0.270946</td>\n",
       "      <td>0.012086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.235</td>\n",
       "      <td>1.187908</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0.008421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.455</td>\n",
       "      <td>1.525451</td>\n",
       "      <td>0.254022</td>\n",
       "      <td>0.010373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.705312</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.680</td>\n",
       "      <td>1.153581</td>\n",
       "      <td>0.252045</td>\n",
       "      <td>0.018139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.115</td>\n",
       "      <td>0.273633</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.806525</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.613239</td>\n",
       "      <td>0.327414</td>\n",
       "      <td>0.035004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.705568</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.918286</td>\n",
       "      <td>0.241868</td>\n",
       "      <td>0.008305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.090</td>\n",
       "      <td>0.915253</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.798606</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.825</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>0.049013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.753329</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.590</td>\n",
       "      <td>1.004459</td>\n",
       "      <td>0.283009</td>\n",
       "      <td>0.010858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.694860</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.942802</td>\n",
       "      <td>0.237810</td>\n",
       "      <td>0.009988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p1 1   AttentionModel          aumc         eicu  0.698006  0.010686   \n",
       "   3   AttentionModel          aumc        hirid  0.733108  0.013438   \n",
       "   4   AttentionModel          aumc        mimic  0.685606  0.013133   \n",
       "   5   AttentionModel          eicu         aumc  0.731821  0.007108   \n",
       "   8   AttentionModel          eicu        hirid  0.705312  0.026872   \n",
       "   9   AttentionModel          eicu        mimic  0.714940  0.009202   \n",
       "   15  AttentionModel         hirid         aumc  0.806525  0.014401   \n",
       "   16  AttentionModel         hirid         eicu  0.705568  0.009348   \n",
       "   19  AttentionModel         hirid        mimic  0.648077  0.012512   \n",
       "   20  AttentionModel         mimic         aumc  0.798606  0.022978   \n",
       "   21  AttentionModel         mimic         eicu  0.753329  0.007784   \n",
       "   23  AttentionModel         mimic        hirid  0.694860  0.010384   \n",
       "\n",
       "       finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p1 1       False     False              NaN   False  attn             3.755   \n",
       "   3       False     False              NaN   False  attn             3.220   \n",
       "   4       False     False              NaN   False  attn             4.235   \n",
       "   5       False     False              NaN   False  attn             3.455   \n",
       "   8       False     False              NaN   False  attn             2.680   \n",
       "   9       False     False              NaN   False  attn             4.115   \n",
       "   15      False     False              NaN   False  attn             1.280   \n",
       "   16      False     False              NaN   False  attn             2.930   \n",
       "   19      False     False              NaN   False  attn             3.090   \n",
       "   20      False     False              NaN   False  attn             2.825   \n",
       "   21      False     False              NaN   False  attn             3.590   \n",
       "   23      False     False              NaN   False  attn             0.905   \n",
       "\n",
       "       earliness_std  precision_mean  precision_std  \n",
       "p1 1        1.246044        0.244710       0.007552  \n",
       "   3        1.072147        0.270946       0.012086  \n",
       "   4        1.187908        0.227974       0.008421  \n",
       "   5        1.525451        0.254022       0.010373  \n",
       "   8        1.153581        0.252045       0.018139  \n",
       "   9        0.273633        0.243277       0.004945  \n",
       "   15       0.613239        0.327414       0.035004  \n",
       "   16       0.918286        0.241868       0.008305  \n",
       "   19       0.915253        0.205877       0.007570  \n",
       "   20       1.902383        0.315987       0.049013  \n",
       "   21       1.004459        0.283009       0.010858  \n",
       "   23       0.942802        0.237810       0.009988  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "492f1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1354249333.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['pw'] = df_pw.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1354249333.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['pw'] = df_pw.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['pw'] = df_pw.mean()['earliness_mean']\n",
    "earliness_stds['pw'] = df_pw.std()['earliness_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39ed4280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 3.70375, 'pw': 3.0066666666666664}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e84b1a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_pw_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mauc_pw_mean\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_pw_mean' is not defined"
     ]
    }
   ],
   "source": [
    "auc_pw_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b6d664a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mean pooled AUC (no finetuning)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m auc_po_mean \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m & finetuned == False & baseline == False\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m auc_po_std \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m & finetuned == False & baseline == False\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# mean pooled AUC (no finetuning)\n",
    "#auc_po_mean = df.query(\"train_dataset == 'pooled' & finetuned == False & baseline == False\")['auc_mean'].mean()\n",
    "#auc_po_std = df.query(\"train_dataset == 'pooled' & finetuned == False & baseline == False\")['auc_mean'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "090135a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_po = df.query(\"train_dataset == 'pooled' & finetuned == False & baseline == False\")\n",
    "assert len(df_po) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e476a7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p2</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.835573</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.844911</td>\n",
       "      <td>0.368059</td>\n",
       "      <td>0.029036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.470</td>\n",
       "      <td>1.178638</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.011018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729156</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.282332</td>\n",
       "      <td>0.265123</td>\n",
       "      <td>0.010473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.720870</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.449514</td>\n",
       "      <td>0.252695</td>\n",
       "      <td>0.004614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p2 0  AttentionModel        pooled         aumc  0.835573  0.010908   \n",
       "   1  AttentionModel        pooled         eicu  0.759670  0.006823   \n",
       "   2  AttentionModel        pooled        hirid  0.729156  0.013789   \n",
       "   3  AttentionModel        pooled        mimic  0.720870  0.005167   \n",
       "\n",
       "      finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p2 0      False     False              NaN    True  attn             0.955   \n",
       "   1      False     False              NaN    True  attn             1.470   \n",
       "   2      False     False              NaN    True  attn             1.225   \n",
       "   3      False     False              NaN    True  attn             3.365   \n",
       "\n",
       "      earliness_std  precision_mean  precision_std  \n",
       "p2 0       0.844911        0.368059       0.029036  \n",
       "   1       1.178638        0.285484       0.011018  \n",
       "   2       1.282332        0.265123       0.010473  \n",
       "   3       0.449514        0.252695       0.004614  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37cc4a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/4184905722.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['po'] = df_po.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/4184905722.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['po'] = df_po.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['po'] = df_po.mean()['earliness_mean']\n",
    "earliness_stds['po'] = df_po.std()['earliness_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705d783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87d83fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_po_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mauc_po_mean\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_po_mean' is not defined"
     ]
    }
   ],
   "source": [
    "auc_po_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0364e84",
   "metadata": {},
   "source": [
    "## finetuning on 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6caabc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean finetuned AUC 10% \n",
    "#auc_ft10_mean = df.query(\"train_dataset != 'pooled' & finetuned == True & baseline == False & finetuning_size == 0.10\")['auc_mean'].mean()\n",
    "#auc_ft10_std = df.query(\"train_dataset != 'pooled' & finetuned == True & baseline == False & finetuning_size == 0.10\")['auc_std'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac8cd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft(ft_size=0.10):\n",
    "    return df.query(\"train_dataset != 'pooled' & finetuned == True & baseline == False & finetuning_size == @ft_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d39fc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft10 = get_ft(0.10)\n",
    "auc_ft10_mean = df_ft10['auc_mean'].mean()\n",
    "auc_ft10_std = df_ft10['auc_mean'].std()\n",
    "assert len(df_ft10) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "116ff452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p3</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1.219375</td>\n",
       "      <td>0.283055</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.792255</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.085</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.873594</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.850</td>\n",
       "      <td>1.014889</td>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.040464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.293611</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.495</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.303076</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.867947</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.140</td>\n",
       "      <td>1.011589</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.035182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.894071</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.546580</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.020819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p3 0   AttentionModel          aumc         eicu  0.756370  0.007211   \n",
       "   1   AttentionModel          aumc        hirid  0.792255  0.002184   \n",
       "   2   AttentionModel          aumc        mimic  0.786427  0.002008   \n",
       "   3   AttentionModel          eicu         aumc  0.873594  0.016944   \n",
       "   4   AttentionModel          eicu        hirid  0.774104  0.001855   \n",
       "   5   AttentionModel          eicu        mimic  0.789502  0.002433   \n",
       "   6   AttentionModel         hirid         aumc  0.867947  0.010989   \n",
       "   7   AttentionModel         hirid         eicu  0.761983  0.004556   \n",
       "   8   AttentionModel         hirid        mimic  0.794380  0.002116   \n",
       "   9   AttentionModel         mimic         aumc  0.894071  0.003426   \n",
       "   10  AttentionModel         mimic         eicu  0.793121  0.003726   \n",
       "   11  AttentionModel         mimic        hirid  0.797083  0.004061   \n",
       "\n",
       "       finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p3 0        True     False              0.1   False  attn             3.375   \n",
       "   1        True     False              0.1   False  attn             4.275   \n",
       "   2        True     False              0.1   False  attn             4.085   \n",
       "   3        True     False              0.1   False  attn             2.850   \n",
       "   4        True     False              0.1   False  attn             2.765   \n",
       "   5        True     False              0.1   False  attn             3.495   \n",
       "   6        True     False              0.1   False  attn             3.140   \n",
       "   7        True     False              0.1   False  attn             2.855   \n",
       "   8        True     False              0.1   False  attn             2.675   \n",
       "   9        True     False              0.1   False  attn             1.500   \n",
       "   10       True     False              0.1   False  attn             3.980   \n",
       "   11       True     False              0.1   False  attn             2.405   \n",
       "\n",
       "       earliness_std  precision_mean  precision_std  \n",
       "p3 0        1.219375        0.283055       0.010133  \n",
       "   1        0.269838        0.314837       0.004383  \n",
       "   2        0.271339        0.294975       0.002620  \n",
       "   3        1.014889        0.446497       0.040464  \n",
       "   4        0.621892        0.293611       0.003086  \n",
       "   5        0.361594        0.303076       0.006224  \n",
       "   6        1.011589        0.410842       0.035182  \n",
       "   7        0.733655        0.286573       0.006346  \n",
       "   8        0.446864        0.303278       0.005638  \n",
       "   9        0.546580        0.487837       0.020819  \n",
       "   10       0.758782        0.321017       0.008107  \n",
       "   11       0.361594        0.304273       0.005657  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffec95fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p3</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1.219375</td>\n",
       "      <td>0.283055</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.792255</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.085</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.873594</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.850</td>\n",
       "      <td>1.014889</td>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.040464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.293611</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.495</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.303076</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.867947</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.140</td>\n",
       "      <td>1.011589</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.035182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.894071</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.546580</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.020819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p3 0   AttentionModel          aumc         eicu  0.756370  0.007211   \n",
       "   1   AttentionModel          aumc        hirid  0.792255  0.002184   \n",
       "   2   AttentionModel          aumc        mimic  0.786427  0.002008   \n",
       "   3   AttentionModel          eicu         aumc  0.873594  0.016944   \n",
       "   4   AttentionModel          eicu        hirid  0.774104  0.001855   \n",
       "   5   AttentionModel          eicu        mimic  0.789502  0.002433   \n",
       "   6   AttentionModel         hirid         aumc  0.867947  0.010989   \n",
       "   7   AttentionModel         hirid         eicu  0.761983  0.004556   \n",
       "   8   AttentionModel         hirid        mimic  0.794380  0.002116   \n",
       "   9   AttentionModel         mimic         aumc  0.894071  0.003426   \n",
       "   10  AttentionModel         mimic         eicu  0.793121  0.003726   \n",
       "   11  AttentionModel         mimic        hirid  0.797083  0.004061   \n",
       "\n",
       "       finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p3 0        True     False              0.1   False  attn             3.375   \n",
       "   1        True     False              0.1   False  attn             4.275   \n",
       "   2        True     False              0.1   False  attn             4.085   \n",
       "   3        True     False              0.1   False  attn             2.850   \n",
       "   4        True     False              0.1   False  attn             2.765   \n",
       "   5        True     False              0.1   False  attn             3.495   \n",
       "   6        True     False              0.1   False  attn             3.140   \n",
       "   7        True     False              0.1   False  attn             2.855   \n",
       "   8        True     False              0.1   False  attn             2.675   \n",
       "   9        True     False              0.1   False  attn             1.500   \n",
       "   10       True     False              0.1   False  attn             3.980   \n",
       "   11       True     False              0.1   False  attn             2.405   \n",
       "\n",
       "       earliness_std  precision_mean  precision_std  \n",
       "p3 0        1.219375        0.283055       0.010133  \n",
       "   1        0.269838        0.314837       0.004383  \n",
       "   2        0.271339        0.294975       0.002620  \n",
       "   3        1.014889        0.446497       0.040464  \n",
       "   4        0.621892        0.293611       0.003086  \n",
       "   5        0.361594        0.303076       0.006224  \n",
       "   6        1.011589        0.410842       0.035182  \n",
       "   7        0.733655        0.286573       0.006346  \n",
       "   8        0.446864        0.303278       0.005638  \n",
       "   9        0.546580        0.487837       0.020819  \n",
       "   10       0.758782        0.321017       0.008107  \n",
       "   11       0.361594        0.304273       0.005657  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft10 #old run, with bug in line 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7ee5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/894474592.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['ft10'] = df_ft10.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/894474592.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['ft10'] = df_ft10.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['ft10'] = df_ft10.mean()['earliness_mean']\n",
    "earliness_stds['ft10'] = df_ft10.std()['earliness_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d9c98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 3.70375,\n",
       " 'pw': 3.0066666666666664,\n",
       " 'po': 1.7537500000000001,\n",
       " 'ft10': 3.116666666666667}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a18f78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8067364255118971"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_ft10_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10821dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean baseline AUC (internally trained on finetuning data) 10% size\n",
    "#auc_base10_mean = df.query(\"train_dataset != 'pooled' & finetuned == False & baseline == True & finetuning_size == 0.10\")['auc_mean'].mean()\n",
    "#auc_base10_std =  df.query(\"train_dataset != 'pooled' & finetuned == False & baseline == True & finetuning_size == 0.10\")['auc_std'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5aac505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(ft_size=0.10):\n",
    "    return df.query(\"train_dataset != 'pooled' & finetuned == False & baseline == True & finetuning_size == @ft_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85318a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base10 = get_baseline(0.10)\n",
    "auc_base10_mean = df_base10['auc_mean'].mean()\n",
    "auc_base10_std = df_base10['auc_mean'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20bf5a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7822397449362146"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_base10_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "247a99e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_base2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_base2\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_base2' is not defined"
     ]
    }
   ],
   "source": [
    "df_base2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0cbfcc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1832192828.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['int10'] = df_base10.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1832192828.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['int10'] = df_base10.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['int10'] = df_base10.mean()['earliness_mean']\n",
    "earliness_stds['int10'] = df_base10.std()['earliness_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3d34e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 0.8080261443790046,\n",
       " 'pw': 1.0195773039784077,\n",
       " 'po': 1.0945651724162737,\n",
       " 'ft10': 0.7870293206197286,\n",
       " 'int10': 1.9149298290015755}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_stds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5793918",
   "metadata": {},
   "source": [
    "## finetuning on 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da1f6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft2 = get_ft(0.02)\n",
    "auc_ft2_mean = df_ft2['auc_mean'].mean()\n",
    "auc_ft2_std = df_ft2['auc_mean'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b502d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/2657514013.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['ft2'] = df_ft2.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/2657514013.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['ft2'] = df_ft2.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['ft2'] = df_ft2.mean()['earliness_mean']\n",
    "earliness_stds['ft2'] = df_ft2.std()['earliness_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a9647c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 3.70375,\n",
       " 'pw': 3.0066666666666664,\n",
       " 'po': 1.7537500000000001,\n",
       " 'ft10': 3.116666666666667,\n",
       " 'int10': 6.03875,\n",
       " 'ft2': 3.507916666666667}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a04c0d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7632434823574795"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_ft2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f8408b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base2 = get_baseline(0.02)\n",
    "auc_base2_mean = df_base2['auc_mean'].mean()\n",
    "auc_base2_std = df_base2['auc_mean'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4c15a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1515663671.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_means['int2'] = df_base2.mean()['earliness_mean']\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/1515663671.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  earliness_stds['int2'] = df_base2.std()['earliness_mean']\n"
     ]
    }
   ],
   "source": [
    "earliness_means['int2'] = df_base2.mean()['earliness_mean']\n",
    "earliness_stds['int2'] = df_base2.std()['earliness_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ab35cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7569484196682784"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_base2_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32649602",
   "metadata": {},
   "source": [
    "## finetuned 10% with pooled preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0e2c11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft10po = df.query(\"train_dataset == 'pooled' & finetuned == True & baseline == False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "531bfc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p7</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.883985</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.470225</td>\n",
       "      <td>0.479475</td>\n",
       "      <td>0.019430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.687750</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.004886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.779573</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.370220</td>\n",
       "      <td>0.303068</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.244310</td>\n",
       "      <td>0.309495</td>\n",
       "      <td>0.006718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p7 0  AttentionModel        pooled         aumc  0.883985  0.009676   \n",
       "   1  AttentionModel        pooled         eicu  0.770197  0.002738   \n",
       "   2  AttentionModel        pooled        hirid  0.779573  0.001816   \n",
       "   3  AttentionModel        pooled        mimic  0.795438  0.002163   \n",
       "\n",
       "      finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p7 0       True     False              0.1    True  attn              2.00   \n",
       "   1       True     False              0.1    True  attn              3.26   \n",
       "   2       True     False              0.1    True  attn              2.84   \n",
       "   3       True     False              0.1    True  attn              3.45   \n",
       "\n",
       "      earliness_std  precision_mean  precision_std  \n",
       "p7 0       1.470225        0.479475       0.019430  \n",
       "   1       0.687750        0.293600       0.004886  \n",
       "   2       0.370220        0.303068       0.002332  \n",
       "   3       0.244310        0.309495       0.006718  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft10po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6186ecab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_pw_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      2\u001b[0m x_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m aucs \u001b[38;5;241m=\u001b[39m [\u001b[43mauc_pw_mean\u001b[49m, auc_po_mean, auc_ft_mean, auc_int_mean]\n\u001b[1;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m [auc_pw_std, auc_po_std, auc_ft_std, auc_int_std]\n\u001b[1;32m      6\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair-wise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_pw_mean' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_pos = np.arange(4)\n",
    "aucs = [auc_pw_mean, auc_po_mean, auc_ft_mean, auc_int_mean]\n",
    "stds = [auc_pw_std, auc_po_std, auc_ft_std, auc_int_std]\n",
    "\n",
    "names = ['pair-wise', 'pooled', 'finetuning', 'internal']\n",
    "ax.bar(x_pos, aucs, yerr=stds, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylim(0.5,0.9)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(f'AUROC (mean $\\pm$ std)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "212428c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_pw_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      2\u001b[0m x_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m aucs \u001b[38;5;241m=\u001b[39m [\u001b[43mauc_pw_mean\u001b[49m, auc_po_mean, auc_ft_mean, auc_int_mean]\n\u001b[1;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m [auc_pw_std, auc_po_std, auc_ft_std, auc_int_std]\n\u001b[1;32m      6\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair-wise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_pw_mean' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_pos = np.arange(4)\n",
    "aucs = [auc_pw_mean, auc_po_mean, auc_ft_mean, auc_int_mean]\n",
    "stds = [auc_pw_std, auc_po_std, auc_ft_std, auc_int_std]\n",
    "\n",
    "names = ['pair-wise', 'pooled', 'finetuning', 'internal']\n",
    "ax.errorbar(x_pos, aucs, yerr=stds, ecolor='black', capsize=7, fmt='d', color='black')\n",
    "ax.set_ylim(0.7,0.9)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(f'AUROC (mean $\\pm$ std)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bf17138",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+200B (347523906.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [72]\u001b[0;36m\u001b[0m\n\u001b[0;31m    â€‹\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+200B\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c786767",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_pw_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      2\u001b[0m x_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m aucs \u001b[38;5;241m=\u001b[39m [\u001b[43mauc_pw_mean\u001b[49m, auc_po_mean, auc_ft_mean, auc_base_mean, auc_int_mean]\n\u001b[1;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m [auc_pw_std, auc_po_std, auc_ft_std, auc_base_std, auc_int_std]\n\u001b[1;32m      6\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair-wise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal-small\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_pw_mean' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_pos = np.arange(5)\n",
    "aucs = [auc_pw_mean, auc_po_mean, auc_ft_mean, auc_base_mean, auc_int_mean]\n",
    "stds = [auc_pw_std, auc_po_std, auc_ft_std, auc_base_std, auc_int_std]\n",
    "\n",
    "names = ['pair-wise', 'pooled', 'finetuning', 'internal-small', 'internal']\n",
    "ax.bar(x_pos, aucs, yerr=stds, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylim(0.7,0.9)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(f'AUROC (mean $\\pm$ std)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e029e17c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_pw_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      2\u001b[0m x_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m aucs \u001b[38;5;241m=\u001b[39m [\u001b[43mauc_pw_mean\u001b[49m, auc_po_mean, auc_ft2_mean, auc_base2_mean, auc_ft10_mean, auc_base10_mean, auc_int_mean]\n\u001b[1;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m [auc_pw_std, auc_po_std, auc_ft2_std, auc_base2_std, auc_ft10_std, auc_base10_std, auc_int_std]\n\u001b[1;32m      6\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair-wise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuning-2\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal-2\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuning-10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal-10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_pw_mean' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAEzCAYAAAAGisbbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPd0lEQVR4nO3dX6jkd3nH8c9jYipoVGi2INnEBLqpblXQHkKKFwrassnF5sJWEghWCe5NI7aKEFFU4pVKLQjxz5ZKqqBp9EIWXMmFTQmIkWxIG0wkskRrNgpZ/+VGNKZ9enFGOa67eybrPGd3ktcLFs7vN98z88CXs/ve38yZqe4OAAAznnO2BwAAeCYTWwAAg8QWAMAgsQUAMEhsAQAMElsAAIO2ja2q+mxVPV5V3z7F7VVVn6iqo1X1QFW9ZvVjAgCsp2WubN2WZN9pbr86yZ7FnwNJPvWHjwUA8MywbWx1991JfnqaJdcm+VxvuifJi6vqJasaEABgna3iNVsXJ3l0y/GxxTkAgGe983fywarqQDafaszzn//8v3jZy162kw8PAHBG7rvvvh93964z+d5VxNZjSS7Zcrx7ce73dPfBJAeTZGNjo48cObKChwcAmFVV/3Om37uKpxEPJXnL4rcSr0ryRHf/aAX3CwCw9ra9slVVX0zy+iQXVdWxJB9M8twk6e5PJzmc5JokR5P8IsnbpoYFAFg328ZWd1+/ze2d5O9XNhEAwDOId5AHABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGLRVbVbWvqh6uqqNVdfNJbr+0qu6qqvur6oGqumb1owIArJ9tY6uqzktya5Krk+xNcn1V7T1h2fuT3NHdr05yXZJPrnpQAIB1tMyVrSuTHO3uR7r7ySS3J7n2hDWd5IWLr1+U5IerGxEAYH0tE1sXJ3l0y/GxxbmtPpTkhqo6luRwknec7I6q6kBVHamqI8ePHz+DcQEA1suqXiB/fZLbunt3kmuSfL6qfu++u/tgd29098auXbtW9NAAAOeuZWLrsSSXbDnevTi31Y1J7kiS7v5mkucluWgVAwIArLNlYuveJHuq6vKquiCbL4A/dMKaHyR5Q5JU1cuzGVueJwQAnvW2ja3ufirJTUnuTPKdbP7W4YNVdUtV7V8se3eSt1fVfyf5YpK3dndPDQ0AsC7OX2ZRdx/O5gvft577wJavH0ry2tWOBgCw/ryDPADAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMCgpWKrqvZV1cNVdbSqbj7FmjdX1UNV9WBVfWG1YwIArKfzt1tQVecluTXJXyU5luTeqjrU3Q9tWbMnyXuTvLa7f1ZVfzI1MADAOlnmytaVSY529yPd/WSS25Nce8Katye5tbt/liTd/fhqxwQAWE/LxNbFSR7dcnxscW6rK5JcUVXfqKp7qmrfqgYEAFhn2z6N+DTuZ0+S1yfZneTuqnpld/9866KqOpDkQJJceumlK3poAIBz1zJXth5LcsmW492Lc1sdS3Kou3/d3d9L8t1sxtfv6O6D3b3R3Ru7du0605kBANbGMrF1b5I9VXV5VV2Q5Lokh05Y85VsXtVKVV2UzacVH1ndmAAA62nb2Orup5LclOTOJN9Jckd3P1hVt1TV/sWyO5P8pKoeSnJXkvd090+mhgYAWBfV3WflgTc2NvrIkSNn5bEBAJ6OqrqvuzfO5Hu9gzwAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAoKViq6r2VdXDVXW0qm4+zbo3VVVX1cbqRgQAWF/bxlZVnZfk1iRXJ9mb5Pqq2nuSdRcmeWeSb616SACAdbXMla0rkxzt7ke6+8kktye59iTrPpzkI0l+ucL5AADW2jKxdXGSR7ccH1uc+62qek2SS7r7qyucDQBg7f3BL5Cvquck+XiSdy+x9kBVHamqI8ePH/9DHxoA4Jy3TGw9luSSLce7F+d+48Ikr0jyn1X1/SRXJTl0shfJd/fB7t7o7o1du3ad+dQAAGtimdi6N8meqrq8qi5Icl2SQ7+5sbuf6O6Luvuy7r4syT1J9nf3kZGJAQDWyLax1d1PJbkpyZ1JvpPkju5+sKpuqar90wMCAKyz85dZ1N2Hkxw+4dwHTrH29X/4WAAAzwzeQR4AYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABi0VGxV1b6qeriqjlbVzSe5/V1V9VBVPVBVX6+ql65+VACA9bNtbFXVeUluTXJ1kr1Jrq+qvScsuz/JRne/KsmXk3x01YMCAKyjZa5sXZnkaHc/0t1PJrk9ybVbF3T3Xd39i8XhPUl2r3ZMAID1tExsXZzk0S3HxxbnTuXGJF872Q1VdaCqjlTVkePHjy8/JQDAmlrpC+Sr6oYkG0k+drLbu/tgd29098auXbtW+dAAAOek85dY81iSS7Yc716c+x1V9cYk70vyuu7+1WrGAwBYb8tc2bo3yZ6quryqLkhyXZJDWxdU1auTfCbJ/u5+fPVjAgCsp21jq7ufSnJTkjuTfCfJHd39YFXdUlX7F8s+luQFSb5UVf9VVYdOcXcAAM8qyzyNmO4+nOTwCec+sOXrN654LgCAZwTvIA8AMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwaKnYqqp9VfVwVR2tqptPcvsfVdW/L27/VlVdtvJJAQDW0LaxVVXnJbk1ydVJ9ia5vqr2nrDsxiQ/6+4/TfLPST6y6kEBANbRMle2rkxytLsf6e4nk9ye5NoT1lyb5N8WX385yRuqqlY3JgDAelomti5O8uiW42OLcydd091PJXkiyR+vYkAAgHV2/k4+WFUdSHJgcfirqvr2Tj4+K3VRkh+f7SE4I/Zuvdm/9WXv1tufnek3LhNbjyW5ZMvx7sW5k605VlXnJ3lRkp+ceEfdfTDJwSSpqiPdvXEmQ3P22b/1Ze/Wm/1bX/ZuvVXVkTP93mWeRrw3yZ6quryqLkhyXZJDJ6w5lOTvFl//TZL/6O4+06EAAJ4ptr2y1d1PVdVNSe5Mcl6Sz3b3g1V1S5Ij3X0oyb8m+XxVHU3y02wGGQDAs95Sr9nq7sNJDp9w7gNbvv5lkr99mo998Gmu59xi/9aXvVtv9m992bv1dsb7V57tAwCY4+N6AAAGjceWj/pZX0vs3buq6qGqeqCqvl5VLz0bc3Jy2+3flnVvqqquKr8ldQ5ZZv+q6s2Ln8EHq+oLOz0jJ7fE352XVtVdVXX/4u/Pa87GnPy+qvpsVT1+qremqk2fWOztA1X1mmXudzS2fNTP+lpy7+5PstHdr8rmJwd8dGen5FSW3L9U1YVJ3pnkWzs7IaezzP5V1Z4k703y2u7+8yT/sNNz8vuW/Nl7f5I7uvvV2fyFsk/u7JScxm1J9p3m9quT7Fn8OZDkU8vc6fSVLR/1s7623bvuvqu7f7E4vCeb78HGuWGZn70k+XA2/4Pzy50cjm0ts39vT3Jrd/8sSbr78R2ekZNbZu86yQsXX78oyQ93cD5Oo7vvzua7KpzKtUk+15vuSfLiqnrJdvc7HVs+6md9LbN3W92Y5GujE/F0bLt/i8vfl3T3V3dyMJayzM/fFUmuqKpvVNU9VXW6/42zc5bZuw8luaGqjmXzN/3fsTOjsQJP99/GJDv8cT08M1XVDUk2krzubM/CcqrqOUk+nuStZ3kUztz52Xwq4/XZvKp8d1W9srt/fjaHYinXJ7mtu/+pqv4ym+9T+Yru/r+zPRgzpq9sPZ2P+snpPuqHHbfM3qWq3pjkfUn2d/evdmg2trfd/l2Y5BVJ/rOqvp/kqiSHvEj+nLHMz9+xJIe6+9fd/b0k381mfHF2LbN3Nya5I0m6+5tJnpfNz03k3LfUv40nmo4tH/Wzvrbdu6p6dZLPZDO0vF7k3HLa/evuJ7r7ou6+rLsvy+Zr7vZ39xl/9hcrtczfnV/J5lWtVNVF2Xxa8ZEdnJGTW2bvfpDkDUlSVS/PZmwd39EpOVOHkrxl8VuJVyV5ort/tN03jT6N6KN+1teSe/exJC9I8qXF7zT8oLv3n7Wh+a0l949z1JL7d2eSv66qh5L8b5L3dLdnBc6yJffu3Un+par+MZsvln+riwznhqr6Yjb/E3PR4jV1H0zy3CTp7k9n8zV21yQ5muQXSd621P3aXwCAOd5BHgBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCAQf8PjSWwzUqYJGMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "x_pos = np.arange(7)\n",
    "aucs = [auc_pw_mean, auc_po_mean, auc_ft2_mean, auc_base2_mean, auc_ft10_mean, auc_base10_mean, auc_int_mean]\n",
    "stds = [auc_pw_std, auc_po_std, auc_ft2_std, auc_base2_std, auc_ft10_std, auc_base10_std, auc_int_std]\n",
    "\n",
    "names = ['pair-wise', 'pooled', 'finetuning-2%', 'internal-2%', 'finetuning-10%', 'internal-10%', 'internal']\n",
    "ax.errorbar(x_pos, aucs, yerr=stds, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "ax.set_ylim(0.65,0.9)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(f'AUROC (mean $\\pm$ std)')\n",
    "\n",
    "plt.plot((2,3), (auc_ft2_mean, auc_base2_mean), '--', color='black')\n",
    "plt.plot((4,5), (auc_ft10_mean, auc_base10_mean), '--', color='black')\n",
    "plt.hlines(auc_int_mean, 0,6, color='black', linestyle='dotted')\n",
    "\n",
    "plt.title('Performance across datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fbaebfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6b09e611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {'p1': 'internal', \n",
    "           'p2': 'pooled', \n",
    "           'p3': 'finetuning-10%',\n",
    "           'p4': 'internal-10%',\n",
    "           'p5': 'finetuning-2%',\n",
    "           'p6': 'internal-2%',\n",
    "           'p7': 'finetuning-10%-pooled'\n",
    "}\n",
    "df['level_0'] = df['level_0'].apply(lambda x: mapping.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "61e6f582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>internal</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.917711</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.055</td>\n",
       "      <td>0.079844</td>\n",
       "      <td>0.531098</td>\n",
       "      <td>0.022103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internal</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.698006</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.755</td>\n",
       "      <td>1.246044</td>\n",
       "      <td>0.244710</td>\n",
       "      <td>0.007552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>internal</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.220</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>0.270946</td>\n",
       "      <td>0.012086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internal</td>\n",
       "      <td>4</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.235</td>\n",
       "      <td>1.187908</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0.008421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>internal</td>\n",
       "      <td>5</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.455</td>\n",
       "      <td>1.525451</td>\n",
       "      <td>0.254022</td>\n",
       "      <td>0.010373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>internal</td>\n",
       "      <td>6</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.802502</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.625</td>\n",
       "      <td>1.212693</td>\n",
       "      <td>0.321037</td>\n",
       "      <td>0.010810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>internal</td>\n",
       "      <td>8</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.705312</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.680</td>\n",
       "      <td>1.153581</td>\n",
       "      <td>0.252045</td>\n",
       "      <td>0.018139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>internal</td>\n",
       "      <td>9</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.115</td>\n",
       "      <td>0.273633</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>internal</td>\n",
       "      <td>15</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.806525</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.613239</td>\n",
       "      <td>0.327414</td>\n",
       "      <td>0.035004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>internal</td>\n",
       "      <td>16</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.705568</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.918286</td>\n",
       "      <td>0.241868</td>\n",
       "      <td>0.008305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>internal</td>\n",
       "      <td>18</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.834322</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.770</td>\n",
       "      <td>0.130384</td>\n",
       "      <td>0.363912</td>\n",
       "      <td>0.008250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>internal</td>\n",
       "      <td>19</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.090</td>\n",
       "      <td>0.915253</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>internal</td>\n",
       "      <td>20</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.798606</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.825</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>0.049013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>internal</td>\n",
       "      <td>21</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.753329</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.590</td>\n",
       "      <td>1.004459</td>\n",
       "      <td>0.283009</td>\n",
       "      <td>0.010858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>internal</td>\n",
       "      <td>23</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.694860</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.942802</td>\n",
       "      <td>0.237810</td>\n",
       "      <td>0.009988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>internal</td>\n",
       "      <td>24</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.831972</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.236907</td>\n",
       "      <td>0.356650</td>\n",
       "      <td>0.006018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pooled</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.835573</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.844911</td>\n",
       "      <td>0.368059</td>\n",
       "      <td>0.029036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pooled</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.470</td>\n",
       "      <td>1.178638</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.011018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pooled</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729156</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.282332</td>\n",
       "      <td>0.265123</td>\n",
       "      <td>0.010473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pooled</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.720870</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.449514</td>\n",
       "      <td>0.252695</td>\n",
       "      <td>0.004614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1.219375</td>\n",
       "      <td>0.283055</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.792255</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.085</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.873594</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.850</td>\n",
       "      <td>1.014889</td>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.040464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>4</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.293611</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>5</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.495</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.303076</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>6</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.867947</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.140</td>\n",
       "      <td>1.011589</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.035182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>7</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>8</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>9</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.894071</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.546580</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.020819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>10</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>11</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.829317</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.334944</td>\n",
       "      <td>0.399529</td>\n",
       "      <td>0.017069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.729738</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.320</td>\n",
       "      <td>0.499187</td>\n",
       "      <td>0.256771</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.787233</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.135</td>\n",
       "      <td>0.266693</td>\n",
       "      <td>0.302023</td>\n",
       "      <td>0.003675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.782670</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.825</td>\n",
       "      <td>0.298957</td>\n",
       "      <td>0.295055</td>\n",
       "      <td>0.004809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.724866</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.649134</td>\n",
       "      <td>0.252831</td>\n",
       "      <td>0.003548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774966</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.220</td>\n",
       "      <td>0.566348</td>\n",
       "      <td>0.301949</td>\n",
       "      <td>0.005359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.645</td>\n",
       "      <td>1.332807</td>\n",
       "      <td>0.234105</td>\n",
       "      <td>0.015405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.041005</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.195</td>\n",
       "      <td>1.059835</td>\n",
       "      <td>0.303952</td>\n",
       "      <td>0.071046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>4</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729016</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.806730</td>\n",
       "      <td>0.264214</td>\n",
       "      <td>0.006479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>5</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.724120</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.915</td>\n",
       "      <td>0.277038</td>\n",
       "      <td>0.246401</td>\n",
       "      <td>0.009146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>6</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.844473</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.175</td>\n",
       "      <td>0.653357</td>\n",
       "      <td>0.380352</td>\n",
       "      <td>0.021274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>7</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.742710</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.690</td>\n",
       "      <td>1.025549</td>\n",
       "      <td>0.263921</td>\n",
       "      <td>0.008179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>8</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.737982</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.337722</td>\n",
       "      <td>0.255292</td>\n",
       "      <td>0.008648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>9</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.843931</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.865</td>\n",
       "      <td>1.803781</td>\n",
       "      <td>0.386290</td>\n",
       "      <td>0.037804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>10</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.780288</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.125</td>\n",
       "      <td>0.257391</td>\n",
       "      <td>0.294014</td>\n",
       "      <td>0.004669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>11</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.782912</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.366742</td>\n",
       "      <td>0.305844</td>\n",
       "      <td>0.004670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.807556</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.575</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.327786</td>\n",
       "      <td>0.011386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.711287</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.350</td>\n",
       "      <td>0.385276</td>\n",
       "      <td>0.255480</td>\n",
       "      <td>0.005198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.763304</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.455</td>\n",
       "      <td>0.156525</td>\n",
       "      <td>0.287224</td>\n",
       "      <td>0.003679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.745647</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.305</td>\n",
       "      <td>0.207214</td>\n",
       "      <td>0.267340</td>\n",
       "      <td>0.003602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.883985</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.470225</td>\n",
       "      <td>0.479475</td>\n",
       "      <td>0.019430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.687750</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.004886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.779573</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.840</td>\n",
       "      <td>0.370220</td>\n",
       "      <td>0.303068</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.450</td>\n",
       "      <td>0.244310</td>\n",
       "      <td>0.309495</td>\n",
       "      <td>0.006718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  level_0  level_1         model_x train_dataset eval_dataset  \\\n",
       "0                internal        0  AttentionModel          aumc         aumc   \n",
       "1                internal        1  AttentionModel          aumc         eicu   \n",
       "2                internal        3  AttentionModel          aumc        hirid   \n",
       "3                internal        4  AttentionModel          aumc        mimic   \n",
       "4                internal        5  AttentionModel          eicu         aumc   \n",
       "5                internal        6  AttentionModel          eicu         eicu   \n",
       "6                internal        8  AttentionModel          eicu        hirid   \n",
       "7                internal        9  AttentionModel          eicu        mimic   \n",
       "8                internal       15  AttentionModel         hirid         aumc   \n",
       "9                internal       16  AttentionModel         hirid         eicu   \n",
       "10               internal       18  AttentionModel         hirid        hirid   \n",
       "11               internal       19  AttentionModel         hirid        mimic   \n",
       "12               internal       20  AttentionModel         mimic         aumc   \n",
       "13               internal       21  AttentionModel         mimic         eicu   \n",
       "14               internal       23  AttentionModel         mimic        hirid   \n",
       "15               internal       24  AttentionModel         mimic        mimic   \n",
       "16                 pooled        0  AttentionModel        pooled         aumc   \n",
       "17                 pooled        1  AttentionModel        pooled         eicu   \n",
       "18                 pooled        2  AttentionModel        pooled        hirid   \n",
       "19                 pooled        3  AttentionModel        pooled        mimic   \n",
       "20         finetuning-10%        0  AttentionModel          aumc         eicu   \n",
       "21         finetuning-10%        1  AttentionModel          aumc        hirid   \n",
       "22         finetuning-10%        2  AttentionModel          aumc        mimic   \n",
       "23         finetuning-10%        3  AttentionModel          eicu         aumc   \n",
       "24         finetuning-10%        4  AttentionModel          eicu        hirid   \n",
       "25         finetuning-10%        5  AttentionModel          eicu        mimic   \n",
       "26         finetuning-10%        6  AttentionModel         hirid         aumc   \n",
       "27         finetuning-10%        7  AttentionModel         hirid         eicu   \n",
       "28         finetuning-10%        8  AttentionModel         hirid        mimic   \n",
       "29         finetuning-10%        9  AttentionModel         mimic         aumc   \n",
       "30         finetuning-10%       10  AttentionModel         mimic         eicu   \n",
       "31         finetuning-10%       11  AttentionModel         mimic        hirid   \n",
       "32           internal-10%        0  AttentionModel          aumc         aumc   \n",
       "33           internal-10%        1  AttentionModel          eicu         eicu   \n",
       "34           internal-10%        2  AttentionModel         hirid        hirid   \n",
       "35           internal-10%        3  AttentionModel         mimic        mimic   \n",
       "36          finetuning-2%        0  AttentionModel          aumc         eicu   \n",
       "37          finetuning-2%        1  AttentionModel          aumc        hirid   \n",
       "38          finetuning-2%        2  AttentionModel          aumc        mimic   \n",
       "39          finetuning-2%        3  AttentionModel          eicu         aumc   \n",
       "40          finetuning-2%        4  AttentionModel          eicu        hirid   \n",
       "41          finetuning-2%        5  AttentionModel          eicu        mimic   \n",
       "42          finetuning-2%        6  AttentionModel         hirid         aumc   \n",
       "43          finetuning-2%        7  AttentionModel         hirid         eicu   \n",
       "44          finetuning-2%        8  AttentionModel         hirid        mimic   \n",
       "45          finetuning-2%        9  AttentionModel         mimic         aumc   \n",
       "46          finetuning-2%       10  AttentionModel         mimic         eicu   \n",
       "47          finetuning-2%       11  AttentionModel         mimic        hirid   \n",
       "48            internal-2%        0  AttentionModel          aumc         aumc   \n",
       "49            internal-2%        1  AttentionModel          eicu         eicu   \n",
       "50            internal-2%        2  AttentionModel         hirid        hirid   \n",
       "51            internal-2%        3  AttentionModel         mimic        mimic   \n",
       "52  finetuning-10%-pooled        0  AttentionModel        pooled         aumc   \n",
       "53  finetuning-10%-pooled        1  AttentionModel        pooled         eicu   \n",
       "54  finetuning-10%-pooled        2  AttentionModel        pooled        hirid   \n",
       "55  finetuning-10%-pooled        3  AttentionModel        pooled        mimic   \n",
       "\n",
       "    auc_mean   auc_std  finetuned  baseline  finetuning_size  pooled model_y  \\\n",
       "0   0.917711  0.003569      False     False              NaN   False  attn     \n",
       "1   0.698006  0.010686      False     False              NaN   False  attn     \n",
       "2   0.733108  0.013438      False     False              NaN   False  attn     \n",
       "3   0.685606  0.013133      False     False              NaN   False  attn     \n",
       "4   0.731821  0.007108      False     False              NaN   False  attn     \n",
       "5   0.802502  0.003536      False     False              NaN   False  attn     \n",
       "6   0.705312  0.026872      False     False              NaN   False  attn     \n",
       "7   0.714940  0.009202      False     False              NaN   False  attn     \n",
       "8   0.806525  0.014401      False     False              NaN   False  attn     \n",
       "9   0.705568  0.009348      False     False              NaN   False  attn     \n",
       "10  0.834322  0.002237      False     False              NaN   False  attn     \n",
       "11  0.648077  0.012512      False     False              NaN   False  attn     \n",
       "12  0.798606  0.022978      False     False              NaN   False  attn     \n",
       "13  0.753329  0.007784      False     False              NaN   False  attn     \n",
       "14  0.694860  0.010384      False     False              NaN   False  attn     \n",
       "15  0.831972  0.003256      False     False              NaN   False  attn     \n",
       "16  0.835573  0.010908      False     False              NaN    True  attn     \n",
       "17  0.759670  0.006823      False     False              NaN    True  attn     \n",
       "18  0.729156  0.013789      False     False              NaN    True  attn     \n",
       "19  0.720870  0.005167      False     False              NaN    True  attn     \n",
       "20  0.756370  0.007211       True     False             0.10   False  attn     \n",
       "21  0.792255  0.002184       True     False             0.10   False  attn     \n",
       "22  0.786427  0.002008       True     False             0.10   False  attn     \n",
       "23  0.873594  0.016944       True     False             0.10   False  attn     \n",
       "24  0.774104  0.001855       True     False             0.10   False  attn     \n",
       "25  0.789502  0.002433       True     False             0.10   False  attn     \n",
       "26  0.867947  0.010989       True     False             0.10   False  attn     \n",
       "27  0.761983  0.004556       True     False             0.10   False  attn     \n",
       "28  0.794380  0.002116       True     False             0.10   False  attn     \n",
       "29  0.894071  0.003426       True     False             0.10   False  attn     \n",
       "30  0.793121  0.003726       True     False             0.10   False  attn     \n",
       "31  0.797083  0.004061       True     False             0.10   False  attn     \n",
       "32  0.829317  0.004322      False      True             0.10   False  attn     \n",
       "33  0.729738  0.005943      False      True             0.10   False  attn     \n",
       "34  0.787233  0.001456      False      True             0.10   False  attn     \n",
       "35  0.782670  0.002296      False      True             0.10   False  attn     \n",
       "36  0.724866  0.003635       True     False             0.02   False  attn     \n",
       "37  0.774966  0.002813       True     False             0.02   False  attn     \n",
       "38  0.699866  0.023450       True     False             0.02   False  attn     \n",
       "39  0.773791  0.041005       True     False             0.02   False  attn     \n",
       "40  0.729016  0.010541       True     False             0.02   False  attn     \n",
       "41  0.724120  0.011153       True     False             0.02   False  attn     \n",
       "42  0.844473  0.011185       True     False             0.02   False  attn     \n",
       "43  0.742710  0.006265       True     False             0.02   False  attn     \n",
       "44  0.737982  0.009524       True     False             0.02   False  attn     \n",
       "45  0.843931  0.013971       True     False             0.02   False  attn     \n",
       "46  0.780288  0.002248       True     False             0.02   False  attn     \n",
       "47  0.782912  0.003641       True     False             0.02   False  attn     \n",
       "48  0.807556  0.006384      False      True             0.02   False  attn     \n",
       "49  0.711287  0.003618      False      True             0.02   False  attn     \n",
       "50  0.763304  0.001136      False      True             0.02   False  attn     \n",
       "51  0.745647  0.002570      False      True             0.02   False  attn     \n",
       "52  0.883985  0.009676       True     False             0.10    True  attn     \n",
       "53  0.770197  0.002738       True     False             0.10    True  attn     \n",
       "54  0.779573  0.001816       True     False             0.10    True  attn     \n",
       "55  0.795438  0.002163       True     False             0.10    True  attn     \n",
       "\n",
       "    earliness_mean  earliness_std  precision_mean  precision_std  \n",
       "0            4.055       0.079844        0.531098       0.022103  \n",
       "1            3.755       1.246044        0.244710       0.007552  \n",
       "2            3.220       1.072147        0.270946       0.012086  \n",
       "3            4.235       1.187908        0.227974       0.008421  \n",
       "4            3.455       1.525451        0.254022       0.010373  \n",
       "5            4.625       1.212693        0.321037       0.010810  \n",
       "6            2.680       1.153581        0.252045       0.018139  \n",
       "7            4.115       0.273633        0.243277       0.004945  \n",
       "8            1.280       0.613239        0.327414       0.035004  \n",
       "9            2.930       0.918286        0.241868       0.008305  \n",
       "10           2.770       0.130384        0.363912       0.008250  \n",
       "11           3.090       0.915253        0.205877       0.007570  \n",
       "12           2.825       1.902383        0.315987       0.049013  \n",
       "13           3.590       1.004459        0.283009       0.010858  \n",
       "14           0.905       0.942802        0.237810       0.009988  \n",
       "15           3.365       0.236907        0.356650       0.006018  \n",
       "16           0.955       0.844911        0.368059       0.029036  \n",
       "17           1.470       1.178638        0.285484       0.011018  \n",
       "18           1.225       1.282332        0.265123       0.010473  \n",
       "19           3.365       0.449514        0.252695       0.004614  \n",
       "20           3.375       1.219375        0.283055       0.010133  \n",
       "21           4.275       0.269838        0.314837       0.004383  \n",
       "22           4.085       0.271339        0.294975       0.002620  \n",
       "23           2.850       1.014889        0.446497       0.040464  \n",
       "24           2.765       0.621892        0.293611       0.003086  \n",
       "25           3.495       0.361594        0.303076       0.006224  \n",
       "26           3.140       1.011589        0.410842       0.035182  \n",
       "27           2.855       0.733655        0.286573       0.006346  \n",
       "28           2.675       0.446864        0.303278       0.005638  \n",
       "29           1.500       0.546580        0.487837       0.020819  \n",
       "30           3.980       0.758782        0.321017       0.008107  \n",
       "31           2.405       0.361594        0.304273       0.005657  \n",
       "32           6.875       0.334944        0.399529       0.017069  \n",
       "33           8.320       0.499187        0.256771       0.003817  \n",
       "34           4.135       0.266693        0.302023       0.003675  \n",
       "35           4.825       0.298957        0.295055       0.004809  \n",
       "36           5.555       0.649134        0.252831       0.003548  \n",
       "37           6.220       0.566348        0.301949       0.005359  \n",
       "38           3.645       1.332807        0.234105       0.015405  \n",
       "39           3.195       1.059835        0.303952       0.071046  \n",
       "40           1.910       0.806730        0.264214       0.006479  \n",
       "41           3.915       0.277038        0.246401       0.009146  \n",
       "42           2.175       0.653357        0.380352       0.021274  \n",
       "43           4.690       1.025549        0.263921       0.008179  \n",
       "44           1.555       1.337722        0.255292       0.008648  \n",
       "45           2.865       1.803781        0.386290       0.037804  \n",
       "46           5.125       0.257391        0.294014       0.004669  \n",
       "47           1.245       0.366742        0.305844       0.004670  \n",
       "48           8.575       0.729512        0.327786       0.011386  \n",
       "49           8.350       0.385276        0.255480       0.005198  \n",
       "50           5.455       0.156525        0.287224       0.003679  \n",
       "51           5.305       0.207214        0.267340       0.003602  \n",
       "52           2.000       1.470225        0.479475       0.019430  \n",
       "53           3.260       0.687750        0.293600       0.004886  \n",
       "54           2.840       0.370220        0.303068       0.002332  \n",
       "55           3.450       0.244310        0.309495       0.006718  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1a1ecf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'level_0': 'task'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d0d19529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>level_1</th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>internal</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.917711</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.055</td>\n",
       "      <td>0.079844</td>\n",
       "      <td>0.531098</td>\n",
       "      <td>0.022103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internal</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.698006</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.755</td>\n",
       "      <td>1.246044</td>\n",
       "      <td>0.244710</td>\n",
       "      <td>0.007552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>internal</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.220</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>0.270946</td>\n",
       "      <td>0.012086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internal</td>\n",
       "      <td>4</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.235</td>\n",
       "      <td>1.187908</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0.008421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>internal</td>\n",
       "      <td>5</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.455</td>\n",
       "      <td>1.525451</td>\n",
       "      <td>0.254022</td>\n",
       "      <td>0.010373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>internal</td>\n",
       "      <td>6</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.802502</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.625</td>\n",
       "      <td>1.212693</td>\n",
       "      <td>0.321037</td>\n",
       "      <td>0.010810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>internal</td>\n",
       "      <td>8</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.705312</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.680</td>\n",
       "      <td>1.153581</td>\n",
       "      <td>0.252045</td>\n",
       "      <td>0.018139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>internal</td>\n",
       "      <td>9</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.115</td>\n",
       "      <td>0.273633</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>internal</td>\n",
       "      <td>15</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.806525</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.613239</td>\n",
       "      <td>0.327414</td>\n",
       "      <td>0.035004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>internal</td>\n",
       "      <td>16</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.705568</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.918286</td>\n",
       "      <td>0.241868</td>\n",
       "      <td>0.008305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>internal</td>\n",
       "      <td>18</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.834322</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.770</td>\n",
       "      <td>0.130384</td>\n",
       "      <td>0.363912</td>\n",
       "      <td>0.008250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>internal</td>\n",
       "      <td>19</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.090</td>\n",
       "      <td>0.915253</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.007570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>internal</td>\n",
       "      <td>20</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.798606</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.825</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>0.049013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>internal</td>\n",
       "      <td>21</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.753329</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.590</td>\n",
       "      <td>1.004459</td>\n",
       "      <td>0.283009</td>\n",
       "      <td>0.010858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>internal</td>\n",
       "      <td>23</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.694860</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.942802</td>\n",
       "      <td>0.237810</td>\n",
       "      <td>0.009988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>internal</td>\n",
       "      <td>24</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.831972</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.236907</td>\n",
       "      <td>0.356650</td>\n",
       "      <td>0.006018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pooled</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.835573</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.844911</td>\n",
       "      <td>0.368059</td>\n",
       "      <td>0.029036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pooled</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.470</td>\n",
       "      <td>1.178638</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.011018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pooled</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729156</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.282332</td>\n",
       "      <td>0.265123</td>\n",
       "      <td>0.010473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pooled</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.720870</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.449514</td>\n",
       "      <td>0.252695</td>\n",
       "      <td>0.004614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1.219375</td>\n",
       "      <td>0.283055</td>\n",
       "      <td>0.010133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.792255</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.085</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.873594</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.850</td>\n",
       "      <td>1.014889</td>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.040464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>4</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.293611</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>5</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.495</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.303076</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>6</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.867947</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.140</td>\n",
       "      <td>1.011589</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.035182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>7</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>8</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.005638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>9</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.894071</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.546580</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.020819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>10</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>0.008107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>finetuning-10%</td>\n",
       "      <td>11</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.829317</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.334944</td>\n",
       "      <td>0.399529</td>\n",
       "      <td>0.017069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.729738</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.320</td>\n",
       "      <td>0.499187</td>\n",
       "      <td>0.256771</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.787233</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.135</td>\n",
       "      <td>0.266693</td>\n",
       "      <td>0.302023</td>\n",
       "      <td>0.003675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>internal-10%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.782670</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.825</td>\n",
       "      <td>0.298957</td>\n",
       "      <td>0.295055</td>\n",
       "      <td>0.004809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.724866</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.649134</td>\n",
       "      <td>0.252831</td>\n",
       "      <td>0.003548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774966</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.220</td>\n",
       "      <td>0.566348</td>\n",
       "      <td>0.301949</td>\n",
       "      <td>0.005359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.645</td>\n",
       "      <td>1.332807</td>\n",
       "      <td>0.234105</td>\n",
       "      <td>0.015405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.041005</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.195</td>\n",
       "      <td>1.059835</td>\n",
       "      <td>0.303952</td>\n",
       "      <td>0.071046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>4</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729016</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.806730</td>\n",
       "      <td>0.264214</td>\n",
       "      <td>0.006479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>5</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.724120</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.915</td>\n",
       "      <td>0.277038</td>\n",
       "      <td>0.246401</td>\n",
       "      <td>0.009146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>6</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.844473</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.175</td>\n",
       "      <td>0.653357</td>\n",
       "      <td>0.380352</td>\n",
       "      <td>0.021274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>7</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.742710</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.690</td>\n",
       "      <td>1.025549</td>\n",
       "      <td>0.263921</td>\n",
       "      <td>0.008179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>8</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.737982</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.337722</td>\n",
       "      <td>0.255292</td>\n",
       "      <td>0.008648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>9</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.843931</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.865</td>\n",
       "      <td>1.803781</td>\n",
       "      <td>0.386290</td>\n",
       "      <td>0.037804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>10</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.780288</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.125</td>\n",
       "      <td>0.257391</td>\n",
       "      <td>0.294014</td>\n",
       "      <td>0.004669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>finetuning-2%</td>\n",
       "      <td>11</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.782912</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.366742</td>\n",
       "      <td>0.305844</td>\n",
       "      <td>0.004670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.807556</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.575</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.327786</td>\n",
       "      <td>0.011386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.711287</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.350</td>\n",
       "      <td>0.385276</td>\n",
       "      <td>0.255480</td>\n",
       "      <td>0.005198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.763304</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.455</td>\n",
       "      <td>0.156525</td>\n",
       "      <td>0.287224</td>\n",
       "      <td>0.003679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>internal-2%</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.745647</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.305</td>\n",
       "      <td>0.207214</td>\n",
       "      <td>0.267340</td>\n",
       "      <td>0.003602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>0</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.883985</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.470225</td>\n",
       "      <td>0.479475</td>\n",
       "      <td>0.019430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>1</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.687750</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.004886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>2</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.779573</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.840</td>\n",
       "      <td>0.370220</td>\n",
       "      <td>0.303068</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>finetuning-10%-pooled</td>\n",
       "      <td>3</td>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.450</td>\n",
       "      <td>0.244310</td>\n",
       "      <td>0.309495</td>\n",
       "      <td>0.006718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     task  level_1         model_x train_dataset eval_dataset  \\\n",
       "0                internal        0  AttentionModel          aumc         aumc   \n",
       "1                internal        1  AttentionModel          aumc         eicu   \n",
       "2                internal        3  AttentionModel          aumc        hirid   \n",
       "3                internal        4  AttentionModel          aumc        mimic   \n",
       "4                internal        5  AttentionModel          eicu         aumc   \n",
       "5                internal        6  AttentionModel          eicu         eicu   \n",
       "6                internal        8  AttentionModel          eicu        hirid   \n",
       "7                internal        9  AttentionModel          eicu        mimic   \n",
       "8                internal       15  AttentionModel         hirid         aumc   \n",
       "9                internal       16  AttentionModel         hirid         eicu   \n",
       "10               internal       18  AttentionModel         hirid        hirid   \n",
       "11               internal       19  AttentionModel         hirid        mimic   \n",
       "12               internal       20  AttentionModel         mimic         aumc   \n",
       "13               internal       21  AttentionModel         mimic         eicu   \n",
       "14               internal       23  AttentionModel         mimic        hirid   \n",
       "15               internal       24  AttentionModel         mimic        mimic   \n",
       "16                 pooled        0  AttentionModel        pooled         aumc   \n",
       "17                 pooled        1  AttentionModel        pooled         eicu   \n",
       "18                 pooled        2  AttentionModel        pooled        hirid   \n",
       "19                 pooled        3  AttentionModel        pooled        mimic   \n",
       "20         finetuning-10%        0  AttentionModel          aumc         eicu   \n",
       "21         finetuning-10%        1  AttentionModel          aumc        hirid   \n",
       "22         finetuning-10%        2  AttentionModel          aumc        mimic   \n",
       "23         finetuning-10%        3  AttentionModel          eicu         aumc   \n",
       "24         finetuning-10%        4  AttentionModel          eicu        hirid   \n",
       "25         finetuning-10%        5  AttentionModel          eicu        mimic   \n",
       "26         finetuning-10%        6  AttentionModel         hirid         aumc   \n",
       "27         finetuning-10%        7  AttentionModel         hirid         eicu   \n",
       "28         finetuning-10%        8  AttentionModel         hirid        mimic   \n",
       "29         finetuning-10%        9  AttentionModel         mimic         aumc   \n",
       "30         finetuning-10%       10  AttentionModel         mimic         eicu   \n",
       "31         finetuning-10%       11  AttentionModel         mimic        hirid   \n",
       "32           internal-10%        0  AttentionModel          aumc         aumc   \n",
       "33           internal-10%        1  AttentionModel          eicu         eicu   \n",
       "34           internal-10%        2  AttentionModel         hirid        hirid   \n",
       "35           internal-10%        3  AttentionModel         mimic        mimic   \n",
       "36          finetuning-2%        0  AttentionModel          aumc         eicu   \n",
       "37          finetuning-2%        1  AttentionModel          aumc        hirid   \n",
       "38          finetuning-2%        2  AttentionModel          aumc        mimic   \n",
       "39          finetuning-2%        3  AttentionModel          eicu         aumc   \n",
       "40          finetuning-2%        4  AttentionModel          eicu        hirid   \n",
       "41          finetuning-2%        5  AttentionModel          eicu        mimic   \n",
       "42          finetuning-2%        6  AttentionModel         hirid         aumc   \n",
       "43          finetuning-2%        7  AttentionModel         hirid         eicu   \n",
       "44          finetuning-2%        8  AttentionModel         hirid        mimic   \n",
       "45          finetuning-2%        9  AttentionModel         mimic         aumc   \n",
       "46          finetuning-2%       10  AttentionModel         mimic         eicu   \n",
       "47          finetuning-2%       11  AttentionModel         mimic        hirid   \n",
       "48            internal-2%        0  AttentionModel          aumc         aumc   \n",
       "49            internal-2%        1  AttentionModel          eicu         eicu   \n",
       "50            internal-2%        2  AttentionModel         hirid        hirid   \n",
       "51            internal-2%        3  AttentionModel         mimic        mimic   \n",
       "52  finetuning-10%-pooled        0  AttentionModel        pooled         aumc   \n",
       "53  finetuning-10%-pooled        1  AttentionModel        pooled         eicu   \n",
       "54  finetuning-10%-pooled        2  AttentionModel        pooled        hirid   \n",
       "55  finetuning-10%-pooled        3  AttentionModel        pooled        mimic   \n",
       "\n",
       "    auc_mean   auc_std  finetuned  baseline  finetuning_size  pooled model_y  \\\n",
       "0   0.917711  0.003569      False     False              NaN   False  attn     \n",
       "1   0.698006  0.010686      False     False              NaN   False  attn     \n",
       "2   0.733108  0.013438      False     False              NaN   False  attn     \n",
       "3   0.685606  0.013133      False     False              NaN   False  attn     \n",
       "4   0.731821  0.007108      False     False              NaN   False  attn     \n",
       "5   0.802502  0.003536      False     False              NaN   False  attn     \n",
       "6   0.705312  0.026872      False     False              NaN   False  attn     \n",
       "7   0.714940  0.009202      False     False              NaN   False  attn     \n",
       "8   0.806525  0.014401      False     False              NaN   False  attn     \n",
       "9   0.705568  0.009348      False     False              NaN   False  attn     \n",
       "10  0.834322  0.002237      False     False              NaN   False  attn     \n",
       "11  0.648077  0.012512      False     False              NaN   False  attn     \n",
       "12  0.798606  0.022978      False     False              NaN   False  attn     \n",
       "13  0.753329  0.007784      False     False              NaN   False  attn     \n",
       "14  0.694860  0.010384      False     False              NaN   False  attn     \n",
       "15  0.831972  0.003256      False     False              NaN   False  attn     \n",
       "16  0.835573  0.010908      False     False              NaN    True  attn     \n",
       "17  0.759670  0.006823      False     False              NaN    True  attn     \n",
       "18  0.729156  0.013789      False     False              NaN    True  attn     \n",
       "19  0.720870  0.005167      False     False              NaN    True  attn     \n",
       "20  0.756370  0.007211       True     False             0.10   False  attn     \n",
       "21  0.792255  0.002184       True     False             0.10   False  attn     \n",
       "22  0.786427  0.002008       True     False             0.10   False  attn     \n",
       "23  0.873594  0.016944       True     False             0.10   False  attn     \n",
       "24  0.774104  0.001855       True     False             0.10   False  attn     \n",
       "25  0.789502  0.002433       True     False             0.10   False  attn     \n",
       "26  0.867947  0.010989       True     False             0.10   False  attn     \n",
       "27  0.761983  0.004556       True     False             0.10   False  attn     \n",
       "28  0.794380  0.002116       True     False             0.10   False  attn     \n",
       "29  0.894071  0.003426       True     False             0.10   False  attn     \n",
       "30  0.793121  0.003726       True     False             0.10   False  attn     \n",
       "31  0.797083  0.004061       True     False             0.10   False  attn     \n",
       "32  0.829317  0.004322      False      True             0.10   False  attn     \n",
       "33  0.729738  0.005943      False      True             0.10   False  attn     \n",
       "34  0.787233  0.001456      False      True             0.10   False  attn     \n",
       "35  0.782670  0.002296      False      True             0.10   False  attn     \n",
       "36  0.724866  0.003635       True     False             0.02   False  attn     \n",
       "37  0.774966  0.002813       True     False             0.02   False  attn     \n",
       "38  0.699866  0.023450       True     False             0.02   False  attn     \n",
       "39  0.773791  0.041005       True     False             0.02   False  attn     \n",
       "40  0.729016  0.010541       True     False             0.02   False  attn     \n",
       "41  0.724120  0.011153       True     False             0.02   False  attn     \n",
       "42  0.844473  0.011185       True     False             0.02   False  attn     \n",
       "43  0.742710  0.006265       True     False             0.02   False  attn     \n",
       "44  0.737982  0.009524       True     False             0.02   False  attn     \n",
       "45  0.843931  0.013971       True     False             0.02   False  attn     \n",
       "46  0.780288  0.002248       True     False             0.02   False  attn     \n",
       "47  0.782912  0.003641       True     False             0.02   False  attn     \n",
       "48  0.807556  0.006384      False      True             0.02   False  attn     \n",
       "49  0.711287  0.003618      False      True             0.02   False  attn     \n",
       "50  0.763304  0.001136      False      True             0.02   False  attn     \n",
       "51  0.745647  0.002570      False      True             0.02   False  attn     \n",
       "52  0.883985  0.009676       True     False             0.10    True  attn     \n",
       "53  0.770197  0.002738       True     False             0.10    True  attn     \n",
       "54  0.779573  0.001816       True     False             0.10    True  attn     \n",
       "55  0.795438  0.002163       True     False             0.10    True  attn     \n",
       "\n",
       "    earliness_mean  earliness_std  precision_mean  precision_std  \n",
       "0            4.055       0.079844        0.531098       0.022103  \n",
       "1            3.755       1.246044        0.244710       0.007552  \n",
       "2            3.220       1.072147        0.270946       0.012086  \n",
       "3            4.235       1.187908        0.227974       0.008421  \n",
       "4            3.455       1.525451        0.254022       0.010373  \n",
       "5            4.625       1.212693        0.321037       0.010810  \n",
       "6            2.680       1.153581        0.252045       0.018139  \n",
       "7            4.115       0.273633        0.243277       0.004945  \n",
       "8            1.280       0.613239        0.327414       0.035004  \n",
       "9            2.930       0.918286        0.241868       0.008305  \n",
       "10           2.770       0.130384        0.363912       0.008250  \n",
       "11           3.090       0.915253        0.205877       0.007570  \n",
       "12           2.825       1.902383        0.315987       0.049013  \n",
       "13           3.590       1.004459        0.283009       0.010858  \n",
       "14           0.905       0.942802        0.237810       0.009988  \n",
       "15           3.365       0.236907        0.356650       0.006018  \n",
       "16           0.955       0.844911        0.368059       0.029036  \n",
       "17           1.470       1.178638        0.285484       0.011018  \n",
       "18           1.225       1.282332        0.265123       0.010473  \n",
       "19           3.365       0.449514        0.252695       0.004614  \n",
       "20           3.375       1.219375        0.283055       0.010133  \n",
       "21           4.275       0.269838        0.314837       0.004383  \n",
       "22           4.085       0.271339        0.294975       0.002620  \n",
       "23           2.850       1.014889        0.446497       0.040464  \n",
       "24           2.765       0.621892        0.293611       0.003086  \n",
       "25           3.495       0.361594        0.303076       0.006224  \n",
       "26           3.140       1.011589        0.410842       0.035182  \n",
       "27           2.855       0.733655        0.286573       0.006346  \n",
       "28           2.675       0.446864        0.303278       0.005638  \n",
       "29           1.500       0.546580        0.487837       0.020819  \n",
       "30           3.980       0.758782        0.321017       0.008107  \n",
       "31           2.405       0.361594        0.304273       0.005657  \n",
       "32           6.875       0.334944        0.399529       0.017069  \n",
       "33           8.320       0.499187        0.256771       0.003817  \n",
       "34           4.135       0.266693        0.302023       0.003675  \n",
       "35           4.825       0.298957        0.295055       0.004809  \n",
       "36           5.555       0.649134        0.252831       0.003548  \n",
       "37           6.220       0.566348        0.301949       0.005359  \n",
       "38           3.645       1.332807        0.234105       0.015405  \n",
       "39           3.195       1.059835        0.303952       0.071046  \n",
       "40           1.910       0.806730        0.264214       0.006479  \n",
       "41           3.915       0.277038        0.246401       0.009146  \n",
       "42           2.175       0.653357        0.380352       0.021274  \n",
       "43           4.690       1.025549        0.263921       0.008179  \n",
       "44           1.555       1.337722        0.255292       0.008648  \n",
       "45           2.865       1.803781        0.386290       0.037804  \n",
       "46           5.125       0.257391        0.294014       0.004669  \n",
       "47           1.245       0.366742        0.305844       0.004670  \n",
       "48           8.575       0.729512        0.327786       0.011386  \n",
       "49           8.350       0.385276        0.255480       0.005198  \n",
       "50           5.455       0.156525        0.287224       0.003679  \n",
       "51           5.305       0.207214        0.267340       0.003602  \n",
       "52           2.000       1.470225        0.479475       0.019430  \n",
       "53           3.260       0.687750        0.293600       0.004886  \n",
       "54           2.840       0.370220        0.303068       0.002332  \n",
       "55           3.450       0.244310        0.309495       0.006718  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6f83d3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='task', ylabel='auc_mean'>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.pointplot(x='task', y='auc_mean', data=df, join=False, capsize=0.2)\n",
    "# wrong labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8f50819c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 3.70375,\n",
       " 'pw': 3.0066666666666664,\n",
       " 'po': 1.7537500000000001,\n",
       " 'ft10': 3.116666666666667,\n",
       " 'int10': 6.03875,\n",
       " 'ft2': 3.507916666666667,\n",
       " 'int2': 6.921249999999999}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7dba7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measure(df, func, measure):\n",
    "    return getattr(df, func)()[measure]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "923de1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3095324987.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return getattr(df, func)()[measure]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8080261443790046"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_measure(df_int, 'std', 'earliness_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b54c442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['int', 'pw', 'po', 'ft10', 'int10', 'ft2', 'int2', 'ft10po']\n",
    "df_ = [df_int, df_pw, df_po, df_ft10, df_base10, df_ft2, df_base2, df_ft10po]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e192ef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3095324987.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return getattr(df, func)()[measure]\n"
     ]
    }
   ],
   "source": [
    "earliness_means = {}\n",
    "earliness_stds = {}\n",
    "for key, _df in zip(keys, df_):\n",
    "    earliness_means[key] = get_measure(_df, 'mean', 'earliness_mean')\n",
    "    earliness_stds[key] = get_measure(_df, 'std', 'earliness_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "74974e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 3.70375,\n",
       " 'pw': 3.0066666666666664,\n",
       " 'po': 1.7537500000000001,\n",
       " 'ft10': 3.116666666666667,\n",
       " 'int10': 6.03875,\n",
       " 'ft2': 3.507916666666667,\n",
       " 'int2': 6.921249999999999,\n",
       " 'ft10po': 2.8875}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliness_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "95b4ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3095324987.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return getattr(df, func)()[measure]\n"
     ]
    }
   ],
   "source": [
    "precision_means = {}\n",
    "precision_stds = {}\n",
    "for key, _df in zip(keys, df_):\n",
    "    precision_means[key] = get_measure(_df, 'mean', 'precision_mean')\n",
    "    precision_stds[key] = get_measure(_df, 'std', 'precision_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0959b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3095324987.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return getattr(df, func)()[measure]\n"
     ]
    }
   ],
   "source": [
    "auc_means = {}\n",
    "auc_stds = {}\n",
    "for key, _df in zip(keys, df_):\n",
    "    auc_means[key] = get_measure(_df, 'mean', 'auc_mean')\n",
    "    auc_stds[key] = get_measure(_df, 'std', 'auc_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c6dd9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys_sorted = ['pw', 'po', 'ft2', 'ft10', 'ft10po', 'int']\n",
    "# keys_sorted = ['pw', 'po', 'ft2', 'int2', 'ft10', 'ft10po', 'int10', 'int']\n",
    "keys_sorted = ['pw', 'po', 'ft2', 'ft10', 'int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "05c0d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_m = [auc_means[k] for k in keys_sorted]\n",
    "aucs_s = [auc_stds[k] for k in keys_sorted]\n",
    "earliness_m = [earliness_means[k] for k in keys_sorted]\n",
    "earliness_s = [earliness_stds[k] for k in keys_sorted]\n",
    "precision_m = [precision_means[k] for k in keys_sorted]\n",
    "precision_s = [precision_stds[k] for k in keys_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2a5f0e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 0.8466267443625092,\n",
       " 'pw': 0.7229798509928985,\n",
       " 'po': 0.7613171965323347,\n",
       " 'ft10': 0.8067364255118971,\n",
       " 'int10': 0.7822397449362146,\n",
       " 'ft2': 0.7632434823574795,\n",
       " 'int2': 0.7569484196682784,\n",
       " 'ft10po': 0.8072982248917016}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "51f49fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e0ad691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7229798509928985,\n",
       " 0.7613171965323347,\n",
       " 0.7632434823574795,\n",
       " 0.7569484196682784,\n",
       " 0.8044610478536885,\n",
       " 0.7822397449362146,\n",
       " 0.8466267443625092]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2d92f07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3380255286.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_int['task'] = 'internal'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3380255286.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pw['task'] = 'pair-wise'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3380255286.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_po['task'] = 'pooled'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3380255286.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft10['task'] = 'ft-10%'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3380255286.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft2['task'] = 'ft-2%'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3380255286.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft10po['task'] = 'ft-10%-pooled'\n"
     ]
    }
   ],
   "source": [
    "df_int['task'] = 'internal'\n",
    "df_pw['task'] = 'pair-wise' \n",
    "df_po['task'] = 'pooled'\n",
    "df_ft10['task'] = 'ft-10%' \n",
    "df_ft2['task'] = 'ft-2%'\n",
    "df_ft10po['task'] = 'ft-10%-pooled' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c13c74fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw values:\n",
    "df_tot = pd.concat([df_pw, df_po, df_ft2, df_ft10, df_ft10po, df_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcb830cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28296/1196983362.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_ft10po.mean()['precision_mean']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.346409378294573"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft10po.mean()['precision_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b5f1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28296/3433370938.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_int.mean()['precision_mean']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3931742786496343"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int.mean()['precision_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5979283b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'x' and 'y' must have the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#aucs = [auc_pw_mean, auc_po_mean, auc_ft2_mean, auc_base2_mean, auc_ft10_mean, auc_base10_mean, auc_int_mean]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#stds = [auc_pw_std, auc_po_std, auc_ft2_std, auc_base2_std, auc_ft10_std, auc_base10_std, auc_int_std]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair-wise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mft-2\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mft-10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mft-10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-pooled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m \u001b[43max1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maucs_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myerr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maucs_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sns\u001b[38;5;241m.\u001b[39mstripplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdf_tot, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, ax\u001b[38;5;241m=\u001b[39max1,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     11\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0.60\u001b[39m,\u001b[38;5;241m0.93\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/matplotlib/axes/_axes.py:3329\u001b[0m, in \u001b[0;36mAxes.errorbar\u001b[0;34m(self, x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, **kwargs)\u001b[0m\n\u001b[1;32m   3327\u001b[0m x, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(x, y)  \u001b[38;5;66;03m# Make sure all the args are iterable.\u001b[39;00m\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[0;32m-> 3329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must have the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(errorevery, Integral):\n\u001b[1;32m   3332\u001b[0m     errorevery \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, errorevery)\n",
      "\u001b[0;31mValueError\u001b[0m: 'x' and 'y' must have the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,10))\n",
    "ax1 = plt.subplot(311)\n",
    "\n",
    "x_pos = np.arange(6) #7\n",
    "#aucs = [auc_pw_mean, auc_po_mean, auc_ft2_mean, auc_base2_mean, auc_ft10_mean, auc_base10_mean, auc_int_mean]\n",
    "#stds = [auc_pw_std, auc_po_std, auc_ft2_std, auc_base2_std, auc_ft10_std, auc_base10_std, auc_int_std]\n",
    "names = ['pair-wise', 'pooled', 'ft-2%',  'ft-10%', 'ft-10%-pooled', 'internal']\n",
    "ax1.errorbar(x_pos, aucs_m, yerr=aucs_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "sns.stripplot(x=\"task\", y=\"auc_mean\", data=df_tot, jitter=0.1, ax=ax1,alpha=0.5)\n",
    "\n",
    "ax1.set_ylim(0.60,0.93)\n",
    "\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(names)\n",
    "#ax[0].tight_layout()\n",
    "ax1.set_ylabel(f'AUROC')\n",
    "\n",
    "ax2 = plt.subplot(312, sharex= ax1)\n",
    "\n",
    "#plt.plot((2,3), (auc_ft2_mean, auc_base2_mean), '--', color='black')\n",
    "#plt.plot((4,5), (auc_ft10_mean, auc_base10_mean), '--', color='black')\n",
    "#plt.hlines(auc_int_mean, 0,6, color='black', linestyle='dotted')\n",
    "sns.stripplot(x=\"task\", y=\"precision_mean\", data=df_tot, jitter=0.1, ax=ax2, alpha=0.5)\n",
    "ax2.errorbar(x_pos, precision_m, yerr=precision_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "ax2.set_ylabel(f'PPV @ 80% Sensitivity')\n",
    "ax2.set_ylim(0.15,0.55)\n",
    "\n",
    "#ax[1].set_xticklabels(names)\n",
    "#ax[1].tight_layout()\n",
    "\n",
    "ax3 = plt.subplot(313, sharex= ax1)\n",
    "sns.stripplot(x=\"task\", y=\"earliness_mean\", data=df_tot, jitter=0.1, ax=ax3, alpha=0.5)\n",
    "ax3.errorbar(x_pos, earliness_m, yerr=earliness_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "ax3.set_ylabel(f'Earliness @ 80% Sensitivity')\n",
    "#ax[2].set_xticklabels(names)\n",
    "ax3.set_ylim(-0.2,6.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "x1 = np.arange(7)\n",
    "y1 = np.array([0.13] * 7)\n",
    "for i,j, n in zip(x1,y1, aucs_m):\n",
    "    ax1.annotate(f'{n:.2f}',xy=(i-0.1,j+0.5), size=12)\n",
    "y2 = np.array([0.18] * 7)\n",
    "\n",
    "for i,j, n in zip(x1,y2, precision_m):\n",
    "    ax2.annotate(f'{n:.2f}',xy=(i-0.1,j), size=12)\n",
    "\n",
    "y3 = np.array([0.2] * 7)\n",
    "for i,j, n in zip(x1,y3, earliness_m):\n",
    "    ax3.annotate(f'{n:.2f}',xy=(i-0.1,j), size=12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "606b8dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8072982248917016"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_tot = pd.concat([df_pw, df_po, df_ft2, df_ft10, df_ft10po, df_int])\n",
    "df_ft10po['auc_mean'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb989ec",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- csv of these results\n",
    "- plot with increasingly smaller finetuning split\n",
    "- check nemati paper again\n",
    "- how much data collection can be prevented using pretraining?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c6fda",
   "metadata": {},
   "source": [
    "# TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6e2ec799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_int['task'] = 'internal'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pw['task'] = 'pair-wise'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_po['task'] = 'pooled'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft10['task'] = 'ft-10%'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft2['task'] = 'ft-2%'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft10po['task'] = 'ft-10%-pooled'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_base10['task'] = 'internal-10%'\n",
      "/var/folders/vp/67rz7yg93fn4hrmg1q9wpbbm0000gn/T/ipykernel_28367/3085448672.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_base2['task'] = 'internal-2%'\n"
     ]
    }
   ],
   "source": [
    "df_int['task'] = 'internal'\n",
    "df_pw['task'] = 'pair-wise' \n",
    "df_po['task'] = 'pooled'\n",
    "df_ft10['task'] = 'ft-10%' \n",
    "df_ft2['task'] = 'ft-2%'\n",
    "df_ft10po['task'] = 'ft-10%-pooled' \n",
    "df_base10['task'] = 'internal-10%'\n",
    "df_base2['task'] = 'internal-2%'\n",
    "\n",
    "#df_tot2 = pd.concat([df_pw, df_po, df_ft2, df_base2, df_ft10, df_ft10po, df_base10, df_int])\n",
    "df_tot2 = pd.concat([df_pw, df_po, df_ft2, df_ft10, df_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e06db8f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tasl'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_base2 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_base2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtasl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pandas/core/frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4816\u001b[0m ):\n\u001b[1;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pandas/core/indexes/base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tasl'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_base2 = df_base2.drop(columns=['tasl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "21931338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>train_dataset</th>\n",
       "      <th>eval_dataset</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>baseline</th>\n",
       "      <th>finetuning_size</th>\n",
       "      <th>pooled</th>\n",
       "      <th>model_y</th>\n",
       "      <th>earliness_mean</th>\n",
       "      <th>earliness_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p1</th>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.698006</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.755</td>\n",
       "      <td>1.246044</td>\n",
       "      <td>0.244710</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.220</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>0.270946</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.235</td>\n",
       "      <td>1.187908</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.455</td>\n",
       "      <td>1.525451</td>\n",
       "      <td>0.254022</td>\n",
       "      <td>0.010373</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.705312</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.680</td>\n",
       "      <td>1.153581</td>\n",
       "      <td>0.252045</td>\n",
       "      <td>0.018139</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.115</td>\n",
       "      <td>0.273633</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.806525</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.613239</td>\n",
       "      <td>0.327414</td>\n",
       "      <td>0.035004</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.705568</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.918286</td>\n",
       "      <td>0.241868</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.090</td>\n",
       "      <td>0.915253</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.798606</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.825</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>0.049013</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.753329</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.590</td>\n",
       "      <td>1.004459</td>\n",
       "      <td>0.283009</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.694860</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.942802</td>\n",
       "      <td>0.237810</td>\n",
       "      <td>0.009988</td>\n",
       "      <td>pair-wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p2</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.835573</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.844911</td>\n",
       "      <td>0.368059</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.470</td>\n",
       "      <td>1.178638</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729156</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.282332</td>\n",
       "      <td>0.265123</td>\n",
       "      <td>0.010473</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.720870</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.449514</td>\n",
       "      <td>0.252695</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p5</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.724866</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.555</td>\n",
       "      <td>0.649134</td>\n",
       "      <td>0.252831</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774966</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.220</td>\n",
       "      <td>0.566348</td>\n",
       "      <td>0.301949</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.645</td>\n",
       "      <td>1.332807</td>\n",
       "      <td>0.234105</td>\n",
       "      <td>0.015405</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.041005</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.195</td>\n",
       "      <td>1.059835</td>\n",
       "      <td>0.303952</td>\n",
       "      <td>0.071046</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.729016</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.806730</td>\n",
       "      <td>0.264214</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.724120</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.915</td>\n",
       "      <td>0.277038</td>\n",
       "      <td>0.246401</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.844473</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.175</td>\n",
       "      <td>0.653357</td>\n",
       "      <td>0.380352</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.742710</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.690</td>\n",
       "      <td>1.025549</td>\n",
       "      <td>0.263921</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.737982</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.337722</td>\n",
       "      <td>0.255292</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.843931</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.865</td>\n",
       "      <td>1.803781</td>\n",
       "      <td>0.386290</td>\n",
       "      <td>0.037804</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.780288</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.125</td>\n",
       "      <td>0.257391</td>\n",
       "      <td>0.294014</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.782912</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.366742</td>\n",
       "      <td>0.305844</td>\n",
       "      <td>0.004670</td>\n",
       "      <td>ft-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p6</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.807556</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.575</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.327786</td>\n",
       "      <td>0.011386</td>\n",
       "      <td>internal-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.711287</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.350</td>\n",
       "      <td>0.385276</td>\n",
       "      <td>0.255480</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>internal-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.763304</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.455</td>\n",
       "      <td>0.156525</td>\n",
       "      <td>0.287224</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>internal-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.745647</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>5.305</td>\n",
       "      <td>0.207214</td>\n",
       "      <td>0.267340</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>internal-2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">p3</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1.219375</td>\n",
       "      <td>0.283055</td>\n",
       "      <td>0.010133</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.792255</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.085</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.873594</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.850</td>\n",
       "      <td>1.014889</td>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.040464</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.293611</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.495</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.303076</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.867947</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.140</td>\n",
       "      <td>1.011589</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.035182</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.855</td>\n",
       "      <td>0.733655</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.894071</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.546580</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.020819</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.797083</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>ft-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p7</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.883985</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.470225</td>\n",
       "      <td>0.479475</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>ft-10%-pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.260</td>\n",
       "      <td>0.687750</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>ft-10%-pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.779573</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.840</td>\n",
       "      <td>0.370220</td>\n",
       "      <td>0.303068</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>ft-10%-pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>pooled</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.795438</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.450</td>\n",
       "      <td>0.244310</td>\n",
       "      <td>0.309495</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>ft-10%-pooled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p4</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.829317</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.334944</td>\n",
       "      <td>0.399529</td>\n",
       "      <td>0.017069</td>\n",
       "      <td>internal-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.729738</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>8.320</td>\n",
       "      <td>0.499187</td>\n",
       "      <td>0.256771</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>internal-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.787233</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.135</td>\n",
       "      <td>0.266693</td>\n",
       "      <td>0.302023</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>internal-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.782670</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.825</td>\n",
       "      <td>0.298957</td>\n",
       "      <td>0.295055</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>internal-10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">p1</th>\n",
       "      <th>0</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>aumc</td>\n",
       "      <td>aumc</td>\n",
       "      <td>0.917711</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.055</td>\n",
       "      <td>0.079844</td>\n",
       "      <td>0.531098</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>eicu</td>\n",
       "      <td>eicu</td>\n",
       "      <td>0.802502</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>4.625</td>\n",
       "      <td>1.212693</td>\n",
       "      <td>0.321037</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>hirid</td>\n",
       "      <td>hirid</td>\n",
       "      <td>0.834322</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>2.770</td>\n",
       "      <td>0.130384</td>\n",
       "      <td>0.363912</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AttentionModel</td>\n",
       "      <td>mimic</td>\n",
       "      <td>mimic</td>\n",
       "      <td>0.831972</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>attn</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.236907</td>\n",
       "      <td>0.356650</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_x train_dataset eval_dataset  auc_mean   auc_std  \\\n",
       "p1 1   AttentionModel          aumc         eicu  0.698006  0.010686   \n",
       "   3   AttentionModel          aumc        hirid  0.733108  0.013438   \n",
       "   4   AttentionModel          aumc        mimic  0.685606  0.013133   \n",
       "   5   AttentionModel          eicu         aumc  0.731821  0.007108   \n",
       "   8   AttentionModel          eicu        hirid  0.705312  0.026872   \n",
       "   9   AttentionModel          eicu        mimic  0.714940  0.009202   \n",
       "   15  AttentionModel         hirid         aumc  0.806525  0.014401   \n",
       "   16  AttentionModel         hirid         eicu  0.705568  0.009348   \n",
       "   19  AttentionModel         hirid        mimic  0.648077  0.012512   \n",
       "   20  AttentionModel         mimic         aumc  0.798606  0.022978   \n",
       "   21  AttentionModel         mimic         eicu  0.753329  0.007784   \n",
       "   23  AttentionModel         mimic        hirid  0.694860  0.010384   \n",
       "p2 0   AttentionModel        pooled         aumc  0.835573  0.010908   \n",
       "   1   AttentionModel        pooled         eicu  0.759670  0.006823   \n",
       "   2   AttentionModel        pooled        hirid  0.729156  0.013789   \n",
       "   3   AttentionModel        pooled        mimic  0.720870  0.005167   \n",
       "p5 0   AttentionModel          aumc         eicu  0.724866  0.003635   \n",
       "   1   AttentionModel          aumc        hirid  0.774966  0.002813   \n",
       "   2   AttentionModel          aumc        mimic  0.699866  0.023450   \n",
       "   3   AttentionModel          eicu         aumc  0.773791  0.041005   \n",
       "   4   AttentionModel          eicu        hirid  0.729016  0.010541   \n",
       "   5   AttentionModel          eicu        mimic  0.724120  0.011153   \n",
       "   6   AttentionModel         hirid         aumc  0.844473  0.011185   \n",
       "   7   AttentionModel         hirid         eicu  0.742710  0.006265   \n",
       "   8   AttentionModel         hirid        mimic  0.737982  0.009524   \n",
       "   9   AttentionModel         mimic         aumc  0.843931  0.013971   \n",
       "   10  AttentionModel         mimic         eicu  0.780288  0.002248   \n",
       "   11  AttentionModel         mimic        hirid  0.782912  0.003641   \n",
       "p6 0   AttentionModel          aumc         aumc  0.807556  0.006384   \n",
       "   1   AttentionModel          eicu         eicu  0.711287  0.003618   \n",
       "   2   AttentionModel         hirid        hirid  0.763304  0.001136   \n",
       "   3   AttentionModel         mimic        mimic  0.745647  0.002570   \n",
       "p3 0   AttentionModel          aumc         eicu  0.756370  0.007211   \n",
       "   1   AttentionModel          aumc        hirid  0.792255  0.002184   \n",
       "   2   AttentionModel          aumc        mimic  0.786427  0.002008   \n",
       "   3   AttentionModel          eicu         aumc  0.873594  0.016944   \n",
       "   4   AttentionModel          eicu        hirid  0.774104  0.001855   \n",
       "   5   AttentionModel          eicu        mimic  0.789502  0.002433   \n",
       "   6   AttentionModel         hirid         aumc  0.867947  0.010989   \n",
       "   7   AttentionModel         hirid         eicu  0.761983  0.004556   \n",
       "   8   AttentionModel         hirid        mimic  0.794380  0.002116   \n",
       "   9   AttentionModel         mimic         aumc  0.894071  0.003426   \n",
       "   10  AttentionModel         mimic         eicu  0.793121  0.003726   \n",
       "   11  AttentionModel         mimic        hirid  0.797083  0.004061   \n",
       "p7 0   AttentionModel        pooled         aumc  0.883985  0.009676   \n",
       "   1   AttentionModel        pooled         eicu  0.770197  0.002738   \n",
       "   2   AttentionModel        pooled        hirid  0.779573  0.001816   \n",
       "   3   AttentionModel        pooled        mimic  0.795438  0.002163   \n",
       "p4 0   AttentionModel          aumc         aumc  0.829317  0.004322   \n",
       "   1   AttentionModel          eicu         eicu  0.729738  0.005943   \n",
       "   2   AttentionModel         hirid        hirid  0.787233  0.001456   \n",
       "   3   AttentionModel         mimic        mimic  0.782670  0.002296   \n",
       "p1 0   AttentionModel          aumc         aumc  0.917711  0.003569   \n",
       "   6   AttentionModel          eicu         eicu  0.802502  0.003536   \n",
       "   18  AttentionModel         hirid        hirid  0.834322  0.002237   \n",
       "   24  AttentionModel         mimic        mimic  0.831972  0.003256   \n",
       "\n",
       "       finetuned  baseline  finetuning_size  pooled model_y  earliness_mean  \\\n",
       "p1 1       False     False              NaN   False  attn             3.755   \n",
       "   3       False     False              NaN   False  attn             3.220   \n",
       "   4       False     False              NaN   False  attn             4.235   \n",
       "   5       False     False              NaN   False  attn             3.455   \n",
       "   8       False     False              NaN   False  attn             2.680   \n",
       "   9       False     False              NaN   False  attn             4.115   \n",
       "   15      False     False              NaN   False  attn             1.280   \n",
       "   16      False     False              NaN   False  attn             2.930   \n",
       "   19      False     False              NaN   False  attn             3.090   \n",
       "   20      False     False              NaN   False  attn             2.825   \n",
       "   21      False     False              NaN   False  attn             3.590   \n",
       "   23      False     False              NaN   False  attn             0.905   \n",
       "p2 0       False     False              NaN    True  attn             0.955   \n",
       "   1       False     False              NaN    True  attn             1.470   \n",
       "   2       False     False              NaN    True  attn             1.225   \n",
       "   3       False     False              NaN    True  attn             3.365   \n",
       "p5 0        True     False             0.02   False  attn             5.555   \n",
       "   1        True     False             0.02   False  attn             6.220   \n",
       "   2        True     False             0.02   False  attn             3.645   \n",
       "   3        True     False             0.02   False  attn             3.195   \n",
       "   4        True     False             0.02   False  attn             1.910   \n",
       "   5        True     False             0.02   False  attn             3.915   \n",
       "   6        True     False             0.02   False  attn             2.175   \n",
       "   7        True     False             0.02   False  attn             4.690   \n",
       "   8        True     False             0.02   False  attn             1.555   \n",
       "   9        True     False             0.02   False  attn             2.865   \n",
       "   10       True     False             0.02   False  attn             5.125   \n",
       "   11       True     False             0.02   False  attn             1.245   \n",
       "p6 0       False      True             0.02   False  attn             8.575   \n",
       "   1       False      True             0.02   False  attn             8.350   \n",
       "   2       False      True             0.02   False  attn             5.455   \n",
       "   3       False      True             0.02   False  attn             5.305   \n",
       "p3 0        True     False             0.10   False  attn             3.375   \n",
       "   1        True     False             0.10   False  attn             4.275   \n",
       "   2        True     False             0.10   False  attn             4.085   \n",
       "   3        True     False             0.10   False  attn             2.850   \n",
       "   4        True     False             0.10   False  attn             2.765   \n",
       "   5        True     False             0.10   False  attn             3.495   \n",
       "   6        True     False             0.10   False  attn             3.140   \n",
       "   7        True     False             0.10   False  attn             2.855   \n",
       "   8        True     False             0.10   False  attn             2.675   \n",
       "   9        True     False             0.10   False  attn             1.500   \n",
       "   10       True     False             0.10   False  attn             3.980   \n",
       "   11       True     False             0.10   False  attn             2.405   \n",
       "p7 0        True     False             0.10    True  attn             2.000   \n",
       "   1        True     False             0.10    True  attn             3.260   \n",
       "   2        True     False             0.10    True  attn             2.840   \n",
       "   3        True     False             0.10    True  attn             3.450   \n",
       "p4 0       False      True             0.10   False  attn             6.875   \n",
       "   1       False      True             0.10   False  attn             8.320   \n",
       "   2       False      True             0.10   False  attn             4.135   \n",
       "   3       False      True             0.10   False  attn             4.825   \n",
       "p1 0       False     False              NaN   False  attn             4.055   \n",
       "   6       False     False              NaN   False  attn             4.625   \n",
       "   18      False     False              NaN   False  attn             2.770   \n",
       "   24      False     False              NaN   False  attn             3.365   \n",
       "\n",
       "       earliness_std  precision_mean  precision_std           task  \n",
       "p1 1        1.246044        0.244710       0.007552      pair-wise  \n",
       "   3        1.072147        0.270946       0.012086      pair-wise  \n",
       "   4        1.187908        0.227974       0.008421      pair-wise  \n",
       "   5        1.525451        0.254022       0.010373      pair-wise  \n",
       "   8        1.153581        0.252045       0.018139      pair-wise  \n",
       "   9        0.273633        0.243277       0.004945      pair-wise  \n",
       "   15       0.613239        0.327414       0.035004      pair-wise  \n",
       "   16       0.918286        0.241868       0.008305      pair-wise  \n",
       "   19       0.915253        0.205877       0.007570      pair-wise  \n",
       "   20       1.902383        0.315987       0.049013      pair-wise  \n",
       "   21       1.004459        0.283009       0.010858      pair-wise  \n",
       "   23       0.942802        0.237810       0.009988      pair-wise  \n",
       "p2 0        0.844911        0.368059       0.029036         pooled  \n",
       "   1        1.178638        0.285484       0.011018         pooled  \n",
       "   2        1.282332        0.265123       0.010473         pooled  \n",
       "   3        0.449514        0.252695       0.004614         pooled  \n",
       "p5 0        0.649134        0.252831       0.003548          ft-2%  \n",
       "   1        0.566348        0.301949       0.005359          ft-2%  \n",
       "   2        1.332807        0.234105       0.015405          ft-2%  \n",
       "   3        1.059835        0.303952       0.071046          ft-2%  \n",
       "   4        0.806730        0.264214       0.006479          ft-2%  \n",
       "   5        0.277038        0.246401       0.009146          ft-2%  \n",
       "   6        0.653357        0.380352       0.021274          ft-2%  \n",
       "   7        1.025549        0.263921       0.008179          ft-2%  \n",
       "   8        1.337722        0.255292       0.008648          ft-2%  \n",
       "   9        1.803781        0.386290       0.037804          ft-2%  \n",
       "   10       0.257391        0.294014       0.004669          ft-2%  \n",
       "   11       0.366742        0.305844       0.004670          ft-2%  \n",
       "p6 0        0.729512        0.327786       0.011386    internal-2%  \n",
       "   1        0.385276        0.255480       0.005198    internal-2%  \n",
       "   2        0.156525        0.287224       0.003679    internal-2%  \n",
       "   3        0.207214        0.267340       0.003602    internal-2%  \n",
       "p3 0        1.219375        0.283055       0.010133         ft-10%  \n",
       "   1        0.269838        0.314837       0.004383         ft-10%  \n",
       "   2        0.271339        0.294975       0.002620         ft-10%  \n",
       "   3        1.014889        0.446497       0.040464         ft-10%  \n",
       "   4        0.621892        0.293611       0.003086         ft-10%  \n",
       "   5        0.361594        0.303076       0.006224         ft-10%  \n",
       "   6        1.011589        0.410842       0.035182         ft-10%  \n",
       "   7        0.733655        0.286573       0.006346         ft-10%  \n",
       "   8        0.446864        0.303278       0.005638         ft-10%  \n",
       "   9        0.546580        0.487837       0.020819         ft-10%  \n",
       "   10       0.758782        0.321017       0.008107         ft-10%  \n",
       "   11       0.361594        0.304273       0.005657         ft-10%  \n",
       "p7 0        1.470225        0.479475       0.019430  ft-10%-pooled  \n",
       "   1        0.687750        0.293600       0.004886  ft-10%-pooled  \n",
       "   2        0.370220        0.303068       0.002332  ft-10%-pooled  \n",
       "   3        0.244310        0.309495       0.006718  ft-10%-pooled  \n",
       "p4 0        0.334944        0.399529       0.017069   internal-10%  \n",
       "   1        0.499187        0.256771       0.003817   internal-10%  \n",
       "   2        0.266693        0.302023       0.003675   internal-10%  \n",
       "   3        0.298957        0.295055       0.004809   internal-10%  \n",
       "p1 0        0.079844        0.531098       0.022103       internal  \n",
       "   6        1.212693        0.321037       0.010810       internal  \n",
       "   18       0.130384        0.363912       0.008250       internal  \n",
       "   24       0.236907        0.356650       0.006018       internal  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bafaa90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7229798509928985,\n",
       " 0.7613171965323347,\n",
       " 0.7632434823574795,\n",
       " 0.8067364255118971,\n",
       " 0.8466267443625092]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dbe8582e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAALICAYAAAAqrmQHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAACd2UlEQVR4nOzdeXxcdbn48c8zk5ksk6VJk6b7SnfKGspSsICAYC0gKpsoUK7IT3G5143+9ELAq6Be9edV7lWU3IsLIOBVWovstOzQFErbdF8oTZe0aZt9nZnn98c5CZM0SZM2Z2aSPO/Xa14953u2Z76dzDPnnO/5fkVVMcYYY5KNL9EBGGOMMV2xBGWMMSYpWYIyxhiTlCxBGWOMSUqWoIwxxiSllEQH0F/y8/N14sSJiQ7DGGNMH61atapSVQs6lw+aBDVx4kRKS0sTHYYxxpg+EpGdXZXbJT5jjDFJyRKUMcaYpGQJyhhjTFIaNPegjDHGeGP/zhr276wlJeBj9LRhZA9Pj8txLUEZY4zp1qG99by/prJ9vu5wMyd/dByBVL/nx7ZLfMYYY7pVvb+hw3w0EqWmsjEux7YEZYwxpltpmcEjytKzjizzgiUoY4wx3RoxMYthhRkA+PzCmGm5ZGTHJ0HZPShjjDHd8vt9TJs7kpamMD6/kBLw/t5TG0tQxhhjjiqYFv90YZf4jDHGJCVLUMYYY5KSJShjjDFJydMEJSKXisgmEdkqInd0sXyCiLwgImtEZLmIjI1ZdqOIbHFfN3oZpzHGmOTjWYISET9wP3AZMAu4TkRmdVrt34Hfq+pJwD3Ave62ecBdwJnAXOAuEcn1KlZjjDHJx8szqLnAVlXdrqotwKPAFZ3WmQW86E6/FLP8Y8BzqnpIVQ8DzwGXehirMcaYJONlghoD7IqZL3fLYr0HXOVOfxLIEpHhvdwWEblVREpFpPTAgQP9FrgxxpjES/RzUN8EfiUiNwEvA7uBSG83VtUHgAcAioqK1IsAjTGDQ9PmzTSUlkJUST/5ZNLnnJjokMxReHkGtRsYFzM/1i1rp6p7VPUqVT0V+K5bVtWbbY0xprfClZXUPvsckYOHiBw+TN3y5bSUlyc6rKRRXFyMiPTbq7i4uF/iElVvTjxEJAXYDHwUJ7msBK5X1bKYdfKBQ6oaFZEfABFVvdNtJLEKOM1d9R3gdFU91N3xioqKtLS01JP3YowZ2BpXr6bulVc7lGUUnU7o7LMTFNHAc/755wOwfPnyft+3iKxS1aLO5Z6dQalqGLgdeAbYADymqmUico+IXO6udj6wSUQ2A4XAD9xtDwHfx0lqK4F7ekpOxhjTE39+wRFlKfn5CYjE9IWn96BU9SngqU5ld8ZMPwE80c22JUCJl/EZY4aG4NgxZJxxBo2r30WjUdJnzyZ4wgmJDsscRaIbSRhjTFyEzjqTjKLTQRUJBBIdjukFS1DGmCFDUuwrbyCxvviMMcYkJUtQxhhjkpIlKGOMMUnJEpQxxpikZAnKGGNMUrImLcaYQU+jURpXraJ52zb8OTlknHUWKbk2gk+yszMoY8yg1/juu9S/+RbhA5U0b91GzdKlaDSa6LDMUViCMsYMei07dnSYj1TXEDlkvaclO0tQxpgBTyMRmjZtoqG0lPDhw0cs93e6nCeBFHxZWfEKzxwjuwdljBnwapYto2XnBwDUv/02w668ksDo0e3LM848k/CBA4QPVCKBAJkfOQ9famqiwjW9ZAnKGDOghSsr25MTAJEoje+91yFB+TMzyb32WiLV1fjS05FgMAGRmr6yBGWMGdhEelcG+HNyPA7G9Ce7B2WMGdBShg8nOHFi+7yk+Ek/+eTEBWT6jZ1BGWMGvOyPX0bztm1E6+pInTwZ/7BhiQ7J9ANLUMaYAU/8ftKmTUt0GKaf2SU+Y4wxSckSlDHGmKRkCcoYY0xS8jRBicilIrJJRLaKyB1dLB8vIi+JyLsiskZEPu6WTxSRRhFZ7b5+7WWcxhhjko9njSRExA/cD1wMlAMrRWSJqq6PWe17wGOq+l8iMgt4CpjoLtumqqd4FZ8xxpjk5uUZ1Fxgq6puV9UW4FHgik7rKJDtTucAezyMxxgzSBUXFyMi/fYqLi5O9FtKOvX19axcuZKysrK4HVNU1Zsdi3wauFRV/8md/xxwpqreHrPOKOBZIBcIARep6ioRmQiUAZuBGuB7qvpKT8crKirS0tJST96LMWbgO//88wFYvnx5QuMYiOrr6xk+fDjNzc1MmDCBsrIyQqFQv+1fRFapalHn8kQ3krgO+B9VHQt8HPiDiPiAvcB4VT0V+BfgYRHJ7ryxiNwqIqUiUnrgwIG4Bm5Mf2qJtLC9ejuVjZXdrrOrdhcvfPACb+x5g4bWhjhGZ4a6RYsW0dLSAkBFRQW33HJLXI7rZYLaDYyLmR/rlsW6BXgMQFXfANKAfFVtVtWDbvkqYBtwxFN4qvqAqhapalFBQYEHb8EY71U2VvKH9X/g6R1P89imx3h196tHrPNBzQf8fdvf2XRoE+/uf5e/bv0rUbUB94z3SkpKWLZsGW1X25qamli6dCklJSWeH9vLBLUSmCoik0QkCFwLLOm0zgfARwFEZCZOgjogIgVuIwtEZDIwFdjuYazGJMw7Fe/QHGlun197YC21LbUd1tlwaAPKh5fjq5ur2Vu/N24xmqFr8eLF1NfXdyhraGhg8eLFnh/bswSlqmHgduAZYANOa70yEblHRC53V/sG8AUReQ94BLhJnTT9EWCNiKwGngBuU1Ub/tIMSrHJCUDRI8rSU9KP2C7Nn+ZpXMYA3HvvvUfcb8rIyOC+++7z/Nie9sWnqk/hNB2PLbszZno9MK+L7f4C/MXL2IxJFjPyZrCrdlf7/IiMEeSn53dY5+SCk9lRvYP61vr2bYanD49rnGZoWrRoEc888wyPP/44qkpaWhoLFy7k5ptv9vzY1lmsSazWJji4BVLSIG8K+BLdbif+puZOJeALsK16G9nBbObkzzlinZzUHK6feT27a3eTEchgRMaIBERqhqqSkhKefPJJmpubKSws5MEHH4zLcS1BmcRprIJ3fg8t7vXtYePhlOu7HWxuMJuYM5GJORN7XCfgCxx1HWOORWtzhH3bq2lpDJM3OkTuyI6X9EKhEHPmzGH9+vUsW7asX5uY98QSlEmcPe98mJwAqj6Aw+9D3qSEhWTMUKOqbHxjL421TjPyyvJaRp+Qy/CxmWRkB9vXC4VCnHHGGcyePTtusVmCMokTCR9ZFu2izBjjmfqqlvbkFAlHqSyv49CeeoaPyWT42EymnJq4y8lD74K/SR6jTgZfzG+k9FzItbMnY+IpJehrv6xeX9VMuCWCz++khoPlddQeakpcbAk7sjFZhXD6TVCxzmkkMepk8NtH0ph4SgsFKJyUTcX2aqIRxef3kZX34SMMrc2RhMVm3wYmsTILIPOCREdhzJA2YfZwCsZlcWhPHbs3V7W3UwqkppBTcOQzePFiCcoYYwwZ2UEysvPIGp7GgQ/qSAn4GDklB39K4u4EWYIyxhjTLqcgg5yCjESHAVgjCWOMMUnKEpQxxpikZAnKGGNMUrIEZYwxJilZgjLGGJOULEEZY4xJSpagjDHGJCVLUOa4FRcXIyL99iouLk70WzKDUH19PStXrqSsrCzRoZheEmeE9YGvqKhIS0tL+2Vf9c1hKuuaKcxOIy3g75d9DnXnn38+AMuXL09oHGZoqq+vZ/jw4TQ3NzNhwgTKysriNqbRYOHl37CIrFLVos7ldgbVycZ9NTz46g7+953dPPjqDj442JDokIwxx2nRokW0tDhDSlRUVHDLLbckOCLTG5agYqgqL28+QCTqnFW2hKO8vOVAgqMyg51dIvVWSUkJy5Yto+1qUVNTE0uXLqWkpCTBkZmjsQQVI6rQ2BLtUFbfbAPoGW8VFxejqj2+5s+fz/z584+6nqpagupk8eLF1NfXdyhraGhg8eLFCYpo4GlpDDM2axbT8s5i66r9cRuCw9MEJSKXisgmEdkqInd0sXy8iLwkIu+KyBoR+XjMssXudptE5GNextnG7xOmFmZ2KJsxKjsehzbGeOTee+894n5TRkYG9913X4IiGni2rz5AdmoBKb5UDu2p4/01lXE5rmcJSkT8wP3AZcAs4DoRmdVpte8Bj6nqqcC1wH+6285y52cDlwL/6e7PcxfPKuTsKcOZXBDiI9MKOO+EfFrCUVbtPMQLGyrYebD+6DsxxiSNRYsWsWDBAsQd5CgtLY2FCxdy8803JziygUFVqals7FBW3WneK14OtzEX2Kqq2wFE5FHgCmB9zDoKtJ2i5AB73OkrgEdVtRnYISJb3f294WG8AAT8Ps6aPLxD2ZJ3d7PrkNNYYk15NR+fM4rpI7O8DsUY009KSkp48sknGaPKvFGj+dlNNxOprcWfZX/HRyMiZGQHO5R1nveKl5f4xgC7YubL3bJYxcANIlIOPAV8pQ/bIiK3ikipiJQeOOBNY4aqhpb25NRmTXmVJ8cyxvQvjUaJ1NURCoX42KzZXJ43nDtu+Cy6ZQvVf/0rGknccOYDyaRTCmiJON+D6VlBJp2UH5fjJnrAwuuA/1HVn4rI2cAfROTE3m6sqg8AD4DzHJQXAab4fYhA7ONiwQSOMGmMOZK2tNCycyeSmkpg3DhEhJZdu6h97nmi9fX483KZneInd/RoCgpGABCprqF17z6CY4/47Ws6CeWksvXwSnySwpzz747bcb38pt0NjIuZH+uWxboFeAxAVd8A0oD8Xm4bF5mpKZwyblj7fDDFx9xJeYkIxRjThUhdPYf+9DA1Tz9D9ZNLqH7ySaLRKLUvvEDUbb0XOXSYkeGOLXI13EpDaSmH/vBHap57jmhjfO6rJKPePOqwYsUKXlr+QlwfdfDyDGolMFVEJuEkl2uB6zut8wHwUeB/RGQmToI6ACwBHhaRnwGjganA2x7G2qPzp49g+sgsqhpamTA8g4xgok88jTFtmtatJVpX1z7fuquclh07iNbWdViv3uejRT78TR5paKB50yZ8GRlEqqrQ5hZyPrEgbnEnk+Li4qR8PMGzMyhVDQO3A88AG3Ba65WJyD0icrm72jeAL4jIe8AjwE3qKMM5s1oPPA18WVUTerF4VE46M0dlW3IyJsmo20NELAFSRhZ2KNubksLyjHRyrrySlBEjaN60mcZ162jatAmNRmnZ+X58Aja95um3rao+hdP4Ibbszpjp9cC8brb9AfADL+Mzxgx8qTNm0LhuHUSch+z92VkEx48nZcQI6l97jfCBSgLjxlKWmoqKoE2NhPfvx5eeTrShgcjhw7Rs305w0kSiLS34gvFpoWaOzk4HjDEDWmDECIZ9+tM0b9yIBFNJn3MiEgjgDwTIvvTS9vUi7nNQEffSX3DSJJo3b6a1vBxtaSZl1EiqHn2UYVdfjS8tLSHvxXRkCcqYAe5AwwGqm6sZkzWG9JT0RIeTEIERIwiMGNGrdVMnT6LhrTfxh0IExo5Fm5tJO/kk/OkZRKpraN60ifSTT/Y4YtMblqCMGcBe3/M6q/evBiDoD7Jw8kIKQ4U9bzTE+YcNI+fKK2lcvRoJBPClpuJPz2hfrmHrfzNZ2AM9xgxALZEWXvjgBR7e8DA7a3YSjoZpibSwqmJVokMbEAKjRpF92WXk3fh5AqNGtZf70tNInT49gZGZWHYGZcwA9HL5y6w5sIb61nrqW+tpjbRyQu4JNEeaEx3agOJLT2fY1VfTvHEDGo2SNnMm/szMo29o4sISlDED0I7qHaSlpJEVzKK2pZbDzYdRVWYN79wfszkaf2aIjKIjBnM1ScASlDEDhKKUVZaxu243NS01pPpTmTpsKvsb9pPiS2HB5AVMzJmY6DCTQqSmBgkGrTXeANdtgnLHYMpS1Sc6lX8aqFbV57wOzpihanfdbl4tf5WalhomD5uMihIeGWZF+QoAItEIexv3MiZrDJOHTeaSiZcwJtP6lIu2tFDz1FO07ioHv4+M004ndNaZiQ7LHKOezqDuBK7sonw5sBSwBGWMB1qjrTy94+n2+0mbDm2idWQrkbwPO1PJDGYSCob49LRPk5eWR4rPLoYANK1Z4yQngEiUhpUrSZ16AinDh/e8oUlKPbXiS1XVI8awUNVKINTF+saYfnC46fARjR2imVEkLB3KMlIyKEgvsOQUI1JV1asyMzD0lKCyReSIT76IBICh+TSgMXGQm5ZL0N+xux1fvY/A3gB+d2BpEeHs0We3jxJrHMFJkzrMSzBIYIxd+hyoevrp9b/Ab0XkdlWtBxCRTOAX7jJjjAcCvgAfm/AxXt79MjUtNUzJmUJgXwCJCp+b9TkqGirIT88nK2ijwXaWOmUKmRdcQNOG9fjS0sg44wxrKDGA9ZSgvgf8G7BTRHbidBA8DngQ+Nc4xGbMkDUuexyfzf5s+/x3a7/L+vXr2bF5B7Nnz05gZMkv/cTZpJ9odTQYdHuJT1XDqnoHTlK6CbgRGK+qd6hqa5zi81xvBurqyysZx1RJBvX19axcuZKysrJEhzLg1NfXs3btWhoaGliwYAH17iB8po/Uk0G3jYe6TVAicpWIXAVchjNg4AlAkYgMqusKxcXFqGqPr/nz5zN//vyjrqeqlqC6YF+wx2fRokW0uGMeVVRUcMsttyQ4ooFFw2Fqn3+ey+rrubC+nuZt2xIdkumlnhpJLOz0uhz4JrBGRC6MQ2xmIKuvhM3PwuZn+foXPmtfsMeopKSEZcuWoe6v/6amJpYuXUpJSUmCIxs4GlevpmnDRnwK6VGl9tlniTY0JDos0ws9XeK7uYvXFcD5wL1xi9AMPE3V8M5DsHsV7/79d4ypeIHsoH3BHovFixcfccbZ0NDA4sWLExTRwNO6r6LDvIYjhCsrExSN6Ys+92auqjuBgAexmMHiwCYIO2dMz7/wPERamT3C377YvmB779577yUU6vjYYUZGBvfdd1+CIkpOPd1Lvuyaq/nve39IYN9easrL+dfiYrInTrR7yQNAn5/wE5EZgHWZbLqX8mGz3os+ehH/ePofNLR+2K7GvmB7b9GiRTzzzDM8/vjjqCppaWksXLiQm2++OdGhJZXi4uIuk4pGoxz6wx9pWrOG8MGDSCBA3g2f5dfz58c/SNNnPfXFtxTo3OwlDxgF3OBlUMlq/Z4aSnceQhVOn5DLiWNyEh1SchoxE/a8CzV7OPXUU1m5qZyyF98EsC/YY1BSUsKTTz5Jc3MzhYWFPPjgg4kOacAIV1QQrakhOHEiwYkTgQ+HfDfJr6czqH/vNK/AIZwkdQPwxtF2LiKX4jzY6wd+p6r3dVr+c+ACdzYDGKGqw9xlEWCtu+wDVb38aMfzUjgQ4pmyfe3zz62vIC8UZPQw61TjCP4AnPZ5OPw+AJ8943a++lABROwL9liEQiHmzJnD+vXrWbZs2RGX/Ez3fBkZINKhibkvlNHDFiaZdJugVHVF27SInApcD3wG2AH85Wg7FhE/cD9wMVAOrBSRJaq6PuYY/xyz/leAU2N20aiqp/T6nXgsnDbsiLIPDjVYguqOCOQ53c6EwL5gj1MoFOKMM86wh3T7yJ+TQ/opp9D47rsA+LIyyTj99ARHZXqrp0t804Dr3Fcl8GdAVPWC7rbpZC6wVVW3u/t7FLgCWN/N+tcBd/Vy33Hnbzny2Z2CrNQERDIw2ResSZTMc+eRNnsW0fp6AqNGIX7/0TcySaGnVnwbgQuBT6jquar6SyDSw/qdjQF2xcyXu2VHEJEJwCTgxZjiNBEpFZE3ReTKPhzXE4HmKs6YmEeKT0jxCadNyGVKgQ0NbcxAkJKbS3DsWEtOA0xP96CuAq4FXhKRp4FHcfrj88K1wBOqGpsAJ6jqbhGZDLwoImtVtcMj4CJyK3ArwPjx4z0K7UPnTs3nrMl5KBDw97mFvjHGmD7o6UHdv6nqtcAM4CXg68AIEfkvEbmkF/vejdOPX5uxbllXrgUe6XT83e6/23EGSTy180aq+oCqFqlqUUFBQS9COn4pfp8lJxM34WiY9w68R/OEZsJ54fYeJYwZCo76Tauq9ar6sKouxEky7wLf6cW+VwJTRWSSiARxktCSziu5z1XlEtMqUERyRSTVnc4H5tH9vStjBq0Vu1bw2u7XiORGaBnfQmlFaaJDMiZu+nQqoKqH3bOWj/Zi3TBwO/AMsAF4TFXLROQeEYltMn4t8Kh2/Gk4EygVkfdwzt7ui239Z8xQ0BptZXPV5g5lGw5uSFA0xsSfp2NFq+pTwFOdyu7sNF/cxXavA3O8jM2YZLfh4AY2HtyIokSDUXwtPtJSbPA9M3TYzRRjktD26u28uvtVctNyqW2pJZIbQUU5c9SZiQ7NmLixBGVMEvqg5gMARmSMYE7+HPzVfoK7gkzInpDgyIyJH08v8Q0kW/fX8cqWAzS0RJg5Kov500bg93Xfqj4ciXKgrpncjCBpAXu2wvSvvLS89um0lDR8TT789fY5M0OLJSigoSXMP9buJRx12mm8t6uanPQgp0/I7XL9fdVNPLl6Nw0tEQJ+4ZLZI5lWOKgGGjYJNmv4LPbU72FH1Q584iOwN4Cv0S54mKHFEhRQUdPcnpza7Klq7DZBveyeaQG0RpSXNu7nhIJMfD2ccQ1pdQc4K6+6fZrM+DyzNtDUttSy8dBGVJUZw2dw6cRLaWhtIMWXwiMVjxx9B8YMMpaggMLsVPw+IRKTpEYP6761VE1ja4f5hpYIrdEoqT67BHMEd3TdGdnuENvvPARn/BOkD0toWMmmobWBJzY/QWO4EYB1B9dxzfRrCAWsY10zdNk1AyAjmMJlJ44kOz1Aik+YMyaHU8Z9ePZUX1/PypUrKSsrAzjict7E/AxSUyw5dWn/RojEJPRIKxzYmLh4ktTWqq3tyQmgKdzElsNb2ucVJRqIEtVoIsIzJiHsDMo1tTCLqV3cR6qvr2ft2rU0NzezYMECysrKmHdCPmkBPzsP1jMiO40zJ+V1sUcDQKCL4Ui6KhviUnxH/imm+FKob63n5fKXqZtXh4SFP6z/A5dMuIRRmaMSEKUx8WVnUEexaNEiWlpaAKioqOCWW27B7xPmTsrjM0XjmD+twFrx9WTETMge/eF89igYMStx8SSpE4adQG7ah2ftw1KHMWXYFJ7c+iRPbX+KaFaUSG6EXbW7eHHXiz3syZjBw86gelBSUsKyZcvaO+hsampi6dKllJSUsGjRogRHN0C4o+s+ve/XANx02o3Q2gjbXoL6/ZA3GcadBb6h/Vsp6A/y6WmfZmf1ThRlYs5EDjYepKq5qsOlv4ONB8lLyyMSjeC3e55mkBva3wpHsXjxYurrOw5U2NDQwOLFixMU0QAlwr6mVPY1pToj7Zb9L+xeBVW7YPsKeP/lREeYFAK+ACfknsDU3KkEfAHSU5xLoTmpOR3WGZs11pKTGRIsQfXg3nvvPWJ48oyMDO67774ERTQItNQ7iSnWgU2JiSXJ5aTmcFLBSUzInoCv0Yc0C+eOOZeLxl+U6NCMiQtLUD1YtGgRCxYsQMR5viktLY2FCxdy88030xqJsutQA7VNrUfZi+nAnwqBTk3403K6Xtdw7phzuWHWDWS8k0HW8iwuP+FyMgIZiQ7LmLiwBHUUJSUlBINBAAoLC3nwwQfZX9PEg6/u4IlV5ZS8+j7vfnA4wVEOIP4UmHqJ8y9AaiZMviCxMSW5vLQ8/LV+RO1BcDO0WII6ilAoxJw5c8jIyGDZsmWEQiFe33aQRrcniagqr22tpCVsz6f0WuFsOPsrcPpNcNaXIKsw0RElVHFxMSLS42vFihWsWLHiqOuJCMXFxYl+S8b0C2vF1wuhUIgzzjiD2bNnA1DXHO6wvDWiNIcjBFMs3/daIA0C9iwPOAnKkooxR7Jv1GMwY2THB3rH5KaTlRZIUDTGGDM42RnUMTh9Qi6pKX62V9YxPJRK0cSuO5U1xhhz7CxBxahvDrN2dzUt4Sgjs9PYXllPczhCS1oewaZD7euJCHPG5jBnrLU+M8YYr1iCcrVGojy6chc1ja2Eo1HWlFczY2QWGcEU6vNnIAfWJzrEpFVcXMzdd9/dq3Xbmuz35K677rJ7MsYYS1Bt3q+sbx9Go6YxTEs4yv7aZiYOd6qoNSM/keElNbvJb4zxgqeNJETkUhHZJCJbReSOLpb/XERWu6/NIlIVs+xGEdnivm70Mk6gQwu8tml/zK99iTR7HYIx7VSVXTW72HJ4Cy2RlkSHY0xCeHYGJSJ+4H7gYqAcWCkiS1S1/VqZqv5zzPpfAU51p/OAu4AiQIFV7raePRE7Pi+DsbnplB9uJDM1hUn5IfJCQVoiUSQaJVC336tDG9OBqvL37X9nV63TJVQoEOKqqVeRFTxyOBhjBjMvz6DmAltVdbuqtgCPAlf0sP51QNu41h8DnlPVQ25Seg641MNYERE+ddpYrjx1DB+fM4p7rjiR+dMKiESUqM9H3chT2HWowcsQjAFgT/2e9uQEUN9az9rKtQmMyJjE8DJBjQFiewUtd8uOICITgElA20A3vdpWRG4VkVIRKT1w4MBxB+zzCZPyQ0wfmYWq8mzZPvehXEF9fl7ZUnncxzDmaLq6pBeOhrtY05jBLVke1L0WeEJVI33ZSFUfUNUiVS0qKCjot2CawxH+9NYHrNtTw7YDdbRkjkTxUd9sXxLGe+OyxpEdzG6f94mPGXkzEhiRMYnhZSu+3cC4mPmxbllXrgW+3Gnb8zttu7wfY+vRloo6qhtbGR4KcrC+BfWlEAlkMHNU9tE3NuY4pfhSuGrqVZQdLKM50sz03OkUZPTfDzBjBgovE9RKYKqITMJJONcC13deSURmALnAGzHFzwA/FJG2LhouAeI2SmA46oygO2VEJulVjaxpbSStbi/nTBkerxDMEJcRyOCMkWckOgxjEsqzS3yqGgZux0k2G4DHVLVMRO4RkctjVr0WeFTbxlV3tj0EfB8nya0E7nHL4mJaYSahVD8+EcbmZpBWt4/QwY34fDbcgTHGxIvE5IUBraioSEtLS/ttf3XNYdbvqUFV+dYXrsMXaWH58uX9tn9jjDEOEVmlqkWdy60niU7WlFfx9o5DRKLKqeNzOXPycHz2oKQxxsSdJagYFTVNvLDhwwdyX9taSX5mMIERGWPM0JUszcyTwu6qxiPK9lQ1JSASY4wxlqBijMpJO6JsZBdlxhhjvGcJKsaonHTmTy8gPegnmOLjzEl5nDAiM9FhGWPMkGT3oDo5bXwup423EXKNMSbR7AzKGGNMUhryCaq4uBgR6fG1YsUKVqxYcdT1RMQG7jPGmH5iD+oaY4xJqO4e1B3yZ1DGGGOSkyUoY4wxSckSlDHGmKRkCcoYY0xSsgRljDEmKVmCMsYYk5QsQRljjElKlqCMMcYkJUtQxhhjkpIlKGOMMUnJEpQxxpik5GmCEpFLRWSTiGwVkTu6WedqEVkvImUi8nBMeUREVruvJV7GaYwxJvl4Nh6UiPiB+4GLgXJgpYgsUdX1MetMBRYD81T1sIiMiNlFo6qe4lV8xhhjkpuXAxbOBbaq6nYAEXkUuAJYH7POF4D7VfUwgKru9zCeozpU38KrWyupbmzlhIJMzpyUh88niQzJGGOGLC8T1BhgV8x8OXBmp3WmAYjIa4AfKFbVp91laSJSCoSB+1T1bx7GSjSq/PXd3dQ0tgJQWduM3yfMnZTn5WGNMcZ0I9FDvqcAU4HzgbHAyyIyR1WrgAmqultEJgMvishaVd0Wu7GI3ArcCjB+/PjjCqSyvrk9ObXZUVlnCcoYYxLEy0YSu4FxMfNj3bJY5cASVW1V1R3AZpyEharudv/dDiwHTu18AFV9QFWLVLWooKDguILNTgsQ8He8nJcXSj2ufRpjjDl2XiaolcBUEZkkIkHgWqBza7y/4Zw9ISL5OJf8totIroikxpTPo+O9q36XFvBz4YxCgilOlRRmp3H2lOFeHtIYY0wPPEtQqhoGbgeeATYAj6lqmYjcIyKXu6s9AxwUkfXAS8C3VPUgMBMoFZH33PL7Ylv/eWXW6Gw+feIwXvjlt/jChTOZPW0KDz/8cJfrXnbZZWRmZra/gsEgc+bMAWD//v1cd911jB49mpycHObNm8dbb73ldfhJ4dChQ3zyk58kFAoxYcKEY6q/Nr/4xS+YNGkSoVCImTNnsnnz5ni8hYSzOjw+va2/5uZmbrvtNgoLC8nLy2PhwoXs3v3hRZ5f/epXFBUVkZqayk033RSn6JNDf9Xh+eefT1paWvtndPr06X0LRFUHxev000/X/nDttdfq1VdfrbW1tfrKK69odna2rlu37qjbzZ8/X++++25VVd22bZv+9Kc/1T179mg4HNbf/OY3Onz4cK2tre2XGJNZf9Sfqupvf/tbnTNnjpaVlWk0GtWtW7fqwYMHvQw9aVgdHp/e1t+PfvQjPemkk3Tfvn3a2Nion/vc5/STn/xk+/K//OUv+te//lVvu+02vfHGG+P4DhKvv+pw/vz5+tvf/vaoxwNKtYvv9YQnlv569UeCqqur00AgoJs2bWovu+GGG/Q73/lOj9vt2LFDfT6f7tixo9t1srKytLS09LhjTGb9VX+RSETHjh2rzz//vJfhJiWrw+PTl/q77bbb9Fvf+lb7/N///nedNm3aEet997vfHVIJqj/r8HgTlHV1FGPz5s2kpKQwbdq09rKTTz6ZsrKyHrf7/e9/z3nnncfEiRO7XL569WpaWlo44YQT+jPcpNNf9VdeXk55eTnr1q1j3LhxTJo0ibvuuotoNOpl+EnB6vD49KX+brnlFl577TX27NlDQ0MDf/rTn7jsssviGW5S6u86XLx4Mfn5+cybN4/ly5f3KZZENzNPKnV1dWRnZ3coy8nJoba2tsftfv/73/O9732vy2U1NTV87nOf46677iInJ6ffYk1G/VV/5eXlADz77LOsXbuWqqoqLrnkEsaOHcsXvvCF/g88iVgdHp++1N/UqVMZN24cY8aMwe/3M2fOHH71q1/FK9Sk1Z91+KMf/YhZs2YRDAZ59NFHWbhwIatXr2bKlCm9isXOoGJkZmZSU1PToaympoasrKxut3n11VfZt28fn/70p49Y1tjYyMKFCznrrLNYvHhxv8ebbPqr/tLT0wH49re/zbBhw5g4cSJf/OIXeeqpp7wJPIlYHR6fvtTfl7/8ZZqbmzl48CD19fVcddVVdgZF/9bhmWeeSVZWFqmpqdx4443MmzevT59BS1Axpk2bRjgcZsuWLe1l7733HrNnz+52m4ceeoirrrqKzMzMDuXNzc1ceeWVjB07lt/85jeexZxM+qv+pk+fTjAYROTD59Jipwczq8Pj05f6W716NTfddBN5eXmkpqbyla98hbfffpvKysp4hpx0vKxDEXEaP/RWVzemBuKrv1rxXXPNNXrttddqXV2dvvrqqz22oGpoaNDs7Gx94YUXOpS3tLToJz7xCb3iiiu0tbW1X+IaKPqj/lRVP/e5z+mCBQu0pqZGd+3apdOnT9ff/e53XoefFKwOj09v6++mm27Sq666SquqqrSlpUV/8IMf6OjRo9uXt7a2amNjo95xxx16ww03aGNj45D5e+6POjx8+LA+/fTT7fX2xz/+UTMyMjo0vmiDteLrnYMHD+oVV1yhGRkZOm7cOP3Tn/6kqqovv/yyhkKhDus+/PDDOn78eI1Gox3Kly9froCmp6drKBRqf7388sv9EmMy64/6U1Wtrq7Wa665RjMzM3Xs2LF69913d7neYGR1eHx6W3+VlZV6/fXXa0FBgebk5Oi8efP0rbfeal9+1113KdDhddddd8X77SREf9Th/v37taioSDMzMzUnJ0fPPPNMffbZZ7s8XncJSrQvp1tJrKioSEtLSxMdhjHGmD4SkVWqWtS53O5BGWOMSUqWoIwxxiQlS1DGGGOSkiUoY4wxSckSlDHGmKRkCcoYY0xSsgRljDEmKVmCMsYYk5QsQRljjElKlqCMMcYkJUtQxhhjkpIlKGOMMUnJEpQxxpik5GmCEpFLRWSTiGwVkTu6WedqEVkvImUi8nBM+Y0issV93ehlnMYYY5JPilc7FhE/cD9wMVAOrBSRJaq6PmadqcBiYJ6qHhaREW55HnAXUIQzDssqd9vDXsVrjDEmuXh5BjUX2Kqq21W1BXgUuKLTOl8A7m9LPKq63y3/GPCcqh5ylz0HXOphrMYYY5KMZ2dQwBhgV8x8OXBmp3WmAYjIa4AfKFbVp7vZdkznA4jIrcCt7mydiGzqn9C7lA9Uerj/ocDq8PhY/R0/q8Pj41X9Teiq0MsE1RspwFTgfGAs8LKIzOntxqr6APCAN6F1JCKlXY34aHrP6vD4WP0dP6vD4xPv+vPyEt9uYFzM/Fi3LFY5sERVW1V1B7AZJ2H1ZltjjDGDmJcJaiUwVUQmiUgQuBZY0mmdv+GcPSEi+TiX/LYDzwCXiEiuiOQCl7hlxhhjhgjPLvGpalhEbsdJLH6gRFXLROQeoFRVl/BhIloPRIBvqepBABH5Pk6SA7hHVQ95FWsvxeVS4iBndXh8rP6On9Xh8Ylr/YmqxvN4xhhjTK9YTxLGGGOSkiUoY4wxSckSFCAi94jIRf2wn9tE5PP9EdNgJiLLRaTXTVVF5HwR+buXMSWCiHxVRDaISL2IzOpmnXEi8lJMd2Bfi1n2IxFZIyK/jym7QUS+HofwE6I3deauVyIi+0VkXafyPBF5zu1C7Tm3ERYi8im3fl8RkeFu2RQR+bO37yg+ROT1XqzzdRHJiEMsvf57tgQFqOqdqvp853K3u6a+7OfXqvr7o69pDABfwukK7HGguy/bMPANVZ0FnAV8WURmiUgOcJqqngS0iMgcEUkHbsbpYmyw6k2dAfwPXfc+cwfwgqpOBV5w5wG+ApwB/Aa43i37N+B7xx9y4qnqOb1Y7etAnxJUX78j+2pQJigRmSgiG0XkT+6vrSdEJENE7hSRlSKyTkQeEBFx1/8fEfm0O/2++8v0HeAzMfscISKr3OmTRURFZLw7v83df7GIfNMt+6r7q3eNiDzqloXcX3Zvi8i7ItK566ek1kO9ftR9P2vd95fqrt9lead9XiIib4jIOyLyuIhkuuWXusd6B7gqzm/VcyLya2AysAO4EfiJiKwWkSmx66nqXlV9x52uBTbg9KoSBQLuZzgDaAW+CfxSVVvj907ip7d1BqCqLwNdtfy9AnjInX4IuNKdjgKpuHUpIucB+1R1S7++iQQRkTr33/PdKxhPxPwti4h8FRgNvCQiL7nrdve32eE70p2/211vrYjMcNeb627/roi8LiLT+xy4qg66FzARp5PZee58Cc4fb17MOn8AFrrT/wN82p1+H/h2N/stA7KB23GawH8Wp4uON9zlxcA33ek9QKo7Pcz994fADW1lOA8mhxJdX8dZr9/D6ZZqmlv2e5xfYmldlbvTy3E6As4HXm6rA+A7wJ0x204FBHgM+Hui378H9fm+Wwftn79e1P8HQLY7/21gNfBTYNRgrKPjqTO3vtZ1KquKmZa2eZyzslXAUiAHeDb2+2Kgv4A699/zgWqczg98wBvAubF16053+bcZs963Y/b9PvAVd/pLwO/c6WwgxZ2+CPhLTAy9+qwOyjMo1y5Vfc2d/iNwLnCBiLwlImuBC4HZ3Wzb3XXn14F5wEdwks1HgPOAV7pYdw3wJxG5AecyDTgPHN8hIqtxvqTTgPF9eE/JoHO9fhTYoaqb3bKHcOplejflsc7CuUzzmlsnN+Ik/BnutlvU+UT/0as3M1C4v17/gpPkawBU9ceqeoqqfgP4PnCniPyTiDwmIoPi0pSX3M+WutPPqerpqroQ5yzrKWCae6bx23jcm4mjt1W1XFWjOD9wJnaxTnd/m206f0f+r/vvqpj95QCPi3Mf8Od0/33brcGcoDo/4KXAf+L86poD/BYnQXSlHkBE/tu9hPCUW/4yTkKaADwJnIyT+LpKUAtw7gWchjPUSArOL7ZPuV8qp6jqeFXdcMzvMDE612vVcexLcHqtb6uPWap6y3Hsb8ATp1HEavd1m1sWwElOf1LV/+1im1Nx6nIT8BlVvRqYIs5wNoNeV3XWgwoRGeVuNwrYH7vQTUQ34fzt3o3zxfwqztWSwaI5ZjpC1x02HO1vs76bfcbu7/vAS6p6IrCQ7r9vuzWYE9R4ETnbnb4e50MGUOn+Gv300Xagqje7/zkfd4teAW4Atri/Pg4BH4/ZNwAi4gPGqepLOKfGOUAmTs8ZX4m593Xq8bzBBOlcr6XARBE5wS37HLAC58uyq/JYbwLz2tZx79FNAza627bdW7jOm7eSNGqBLABV3RXzpfBr97PyILBBVX/WzfbfB/4VCOD02gLOPZXB9Ku/s27r7CjbLcFJOrj/Ptlp+beA/1DnPl46zg+ywV6XbdrrlO7/Nvsihw/7UL3pWAIazAlqE06Lpw1ALvBfOGdN63ASxcoetu2Sqr6P88viZbfoVZxr2J0HUvQDf3QvJb6L84GvwvkiCQBrRKTMnR9oOtfrz3Fajj3uvt8o8GtVbeqqPHZHqnoA54P7iIiswbkePsPd9lZgmXsjtsOv3EHoUeBb7s3kzjf85+Ek9wtjzhLafjAhIlfidB22x/2MrXbrO01V34tT/InQU50hIo/gfJ6mi0i5iLT9+r8PuFhEtuDcF7kvZpvRwFxV/Ztb9Euc74nbgPbRvgexB4CnReSl7v42+7i/HwP3isi7HGO3eoOyqyMRmYhzE+7ERMcymFi9GmPiaTCfQRljjBnABuUZlDHGmIHPzqCMMcYkJUtQxhhjkpIlKGOMMUnJEpQxCSQiw0TkS8e47fsikt/fMRmTLCxBGZNYw3D6LzPGdGIJypjEug+nW6LVIvJzEXkhplfoK6D9Kf5lIvKeOD3xXxO7AxFJF5F/iMgXEvIOjPHIMT3da4zpN3cAJ6rqKW5/jRmqWuNeuntTRJbgjGu0R1UXAIgzFlSbTJxeFX6vNhaZGWTsDMqY5CHAD92uZZ7HGfepEFiL0z3Pj0TkPFWtjtnmSeC/LTmZwcgSlDHJ47NAAXC6qp4CVOD0qbcZp1f8tcC/icidMdu8Blza1gGxMYOJJShjEiu2B+kcYL+qtorIBbjj77idmDao6h+Bn+AkqzZ3AocZ3MO8myHKEpQxCaSqB3EGhVsHnAIUub2Rfx5n2BGAOcDb7sBxdwH/1mk3XwPSReTHcQnamDixvviMMcYkJTuDMsYYk5QsQRljjElKlqCMMcYkJUtQxhhjkpIlKGOMMUnJEpQxxpikZAnKGGNMUrIEZYwxJil5mqBE5FIR2SQiW0Xkji6W3yQiB9yhBlaLyD/FLIvElC/xMk5jjDHJx7OeJETED2wGLgbKgZXAdaq6Pmadm4AiVb29i+3rVDXTk+CMMcYkPS/PoOYCW1V1u6q24IxZc4WHxzPGGDOIeDlg4RhgV8x8OXBmF+t9SkQ+gnO29c+q2rZNmoiUAmHgPlX9W+cNReRW4FaAUCh0+owZM/oxfGOMMfGwatWqSlUt6Fye6BF1lwKPqGqziHwReAi40F02QVV3i8hk4EURWauq22I3VtUHgAcAioqKtLS0NJ6xG2OM6QcisrOrci8v8e0GxsXMj3XL2qnqQVVtdmd/B5wes2y3++92YDlwqoexGmOMSTJeJqiVwFQRmSQiQeBaoENrPBEZFTN7ObDBLc8VkVR3Oh+YB6zHGGPMkOHZJT5VDYvI7cAzgB8oUdUyEbkHKFXVJcBXReRynPtMh4Cb3M1nAr8RkShOEr0vtvWfMcaYwW/QDFho96CMMWZgEpFVqlrUuTzRjSSMMcYkiXBrhEN76olGlLzRIYJpiU0RlqCMMcYQCUdZ/+oemupaAdiztYoTzxtDMD1xacL64jPGGENVRUN7cgIIN0c4sKs2gRFZgjLGGNMNkcQe3xKUMcYYhhVmkJYZbJ9PSfWTPzYrgRHZPShjjDGAP8XH7HNHc3BPnTWSMMYYk1z8AR8jJmQnOox2donPGGNMUrIEZYwxJilZgjLGGJOULEEZY4xJSpagjDHGJCVLUMYYY5KSJShjjDFJyRKUMcaYpORpghKRS0Vkk4hsFZE7ulh+k4gcEJHV7uufYpbdKCJb3NeNXsZpjDEm+XjWk4SI+IH7gYuBcmCliCzpYmTcP6vq7Z22zQPuAooABVa52x72Kl5jjDHJxcszqLnAVlXdrqotwKPAFb3c9mPAc6p6yE1KzwGXehSnMcaYJORlghoD7IqZL3fLOvuUiKwRkSdEZFxfthWRW0WkVERKDxw40F9xG2OMSQKJbiSxFJioqifhnCU91JeNVfUBVS1S1aKCggJPAjTGGJMYXiao3cC4mPmxblk7VT2oqs3u7O+A03u7rTHGmMHNywS1EpgqIpNEJAhcCyyJXUFERsXMXg5scKefAS4RkVwRyQUuccuMMaZfaThM06bNNK4rI9rYmOhwTAzPWvGpalhEbsdJLH6gRFXLROQeoFRVlwBfFZHLgTBwCLjJ3faQiHwfJ8kB3KOqh7yK1RgzNGkkQtUTfyHs3sNueOtNhl19Nf6sxI4kaxyiqomOoV8UFRVpaWlposMwxgwgzdu3U7PsqQ5lGWcUETrrrARFNDSJyCpVLepcnuhGEsYYkzjRaO/KBrni4mJEpN9excXF/RKXnUEZY4YsbW3l8KN/JlJVBYAEgwy7+jOk5OYmNrAkdP755wOwfPnyft93d2dQnt2DMsaYZCeBAMM+82maN25EW1tJnT4df3Z2osMyLktQxpghzZeWRvoppyQ6DNMFuwdljDEmKVmCMsYYk5QsQRljjElKlqCMMcYkJUtQxhhjkpIlKGOMMUnJEpQxxpikZAnKGGNMUrIEZYwxJikdNUGJyCoR+bI7LpMxxhgTF705g7oGGA2sFJFHReRjIiIex2WMMWaIO2qCUtWtqvpdYBrwMFAC7BSRu0Ukr6dtReRSEdkkIltF5I4e1vuUiKiIFLnzE0WkUURWu69f9+1tGWOMGeh61VmsiJwE3Ax8HPgL8CfgXOBF4JRutvED9wMXA+U4Z2BLVHV9p/WygK8Bb3XaxTZV7XLfxhhjBr+jJigRWQVUAQ8Cd6hqs7voLRGZ18Omc4Gtqrrd3c+jwBXA+k7rfR/4EfCtvoVujDHHJ9rQQOuePfiHD7cxoJJQb86gPtOWZNqIyCRV3aGqV/Ww3RhgV8x8OXBmp/2cBoxT1WUi0jlBTRKRd4Ea4Huq+krnA4jIrcCtAOPHj+/FWzHGGEfLrl3ULFuGtoZBhNC8c8g49dREh2Vi9KaRxBO9LOsTEfEBPwO+0cXivcB4VT0V+BfgYRE5YhQxVX1AVYtUtaigoOB4QzLGDCENb73lJCcAVRreehttbU1sUKaDbs+gRGQGMBvIEZHYM6VsIK0X+94NjIuZH+uWtckCTgSWu40CRwJLRORyVS0FmgFUdZWIbMNppGFjuhtj+kW0qbnDvIbDaDSKNVFOHj1d4psOfAIYBiyMKa8FvtCLfa8EporIJJzEdC1wfdtCVa0G8tvmRWQ58E1VLRWRAuCQqkZEZDIwFehwmdEYY45H2qyZ1L/2evt86pTJ+FJTExiR6azbBKWqTwJPisjZqvpGX3esqmERuR14BvADJapaJiL3AKWquqSHzT8C3CMirUAUuE1VD/U1BmOM6U7GaafhC2XS8sFOUobnk37SnESHZDrp6RLft1X1x8D1InJd5+Wq+tWj7VxVnwKe6lR2Zzfrnh8z/Rec5uzGGOOZtOnTSJs+LdFhmG70dIlvg/uv3fcxxhgTdz1d4lvqTq5V1XfiFI8xxhgD9K6Z+U9FZIOIfF9ETvQ8ImOMMYbe9cV3AXABcAD4jYisFZHveR6ZMcaYIa1X40Gp6j5V/Q/gNmA10GVDB2OMMaa/9GY8qJkiUiwia4FfAq/jPHRrjDHGeKY3ffGVAH8GPqaqezyOxxhjjAF6kaBU9ex4BGKMMcbE6ulB3cdU9Wr30p7GLgJUVU/yPDpjjDFDVk9nUF9z//1EPAIxxhhjYnXbSEJV97qTX1LVnbEv4EvxCc8YY8xQ1Ztm5hd3UXZZfwdijDHHI1xZyeHHHuPA/fdTvXQp0fr6RIdkjlO3CUpE/o97/2m6iKyJee0A1sQvRGOMObqaZ54hXLEfokrL+zupe+WIQbjNANPTPaiHgX8A9wJ3xJTX2tAXxphkEm1qInLocIey1j17u1nbDBQ9XeJTVX0f+DLOIIVtL0Qkz/vQjDGmd3xpafjzcjuUBUaPSlA0pr8c7QzqE8AqnGbmsSMhKzDZw7iMMabXiouL+dW//RsXZGZSkJLCrpZWXqyrpUH16Bt34a677qK4uLh/gzR9JnqM/4G92rnIpcAvcEbU/Z2q3tfNep8CngDOUNVSt2wxcAsQAb6qqs/0dKyioiItLbWhq4wxXTv//PMBWL58eULjGKi8rD8RWaWqRZ3Le9MX3zwRCbnTN4jIz0RkfC+28wP347T4mwVcJyKzulgvC+eZq7diymYB1wKzgUuB/3T3Z4wxZojoTTPz/wIaRORk4BvANuAPvdhuLrBVVberagvwKHBFF+t9H/gR0BRTdgXwqKo2q+oOYKu7P2OMMQlQX1/PypUrKSsri9sxe5OgwupcB7wC+JWq3g9k9WK7McCumPlyt6ydiJwGjFPVZX3d1t3+VhEpFZHSAwcO9CIkY4wxfVVfX8/atWtpaGhgwYIF1MfpGbPeJKha937QDcAyEfEBgeM9sLufn+GclR0TVX1AVYtUtaigoOB4QzLGGNOFRYsW0dLSAkBFRQW33HJLXI7bmwR1DdAM3KKq+3DGgvpJL7bbDYyLmR/rlrXJAk4ElovI+8BZwBIRKerFtsYYY+KgpKSEZcuW0dagrqmpiaVLl1JSUuL5sT1rxSciKcBm4KM4yWUlcL2qdnkBU0SWA99U1VIRmY3TzH0uMBp4AZiqqpHujmet+MxQE4lGEBF80quBsYc8a8V3bAoLC9m/f/8R5SNGjKCioqJfjnE8rfiuEpEtIlItIjUiUisiNUfbTlXDwO3AM8AG4DFVLRORe0Tk8qNsWwY8BqwHnga+3FNyMmYoiWqUFbtW8Nu1v6VkXQlrDljPY8Y79957L6FQqENZRkYG993X5VND/eqoZ1AishVYqKobPI/mONgZlBkq1h9cz/JdyzuUXTP9GoanD09IPAOFnUEdu2uuuYbHH38cVSUtLY0rrriCRx99tN/2f8xnUEBFsicnYwYzVaWqqYrWaCsABxqObLF6oNFasRrvlJSUEAwGAeeS34MPPhiX4x51yHegVET+DPwNp7EEAKr6v14FZYxxVDdXs2z7Mqqaqwj6g8wfO58xmWMoO/jhrVwRYVTI+p0z3gmFQsyZM4f169ezbNmyIy75eaU3Z1DZQANwCbDQfdkou8bEwRt736CquQqAlkgLK8pXMCFnAnNHziUUCJGblsslEy4hJzUnsYGaQS8UCnHGGWcwe/bsuB3zqGdQqnpzPAIxxhypqqmqw3xLpIWG1gaKRhZRNPKIS/bGDCq9acU3TUReEJF17vxJIvI970MzxkzMmdhhfnjacDtbMkNGby7x/RZYDLQCqOoanI5cjTEeO6PwDIoKi8hPz2dq7lQum3xZokMyJm5600giQ1XfFokdDoqwR/GYoSAShsbDkJEHPuukvid+n5+5o+Yyd5T1lWyGnt4kqEoRmYIzSCEi8mnAxlI2x+bwTij7K7Q2QjAEsz8Jw8YdfTtjzJDTm0t8XwZ+A8wQkd3A14HbvAzKDGKbn3GSE0BLPWx5NrHxGGP6TKNKU10r0ah3A95C71rxbQcucgct9KlqracRmcGt8XCn+UOJicMYc0zqq5rZUlpBS2OYlFQ/J5w2guz8dE+O1e0ZlIgsFJEJMUXfAF4VkSUiMsmTaMzgl39Cp/lpiYnDGHNMdq47SEuj0wwh3Bxhx5pKz47V0xnUD3CGwEBEPoEzHtR1wKnAr4GPeRaVGbymL4BgFtTshpxxMOm8REdkjOmDxrqWDvPNDWGiUcXnk262OHY9JShV1QZ3+irgQVVdBawSkS/1eyRmaAikwbRLEh2FMaYbLY1hKnbWEGmNUjAui9Cw1A7LcwtDVJZ/eKcnpyDdk+QEPScoEZFMnG6OPgr8Z8yyNE+iMcYYkzCRcJT1r+1pv4R34INaZp07mlDOh0lqwpzh+AM+ag81EcoJMnZmnmfx9JSg/h+wGqgBNqhqKYCInIo1MzfGmEGnen9je3ICp7Ve5a66DgnKn+JjwonxGdql20YSqloCzAduAT4es2gf0Kv++UTkUhHZJCJbReSOLpbfJiJrRWS1iLwqIrPc8oki0uiWrxaRX/fpXRljjOkzf+DIlNBVWbz02MxcVXfjDNceW9arsycR8QP3AxcD5cBKEVmiqutjVntYVX/trn858DPgUnfZNlU9pTfHMsYYc/yy89PIGZFB9X6n+UFqRoARE7ISFk9vepI4VnOBre5zVIjIo8AVOMO4A6CqsUPHh3B7qzDGGBN/IsK0uYXUHmwiEo46DSD8SXoGdZzGALti5suBMzuvJCJfBv4FCAIXxiyaJCLv4twD+56qvtLFtrcCtwKMHz++/yI3xpghSkQ8e/C2r3qdGkUkTUT+SUS+IiL9dodMVe9X1SnAd4C2YTz2AuNV9VSc5PWwiGR3se0DqlqkqkUFBQX9FZIxxpgk0Jdzt18ALcBhnOHfj2Y3ENsL6Fg63c/q5FHgSgBVbVbVg+70KmAbYF0OGGOOWX19PStXrqSsrCzRoZhe6qmro0fcXszb5AGPA38Bcnux75XAVBGZJCJBnDGklnQ6xtSY2QXAFre8wG1kgYhMBqYC23txTJMAxcXFiEi/vYqLixP9lswgU19fz9q1a2loaGDBggXU19cnOiTTCz2dQX0X+L6I/FREhgH/DvwV+AdQfLQdq2oYuB14BtgAPKaqZSJyj9tiD+B2ESkTkdU4l/JudMs/Aqxxy58AblNV61U0SRUXF6OqPb7mz5/P/Pnzj7qeqlqCMv1u0aJFtLQ4XfRUVFRwyy23JDgi0xs9PQe1XVWvx0lKf8Zp4LBAVc9X1Sd6s3NVfUpVp6nqFFX9gVt2p6oucae/pqqzVfUUVb1AVcvc8r/ElJ+mqkuP940ak6zsDNRbJSUlLFu2DFWnkXBTUxNLly6lpKQkwZGZo+npEl+u28JuFvAZnHtPz4jIwngFZ8xQYGeg3lq8ePERl/QaGhpYvHhxgiIyvdXTJb6/AVU4zyb9QVX/ACwEThURO6MxxgwI9957L6FQqENZRkYG9913X4IiMr3VU4IajnP/53GcZ5pQ1UZVvQf32aPBrDUSZXNFLVsqaglHookOxxhzjBYtWsSCBQsQcXrcTktLY+HChdx8c696bDMJ1NODuncCTwMRoEM/er3t7migamqN8OjbH3C4oRWA/KxUrikaRzAlcU9UG2OOXUlJCU8++STNzc0UFhby4IMPJjok0ws9NZL4X7fhwkWq+nw8g0q0jftq25MTQGVtM1v220j3xgxUoVCIOXPmkJGRwbJly4645GeSU0+NJHJE5F4R2SAih0TkoDt9n9vsfNBq7eKSXjhi3QQaM5CFQiHOLipiamYmkbq6RIcz4KT6MwgFconG8ZZHT9esHsNpJHGBquap6nDgApzWfI/FIbaEmT4yi9SYLuYzgn6mFmYmMCJjzPHKjUT4aH0D1UuWcuihh2hcuy7RIQ0Y76+tZEruGUzIOYk1L5XTHHOFyUs9JaiJqvojVd3XVqCq+1T1R8AE70NLnOy0AJ+dO4EzJ+Vx5uQ8rjtzPBlBL/vVNcZ4bUZzCynus1BElfo33kDD4Z43MjTWtbD//Q8HnmhpDLN3W3Vcjt1TgtopIt8WkcK2AhEpFJHv0LGX8kEpJyPAGZPyaI0of3t3N0+t3UtNU3x+NRhj+l+afniZPtrUSLiykmhTUwIjGhhamyO9KvNCTwnqGpym5ivce1CHgeU4ffJdHYfYEu7VrZW8s/MwB+ta2LSvlmVrBnXjRWMGtfKAcxWkZdcHNK5ZS+vevVQ98QSR6vicDQxUWblppGUGOpTlj43PLY+eWvEdVtXvqOoM9x5UrqrOdMuGRL94Ow50fPp8X3UTDS12ScCYgWhLIMCWYABtbiEwZgypU6YQra2jYdWqRIeW1MQnzDh7FAcbdlHdvJ9pc0eSOzI+rSC7vbEiInk4nb3uBkqAxcA5OB2//lBVD8clwgQanhmkuvHDy3qhVD9pKf4ERmSMOWYi7E9JIfWEEzoUR+sbEhTQwBFMS6GiwRlQYlhhRtyO29Mlvj/iDMNeBLwEjAJ+BDQC/+N5ZEngI1MLyAsFAUgP+rloZiE+nyQ4KmPMsar2+fDndhwtKHW6DTWXrHpqmjZaVT8uTv8g5ap6vlv+ijsMxqCXGwry+bMnUNMUJhT0k+K3niRMcmmJtPDq7lfZVbuLvLQ8zhtzHsPShiU6rKSlImRfcQUNr72KRiKkTZ9+xBmVSR49JSifiOQCWUCmiExU1ffd4d6D8Qkv8USEnPTA0Vc05jiVHSyjdF8prdFWTso/iTNGntHef1x3Xt/zOhsPbQSgvrWep99/mmtnXBuPcJNKtLGRxtWridTWkjp1KqmTJnW5XlYkSs1f/0qkuhpfRjoyZw4Azdu20bpnL4GRhQRPOOGo9W7io6cEdS+w0Z1eBPxORBRn+I27e7NzEbkUZ6h4P/A7Vb2v0/LbgC/j9PdXB9yqquvdZYuBW9xlX1XVZ3r7powZaCobK1mxawUAqsoTm5/ghQ9eYHTmaM4adVa325XXlneYP9R0iPrWekKBodOVj6pS/be/Ea48CEDzps1kX3Zpl2dGs1ua21vtRRsaqX3xRVJnzKRx5UrAuX+RUVRJ6Oyz4xa/6V5PrfgeAUYDY1T1L8ClOA0lilT1gaPt2B2y/X7gMpykdp2IzOq02sOqOkdVTwF+DPzM3XYWzhDxs93j/mfbEPDGDEYV9RXt0/sb9nOg8QDVzdXUt9bz4q4XiQa77l4mPyO/w3woECI9Jd3TWJNNeP+B9uTUpmnDhi7XDUU71mO0to6m1e92KGt8b03/BmiOWY83VVQ14g7djqqGVbW0Dz2ZzwW2uiPztgCPAld02n9NzGwIZ+wp3PUeVdVmVd0BbHX3Z8ygVBhqfx6e+lbn8YbMoPOsiaoSTe86Qc0bPY/CDGfbrGAWF024CJ8MrXulvrRU6HRJzpfedZLe7+940SgwejSSmtahTKylbtLwsv+eMXTscaIcZ9j4DtxRe/8F577WhTHbvtlp2zFdbHsr7thU48eP75egjTfq6+tZv349ZWVlzJ49O9HhJJ389Hzmj53Pyn0ryUvLI+gPkpeWB4BPfPgafKhfj7h8lxXM4lPTPkVzpJmgLzgk7534c3JIP2lO+5mPLyOd9NNPP2K90a1h/BrFl5ON+PykFI4g85xzaH7/fepeWg5uTxMZc+23cLJIeAdzqno/cL+IXA98D7ixD9s+ADwAUFRUZN2NJ6n6+nrWrl1Lc3MzCxYsoKyszIY76MLs/NnMzp+NqvLGnjfYcGgDaSlpnDXqLB7IfoAKqeAnL/2Es6aexUUTLiLF9+Gfb6o/NYGRJ17mRz5C2qxZRGprCY4ZgwQ/bMcVbWnh0H//N5+uqaFVhNbde8g4/TSyL74YgPTZswkUFtK6dy8phYUERoxI1NtImOLiYu6+u1dNC3r1I+iuu+6iuLj4OKM6yiW+NiKSJyKni8i4Pux7NxC7/li3rDuPAlce47YmiS1atIiWlhYAKioquOWWWxIcUXITEc4Zcw63zLmFz878LNqi7JN9hCNhHn74YTYe2Mimw5sSHWbSScnPJ3XSpA7JCaDhzTdpWFmKH6c/vpatW2natJmo+5ls2zZ9zpwhmZzASVCq2m+v/khOcJQEJSKTROSvOGcpnwHuFpGlIlLQi32vBKa6+wjiNHpY0mn/U2NmFwBb3OklwLUikioik4CpwNu9ekd9VFxcjIj026u//mMGi5KSEpYtW4a6l0+amppYunQpJSUlCY5s4PiX7/4LkYjTOWd9fT1LliyhqqkqsUENIK179kDKh2eb0ZYWiEQQ39C6VzcQ9dTV0Vjgz8ANqro5pvxE4Mci8gSwRlW77NlcVcMicjvwDE4z8xJVLRORe4BSVV0C3C4iFwGtOONM3ehuWyYijwHrgTDwZVX1pPvc4uLioyaV888/H4Dly5d7EcKgtnjxYurrO/Zp2NDQwOLFi1m0aFGCoho4SkpKWP635aSdk4b4hHA4zObNm3n7mbeZt2heosNLWhqJ0LxlC5HDhyEQJDhmDAoIICkphM47F0lJ+B0OcxSi2vWtGxF5AKcl3YtuMroYJ2HMxDnD+X/Abap6a5xi7VFRUZGWlpZ6sm9LUMfuH/+5mDef+BWNzS0sfz/Myj1RMjIy+NWvfsXNN9+c6PCSXmFhIfv37yeQHyA0LYSkCA3bG8hpyaGiouLoOxiiap5+muYtWwEnWfkyM/nDf/0nKNz2+4dIP/HEBEdoYonIKlUt6lze0znuaar6ojutwBxVPRs4CUhT1Xewpt+s3lXFn1d+wN/X7OFgXXOiw0ku+zdy2cxsTpw5lVBQ+PjUAJPy01i4cKElp1669957CYVCtFa2UvV6FYdfPoz/kJ/77rvv6BsPUZG6Opq3bCVSVUXThg00b94M4TBLMjN5MjvLktMA0lOCCohI2znwZJxLcOAMAz/ZnY7f4PRJaN3ual7auJ89VU1sqajjf9/ZTTgypKukoxqnXcsVV1yB3+88WzJnwnAefPDBREY1oCxatIgFCxa0t5xKS7MEfzQiQrSpiaYtm4nU1hKpraVx3Tqyo/a3OdD0lKBe4sMHa+8CXhCRR4DngHtE5KPAWx7Hl9S2HajrMF/XHGZvtY3Q2S7HaYgZDAQZMWIEgUCAH//mYWti3kclJSUE3ZZphYWFluCPwhcK4R82rP2xfxEhMHIkI8LxGQXW9J+e7hL+EHhaRDaq6t9F5CkgH6gEpuMMx7EwDjEmrWEZQeDDBgAizlDxxlUwDSadB+WltPrS2Zk6k+mnfyTRUQ04oVCIOXPmsH79epYtW2YJvhcyL7yQ8L59RJua8A8bhi8tjXprtTfgdJugVHW/iHwGpx+8/Tg9O0SAs3CeUfqsqu6JT5jJ6YyJuew+3EhFTRN+n3DOlOFkp1mC6mDiuTDxXB7/3t8SHcmAFgqFOOOMM6wXjl5KnTyJ0LxzaFrv9MmXOm0qe60LowGnx3aWqroN+Jj7vNLJbvF9qrqxh82GjIxgCtefOZ5D9S1kBP2kBewPwJhkICJkXXghoTPPRBX8mSH0hz9MdFimj3r1IICqbuHDh2iNq645zIsb97OnqpFROWlcMGOEnUEZk0R8djl0QLOLssfh+fUVbNtfR2NLhO0H6nm2zJ5LMcaY/mIJ6jjsOtTQYb78cEM3aw5RrU0QaU10FMaYAaqnro7uxxlQ8LU4xjOgFGansbuqsX1+RFZaD2sPIdEIbPw77N8APj9MODfREQ0orZFW1lSuoby2nChRpudOR0URHXpDaZihraczqM3Av4vI+yLyYxE5NV5BJZv6+npWrlxJWVlZh/KLZhUyItsZ5iA/K5VLZhd2tfnQs2c1VKx3xteJhGH7cnIDdibVW0+//zQvfvAiS7YtYcnWJfx5459pPqEZxUaU6U5vOn1esWIFK1assE6fB5Cempn/AviFiEzA6Ym8RETSgUeAR2I7kB3MehrLKC8U5LNnTqA1EiXgt6ul7er3H1GUGwxzuNUakBxNfWs9u2p3sb9hP1F1ej6obKwkGooSDVlPCN3pTafPZuA56reqqu5U1R+p6qnAdThjNm3wOrBk0ZuxjCw5dZI3ueO8z8++pmDX65oOAr4Afun4uELswITGDCVH/WYVkRQRWSgifwL+AWwCrvI8siRgYxkdo4LpMPViCOVD9mg48VM0ROwZsd4I+oOcMfIMCjMKnaHexUdBegFEwNdsP4TM0NJTI4mLcc6YFuD0ufcocKuq1ne3zWDT9VhGjXz3Z78j//RLGZmdxmnjh5FiZ1BHGlvkvCKt4LdLe31xWuFpTM6ZzK66XWw9vJX1B9ejKUrDSQ1sr9rO5GGTj74TYwaBnr5ZFwOvAzNU9XJVfXgoJaf9NU18/v/+jNxTLiZl2Mj28pwZZ3HJjV9n2/46XttayfMb7NmnLlWXw5u/hpf/Hd75AyG/ddTZF8PShjEnfw41LTUEfAHC+WHCI8L88t1fUtlYmejwjImLnhLUAiAL+L6I3Boz9EavicilIrJJRLaKyB1dLP8XEVkvImtE5AW3QUbbsoiIrHZfSzpv66X65jCPrypn9KwzmDn3fEKzL8CfNZy0tDROOu9STjnllPZ1N+2rIxK11lUdqML6JdDojtBSXc7cvJrExjQAqSqN4UZ21OxwxqQGaltqebn85cQGZkyc9JSg/gc4DVgLfBz4aV92LCJ+4H7gMmAWcJ2IzOq02rtAkaqeBDwB/DhmWaOqnuK+Lu/LsY/Xjsp6WsJOi6krrriclJQUAvkTKCws5POfvbbDuhlBPz57PKWj1kZoqu5QNDzVmpn3lYgwLXcaTeEPh3DJT8/nUNOhBEZlTPz0dFY0S1XnAIjIg8Dbfdz3XGCrqm539/EozvhS69tWUNWXYtZ/E7ihj8fwRGx/eoFAkBEjCtn9wRpnqIPCMfx9zR5aI4rfJ3xkWgHVja2kBayz2HbBDMgsgLoD7UXWiq9nBxsPsnzXcvY37mds5lguHH8hoUCI+WPns+HgBtY0rUFahHFZ4xiXNS7R4RoTFz2dQbX/5FXV8DHsewywK2a+3C3rzi04rQTbpIlIqYi8KSJXdrWBe+mxVERKDxw40NUqvdIaibJ+Tw3vfnCYuuYw4/LSmTEyq315ujZxyoQ8Zs+ezcT8EP903mSuOm0M188dz7sfHOa/X3uf3768nVU77Zdtu1mfhNyJEEiHwtm8fSg70RElted2PkdFQwWqyq7aXbxS/goAfp+fm0+8meCOIP7DfmYNn8X8sfMTHK0x8dHTGdTJIlIDtF3ASo+ZV1Xtt28cEbkBKAJi//ImqOpuEZkMvCgia93hP9qp6gPAAwBFRUXHdCMoGlWeWFXOPnck3De2H+S6M8Zz2ZxRnDl5OOFIlCd/sLbDNmkBPxOGh3hlywE2V9Syp6qJqCoH6pqZWphlPZoDhIbDKde1z7ZEf5bAYJJbS6TliMt2e+v3tk9nBDJI3en0WHLB+AviGpsxidTtGZSq+lU1W1Wz3FdKzHxvktNunIEN24x1yzoQkYuA7wKXq2pzzPF3u/9uB5YDnnS19MGhhvbkBNDcGmXNbuf+SV4oyIjs7vvX21PVyIa9tRxuaKG6sZVN+2rZtLfWizDNIBb0B8lLy+tQNio0KkHRGJM8uk1QIpImIl8XkV8dYyu+lcBUEZkkIkGc7pI6tMZz+/f7DU5y2h9Tnisiqe50PjCPmHtX/amr0662B3PbRP1B6nOn8vBbH/D6tsr2VntpAT/RmHUDfqGmyRoDmL67eMLFjMgYgYhzn+m8seclOiRjEq6npPMQzn2oV3Ba8c0GvtbbHatqWERuB57BaSRboqplInIPUKqqS4CfAJnA4yIC8IHbYm8m8BsRieIk0ftU1ZMENSEvg4KsVA7UOidvwRQfc8bkdFinfvhMwsEQFTVNVNQ0oQrzTsjntPG5vL61ksq6FoIpPsbmpjM8M9WLMJNacXExd999d6/Wdf+fe3TXXXcNqX7VrP6M6Zp0PltoX+Dc82lrxZcCvK2qp8UzuL4oKirS0tLSY9q2JRxl474amsNRphVmkZP+4T2kmqZWzvunuwC46cabABieGeTzZ08E4IUNFazdXY0qTBiewcKTR1vffMYTmw5t4s29b9IUbmLm8JmcO+ZcfGKfNTPwicgqVS3qXN7TGVSHVny9+eU2UAVTfJw0dliXyzICfiQaRmM67MzN+LDJ9EdnFjJ3Uh7hiJIbsqbUxhu1LbW8uOvF9svP6yrXkZuay5yCOQmOzBjv9PTz62QRqXFftcBJbdNua74hIcXvI+PwdiTqdNUzLD3AuSfkd1gnKy1gycl46kDDgSPujVY0WDdbZnDraTwoe+rUFWysREXwCVQ3tvLWjoNcNLPQOok1cTMiYwQ+8bWPEQXW0s8MfvYN2wtRX4DG3BOIqtPqb8PeWlbtPJzosMwQkhnM5OIJF5OTmkPQH+TkgpOZOXxmosMyxlM2ElovRAIhtNM9uH01Td2sbYw3pgybwpRhUxIdhjFxY2dQveBvqUOiHYfbHpubkaBojDFmaLAzqF7waZjQwY3kZwapa44wY1QWp44bluiwjDFmUBvyZ1DFxcWISI+vFStW8PqzT/L5cybxpQtO4MIZhfj9vi7XtQckjTGmf3T7oO5AczwP6hpjjEmc7h7UHfJnUMYYY5KTJShjjDFJyRKUMcaYpGQJqgct4SgfHGygrvlYBhQ2xhhzPKyZeTf2VDXyt9W7aW6N4hPhwhkjmDM25+gbGmOM6Rd2BtWN17ZW0tzqPJwbVeWVrQcIR6JH2coYY0x/8TRBicilIrJJRLaKyB1dLP8XEVkvImtE5AURmRCz7EYR2eK+bvQyzq7Ud7qs19wapTUyOJrkG2PMQOBZghIRP3A/cBkwC7hORGZ1Wu1doEhVTwKeAH7sbpsH3AWcCcwF7hKRXK9i7cqMUdkd5iflh0gPWgfvxhgTL17eg5oLbFXV7QAi8ihwBdA+dLuqvhSz/pvADe70x4DnVPWQu+1zwKXAIx7G28GZk/JID/h5/2A9BZmpnD4xrvnRGGOGPC8T1BhgV8x8Oc4ZUXduAf7Rw7ZjOm8gIrcCtwKMHz/+eGI9gohw8rhhnGx97hljTEIkRSMJEbkBKAJ+0pftVPUBVS1S1aKCggJvgjPGGJMQXiao3cC4mPmxblkHInIR8F3gclVt7su2Xjh06BCf/OQnCYVCTJgwgYcffrjL9X7yk59w4oknkpWVxaRJk/jJT47Mrb/4xS+YNGkSoVCImTNnsnnzZq/DT7j+qr/XX3+duXPnkpWVxUknncSrr74aj/CTgtXh8elt/f385z9n8uTJZGdnM3r0aP75n/+ZcPjIZx5XrFiBiPC9733P69CTRn/V4XF/BlXVkxfO5cPtwCQgCLwHzO60zqnANmBqp/I8YAeQ6752AHk9He/000/X/nDttdfq1VdfrbW1tfrKK69odna2rlu37oj1fvSjH+mqVau0tbVVN27cqOPHj9dHHnmkfflvf/tbnTNnjpaVlWk0GtWtW7fqwYMH+yXGZNYf9Xfw4EHNy8vTxx57TMPhsP7hD3/QYcOG6aFDh+L9dhLC6vD49Lb+tm7dqocPH1ZVp74uuOAC/elPf9phnZaWFj355JP1zDPP1O9+97vxCD8p9Ecd9uUzCJRqV3mkq8L+egEfBza7Sei7btk9OGdLAM8DFcBq97UkZttFwFb3dfPRjtUfCaqurk4DgYBu2rSpveyGG27Q73znO0fd9itf+YrefvvtqqoaiUR07Nix+vzzzx93TANJf9Xf0qVLddasWR2WT506VX/3u9/1b8BJyOrw+Bxr/VVWVupHP/pR/T//5/90KL/33nv1W9/6lt54441DJkH1Vx325TPYXYLy9B6Uqj6lqtNUdYqq/sAtu1NVl7jTF6lqoaqe4r4uj9m2RFVPcF//7WWcbTZv3kxKSgrTpk1rLzv55JMpKyvrcTtV5ZVXXmH27NkAlJeXU15ezrp16xg3bhyTJk3irrvuIhod3A/69lf9tZV1XmfdunX9G3ASsjo8Pn2tv4cffpjs7Gzy8/N57733+OIXv9i+bOfOnZSUlHDnnXd6Hncy6c86PN7PYFI0kkgWdXV1ZGd3fP4pJyeH2traHrcrLi4mGo1y8803A06CAnj22WdZu3YtL730Eo888ggPPvigN4Enif6qv7PPPps9e/bwyCOP0NraykMPPcS2bdtoaGjwLPZkYXV4fPpaf9dffz01NTVs3ryZ2267jcLCwvZlX/3qV/n+979PZmampzEnm/6qw/74DFqCipGZmUlNTU2HspqaGrKysrrd5le/+hW///3vWbZsGampqQCkp6cD8O1vf5thw4YxceJEvvjFL/LUU095F3wS6K/6Gz58OE8++SQ/+9nPKCws5Omnn+aiiy5i7NixnsafDKwOj8+x1B/A1KlTmT17Nl/60pcAWLp0KbW1tVxzzTWexZqs+qsO++MzaJ3Fxpg2bRrhcJgtW7YwdepUAN57770Ol01ilZSUcN999/Hyyy93qPTp06cTDAYRkfay2OnBqr/qD2D+/PmsXLkSgHA4zOTJk/nGN77h7RtIAlaHx6ev9RcrHA6zbds2AF544QVKS0sZOXIkANXV1fj9ftauXcuTTz7p3RtIAv1Vh9APn8GubkwNxFd/teK75ppr9Nprr9W6ujp99dVXu2298sc//lELCwt1/fr1Xe7nc5/7nC5YsEBramp0165dOn369EF/g1q1/+rvnXfe0ZaWFq2urtavfe1res4553gdetKwOjw+va2/3/72t1pRUaGqqmVlZTpr1iz953/+Z1VVramp0b1797a/rr76av36178+JFriqvZPHar2/jNIIlrxxfPVXwnq4MGDesUVV2hGRoaOGzdO//SnP6mq6ssvv6yhUKh9vYkTJ2pKSoqGQqH21xe/+MX25dXV1XrNNddoZmamjh07Vu+++26NRqP9EmMy66/6u/baazU7O1uzs7P16quvbv8jGAqsDo9Pb+vvpptu0hEjRmhGRoZOmDBBv/nNb2pjY2OX+xxKrfhU+68Oe/sZ7C5Bierg6KG7qKhIS0tLEx2GMcaYPhKRVapa1LncGkkYY4xJSpagjDHGJCVLUMYYY5KSJShjjDFJyRKUMcaYpGQJyhhjTFKyBGWMMSYpWYIyxhiTlCxBGWOMSUqeJigRuVRENonIVhG5o4vlHxGRd0QkLCKf7rQsIiKr3dcSL+M0xhiTfDzrzVxE/MD9wMVAObBSRJao6vqY1T4AbgK+2cUuGlX1FK/iM8YYk9y8HG5jLrBVVbcDiMijwBVAe4JS1ffdZYN7qFljjDF95mWCGgPsipkvB87sw/ZpIlIKhIH7VPVvnVcQkVuBW93ZOhHZdIyx9kY+UOnh/ocCq8PjY/V3/KwOj49X9Tehq8JkHrBwgqruFpHJwIsislZVt8WuoKoPAA/EIxgRKe2qt13Te1aHx8fq7/hZHR6feNefl40kdgPjYubHumW9oqq73X+3A8uBU/szOGOMMcnNywS1EpgqIpNEJAhcC/SqNZ6I5IpIqjudD8wj5t6VMcaYwc+zBKWqYeB24BlgA/CYqpaJyD0icjmAiJwhIuXAZ4DfiEiZu/lMoFRE3gNewrkHlegEFZdLiYOc1eHxsfo7flaHxyeu9TdoRtQ1xhgzuFhPEsYYY5KSJShjjDFJyRIU4N4Xu6gf9nObiHy+P2IazERkuYj0uqmqiJwvIn/3MqZEEJGvisgGEakXkVndrDNORF4SkfUiUiYiX4tZ9iMRWSMiv48pu0FEvh6H8BOiN3XmrlciIvtFZF2n8jwReU5Etrj/5rrln3Lr9xURGe6WTRGRP3v7juJDRF7vxTpfF5GMOMTS679nS1CAqt6pqs93Lne7a+rLfn6tqr8/+prGAPAlnK7AHge6+7INA99Q1VnAWcCXRWSWiOQAp6nqSUCLiMwRkXTgZpwuxgar3tQZwP8Al3ZRfgfwgqpOBV5w5wG+ApwB/Aa43i37N+B7xx9y4qnqOb1Y7etAnxJUX78j+2pQJigRmSgiG0XkT+6vrSdEJENE7hSRlSKyTkQeEBFx1/+fts5qReR995fpOzitC9v2OUJEVrnTJ4uIish4d36bu/9iEfmmW/ZV91fvGrebJ0Qk5P6ye1tE3hWRK+JcNcelh3r9qPt+1rrvr+0RgS7LO+3zEhF5Q5xOgx8XkUy3/FL3WO8AV8X5rXpORH4NTAZ2ADcCPxGnY+Qpseup6l5VfcedrsVpETsGiAIB9zOcAbTi9Gn5S1Vtjd87iZ/e1hmAqr4MHOpiN1cAD7nTDwFXutNRIBW3LkXkPGCfqm7p1zeRICJS5/57vnsF44mYv2URka8Co4GXROQld93u/jY7fEe683e7660VkRnuenPd7d8VkddFZHqfA1fVQfcCJgIKzHPnS3D+ePNi1vkDsNCd/h/g0+70+8C3u9lvGZCN03x+JfBZnC463nCXFwPfdKf3AKnu9DD33x8CN7SVAZuBUKLr6zjr9Xs4XVpNc8t+j/NLLK2rcnd6OVCE023Ky211AHwHuDNm26mAAI8Bf0/0+/egPt9366D989eL+v8AyHbnvw2sBn4KjBqMdXQ8debW17pOZVUx09I2j3NWtgpYCuQAz8Z+Xwz0F1Dn/ns+UI3TcYIPeAM4N7Zu3eku/zZj1vt2zL7fB77iTn8J+J07nQ2kuNMXAX+JiaFXn9VBeQbl2qWqr7nTfwTOBS4QkbdEZC1wITC7m227u+78Os5Dwx/BSTYfAc4DXuli3TXAn0TkBpzLNACXAHeIyGqcL+k0YHwf3lMy6FyvHwV2qOpmt+whnHqZ3k15rLNwLtO85tbJjTgJf4a77RZ1PtF/9OrNDBTur9e/4CT5GgBV/bGqnqKq3wC+D9wpIv8kIo+JyKC4NOUl97Ol7vRzqnq6qi7EOct6Cpjmnmn8Nh73ZuLobVUtV9Uozg+ciV2s093fZpvO35H/6/67KmZ/OcDj4twH/Dndf992azAnqM4PeCnwnzi/uuYAv8VJEF2pBxCR/3YvITzllr+Mk5AmAE8CJ+Mkvq4S1AKcewGn4Qw1koLzi+1T7pfKKao6XlU3HPM7TIzO9Vp1HPsS4LmY+pilqrccx/4GPHEaRbSNg3abWxbASU5/UtX/7WKbU3HqchPwGVW9GpgiIlPjGXuidFVnPagQkVHudqOA/Z32lYEzBND9wN04X8yv4lwtGSyaY6YjdN0n69H+Nuu72Wfs/r4PvKSqJwIL6f77tluDOUGNF5Gz3enrcT5kAJXur9FPd73Zh1T1Zvc/5+Nu0SvADcAW99fHIeDjMfsGQER8wDhVfQnn1DgHyMTpVeMrMfe+BmL/gp3rtRSYKCInuGWfA1bgfFl2VR7rTWBe2zruPbppwEZ327Z7C9d581aSRi2QBaCqu2K+FH7tflYeBDao6s+62f77wL8CAaDtpnWUPt7wHmC6rbOjbLcEJ+ng/vtkp+XfAv5Dnft46Tg/yAZ7XbZpr1O6/9vsixw+7H/1pmMJaDAnqE04LZ42ALnAf+GcNa3DSRQr+7pDdcavEpwzKXASU5WqHu60qh/4o3sp8V2cD3wVzhdJAFgjTrdO3+9rDEmgc73+HKfl2OPu+40Cv1bVpq7KY3ekqgdwPriPiMganOvhM9xtbwWWuTdiO/zKHYQeBb7l3kzufMN/Hk5yvzDmLKHtBxMiciVQqqp73M/Yare+01T1vTjFnwg91Rki8gjO52m6iJSLSNuv//uAi0VkC859kftithkNzNUPh/b5Jc73xG3Aw569k+TxAPC0iLzU3d9mH/f3Y+BeEXmXYxw5Y1B2dSQiE3Fuwp2Y6FgGE6tXY0w8DeYzKGOMMQPYoDyDMsYYM/DZGZQxxpikZAnKGGNMUrIEZYwxJilZgjImgURkmIh86Ri3fV9E8vs7JmOShSUoYxJrGE7/ZcaYTixBGZNY9+F0S7RaRH4uIi/E9Ap9BbQ/xb9MRN4Tpyf+a2J3ICLpIvIPEflCQt6BMR45pqd7jTH95g7gRFU9xe2vMUNVa9xLd2+KyBKccY32qOoCAHHGgmqTidOrwu/VxiIzg4ydQRmTPAT4odu1zPM44z4VAmtxuuf5kYicp6rVMds8Cfy3JSczGFmCMiZ5fBYoAE5X1VOACpw+9Tbj9Iq/Fvg3EbkzZpvXgEvbOiA2ZjCxBGVMYsX2IJ0D7FfVVhG5AHf8HbcT0wZV/SPwE5xk1eZO4DCDe5h3M0RZgjImgVT1IM6gcOuAU4Aitzfyz+MMOwIwB3jbHTjuLuDfOu3ma0C6iPw4LkEbEyfWF58xxpikZGdQxhhjkpIlKGOMMUnJEpQxxpikZAnKGGNMUrIEZYwxJilZgjLGGJOULEEZY4xJSpagjDHGJCVLUMYYY5KSJShjjDFJyRKUMcaYpDRoBizMz8/XiRMnJjoMY4wxfbRq1apKVS3oXD5oEtTEiRMpLS1NdBjGGGP6SER2dlVul/iMMcYkJUtQxhhjkpIlKGOMMUnJEpQxA0x9az0NrQ2JDsMYzw2aRhLGDHZRjfL8zufZWrUVQZg1fBYfGfsRRCTRoRnjCTuDMmaA2HJ4C1urtgKgKGUHy9hVuyvBURnjHUtQxgwQVc1VR5Qdbj4c/0CMiRNLUMYMEBOyJyB8eDnPJz4mZE1IYETGeMvuQRkzQIwMjeSSiZew5sAafOLj1BGnMixtWKLDMsYzlqCMGUCmDJvClGFTEh2GMXGR1Jf4RGSYiDwhIhtFZIOInJ3omIwxxsRHsp9B/QJ4WlU/LSJBICPRARljjImPpE1QIpIDfAS4CUBVW4CWRMZkjDEmfpL5Et8k4ADw3yLyroj8TkRCsSuIyK0iUioipQcOHEhMlMYYYzyRzAkqBTgN+C9VPRWoB+6IXUFVH1DVIlUtKig4YigRY4wxA1gyJ6hyoFxV33Lnn8BJWMYYY4aApE1QqroP2CUi092ijwLrExiSMcaYOEraRhKurwB/clvwbQduTnA8xhhj4iSpE5SqrgaKEh2HMcaY+EvaS3zGGGOGNktQxhhjkpIlKGOMMUnJEpQxxpiklNSNJIwZ6lojrbyx9w121+2mIL2Ac0afQ0bAuqQ0Q4MlKGOS2Cu7X2HjoY0AHG46TF1rHVeecGVigzImTuwSnzFJbGfNzg7ze+r20BppTVA0xsSXJShjklhuWm6H+axgFik+u/BhhgZLUMYksfPGnEdOag4AGSkZnD/ufEQkwVEZEx+e/xQTkTmqutbr4xgzGA1PH871M66npqWGzEAmfp8/0SEZEzfxOIP6TxF5W0S+5A5CaIzpAxEhJzXHkpMZcjxPUKp6HvBZYBywSkQeFpGLvT6uMcaYgS0u96BUdQvwPeA7wHzgP0Rko4hcFY/jG2OMGXg8T1AicpKI/BzYAFwILFTVme70z70+vjHGmIEpHu1Vfwn8Dvi/qtrYVqiqe0Tke3E4vjHGmAEoHpf4/qqqf4hNTiLyNQBV/UMcjm+MMWYAikeC+nwXZTfF4bjGDBrVzdVsPryZmpaaRIdiTNx4dolPRK4DrgcmiciSmEVZwCGvjmvMYLPp0CZe/OBFFEVE+Oj4jzItd1qiwzLGc17eg3od2AvkAz+NKa8F1nh4XGMGlTf3vomiAKgqb+19yxKUGRI8S1CquhPYCZzt1TGMGQyKi4u5++67u11esLAA8X/YvVG0Ncrnl3V15dxx1113UVxc3J8hGpMQoqre7FjkVVU9V0RqgdiDCKCqmt2LfbyPc8YVAcKqWtTdukVFRVpaWnqcURuTfF4pf4Vv/Ps3ALjppps4ZcQpnDP6nARHZUz/EZFVXX2/e3kGda77b9Zx7uoCVa3sh5CMGZDmjZlHcFeQaCjKBeMuYEbejESHZExcxONB3f8QEbvMZ8wx8omPlIMpBD8IMnP4TOvN3AwZ8Whmvgr4VxHZJiL/LiLdXqbrggLPisgqEbm180IRuVVESkWk9MCBA/0WsDHGmMSLR2exD6nqx4EzgE3Aj0RkSy83P1dVTwMuA74sIh/ptO8HVLVIVYsKCgr6N3BjjDEJFc8BC08AZgATgI292UBVd7v/7gf+Csz1LDpjjDFJJR73oH7snjHdA6wDilR1YS+2C4lIVts0cIm7vTHGmCEgHp3FbgPOPoaWeIXAX90bwinAw6r6dH8HZ4wxJjl52dXRDFXdCKwExovI+NjlqvpOT9ur6nbgZK/iM8YYk9y8PIP6F+BWOnZz1EZxxoMyxnQSjoZ5d/+7VDRUMCo0ipML7HeaGZq8fFC3rVn4ZaraFLtMRNK8Oq4xA92K8hVsOrQJgA9qPqC2pTbBERmTGPFoxfd6L8uMMcCWw1t6nDfGSwd21VL2ym42vrmXmsrGo2/gIS/vQY0ExgDpInIqTh98ANlAhlfHNWagCwVCHc6aQoFQAqMxQ0nV/gZ2rP6w04O6Q02cdOE4gmnxaE93JC/PoD4G/DswFvgZzr2on+Lcm/q/Hh7XmAHt3DHnkuJzvhACvgDzxsxLcERmqKiqaOgwH40o1QcSdxbl5T2oh4CHRORTqvoXr45jzGAzKWcSn5/1eQ42HSQ/PZ9Uf2qiQzJDRHpmsIuyQAIicXh5ie8GVf0jMFFE/qXzclX9mVfHNmagS0tJY0zmmESHYYaYgvGZVFc2ULWvAfEJIyflkJmbuDZtXl5YbLtwnunhMYwxxvQTn9/HtDNG0tIYxucXUoL+xMbj1Y5V9Tfuv3d39fLquMYYY/qmuLgYEWl/pWYECKSmdCjry6u/RnSOV1982SISEJEXROSAiNzg9XGNMcb0TnFxMara42v+/PnMnz//qOup6sBJUMAlqloDfAJ4H6dX82/F4bjGGGMGsHgkqLb7XAuAx1W1Og7HNMYYM8DF4+mrv4vIRqAR+D8iUgA0HWUbY4w5JhqJ0LR2La37KgiMHkXaiScivngOfWf6i+cJSlXvEJEfA9WqGhGReuAKr497vHYdaqD8cCOjh6UxYbg9yW/MQFH38ss0rSsDoHnLFiJVVWR+5CNH2coko3j1XzED53mo2OP9Pk7H7rNVOw/x8uYPh686Z8pwzpw8PIERGWN6q3ljxwG7mzZstAQ1QHmeoETkD8AUYDUQcYuVJE5Qpe8f7jC/6oPDlqCMGSAkPR2trWuf96WnJzAaczzicQZVBMxSVY3DsfqFSKd5BFVFOi8wxiSd0DnnUPv88xCJgt9HaN45iQ7JHKN4JKh1wEhgbxyO1S+KJuaxYpPTo++B2mb8PuGXL25l1qhsLpgxAr/PEpUxySpt2jQCo8cQ3r+fQOEIfKEQ0QanE1Rfhg2kMJDEI0HlA+tF5G2gua1QVS+Pw7GPyWnjcynMTmNLRS0vbz5AVlqASFRZu7ua4ZlBTh2fm+gQjTE98GeG8GdOQlWpffFFmtZvACBt5gwyL7zQroYco4yUHBpqWsjIPrJTWS/EI0EVx+EY/W7MsHQaW8JkpXXsybeixlrIGzNQtOzYQVPZ+vb5pvUbCE6cSOqUKQmMauBpbY4wZVgRqSkh1q0oJ39cFpNPKfD8uJ4/HKCqK3B6kAi40yuBd3q7vYj4ReRdEfm7RyF2a2RO+hGX88YMs0sExgwUkcMfNnhSVbS1lcihQwmMaGCq2FFNasqHj9tU7qql7nBzD1v0j3j0xfcF4AngN27RGOBvfdjF14AN/RxWr2SmpvDxOSPJzQiQFvBTNDGXE8dkJyIUY8wxCE6YACJEqqpoXL2ahnffpXHtOiJ1dUff2LRrbY50URb2/LjxeLz6y8A8oAZAVbcAI3qzoYiMxeki6XdeBde5F9/Or6mF2dx87mS+dMEJfGTaCHw+X1x68TWmJ63RVvY37Kc10proUJJaSn4+WR+7hHBlJb60VFKnTiVaX0/9a68nOrQBZfiYTIhpiB1ITSE73/vm+/G4B9Wsqi1tNyXdh3V72+T8/wHfBrK6WigitwK3AowfP/6YgisuLj5qUjn//PMBWL58+TEdw5j+tKduD//Y8Q+aI80E/UEumXAJ47OP7fM/FARGjCD1hBM6lIUPVnaztulKdn46O2vWkps2khETshk5JQd/ivfnN/E4g1ohIv8XSBeRi4HHgaVH20hEPgHsV9VV3a2jqg+oapGqFhUUeH/Dzphk8OruV2mOONf/WyItvFz+coIjSm6+7Gz8w4Z1KAuOn5CYYAaw+tbDlNduYOJJ+aSF4jMMfDwS1B3AAWAt8EXgKeB7vdhuHnC5iLwPPApcKCJ/9CpIYwaKmpaaDvO1rbUMoOfg405EyP7EAoITJ+DPySb95JMInXVmosMyvRCPVnxRVf0t8FngB8CTvelVQlUXq+pYVZ0IXAu8qKo20KEZ8ibnTD5ifqg/13O0e8mBvDyGXX45w2+8kaz58/EFAnYveQDw7B6UiPwa+KWqlolIDvAGTl98eSLyTVV9xKtjeyEcCPHqlkoyUv3MHp1Naoo/0SGZIeq8seeREchgb91eCjMKKRpZlOiQEs7uJXuvvr6e9evXU1ZWxuzZs+NyTC8bSZynqre50zcDm1X1ShEZCfwD6HWCUtXlwPJ+j7CXwsEs6gpOZOX7zvMTG/bWcP3c8UP+V6tJjIAvwFmjzkp0GGYIqa+vZ+3atTQ3N7NgwQLKysoIhbwfhsjLS3wtMdMX4z77pKr7PDymJ5pDI9GYZLS/ppndVY0JjMgYY+Jn0aJFtLQ4X+kVFRXccsstcTmulwmqSkQ+ISKn4jR4eBram5kPqP7vRaNHlKXYCJ3GY/vq97H58Gaawta9lkmckpISli1b1t4Qp6mpiaVLl1JSUuL5sb28xPdF4D9wejL/esyZ00eBZR4et9+l1u2hJePD8aAmF4QYmZOWwIjMYLd813LWH3T6kAv6g0TTo/ga7UeRib/FixdTX1/foayhoYHFixezaNEiT4/tWYJS1c3ApV2UPwM849Vxj9X6PTWs31tDRtDP3El55Gemti/zhxvJ3vcuF88qJCPoZ6INAW88VN1c3Z6cwHnWqXVEK6k7U3vYyhhv3HvvvXz1q1/tkKQyMjK47777PD+2/SQDtu6v5Zmyfew61MCmfbX8ZVU5LeGOl/V80VZOHJPD5IJMfDYelPFQa/TI7ouaw82sXLmSsrKyBERkhrJFixaxYMGC9kZhaWlpLFy4kJtvvtnzY1uCArZUdOw4sqElYo0gTMLkp+dTmFHYPt/a2sqWV7fQ0NDAggULjrjcYj5kDyx7o6SkhGDQGQOqsLCQBx98MC7HtQQFZKcf2W1Hdlo8uik0pmufmPIJzhp1FrOHz2b5/cup3+UkpXi2oBpImrdv59BDD1H5X/9FzXPPoa3WiW5/CoVCzJkzh4yMDJYtWxaXJuYQxwQlImeJyNMislxErozXcXujbQRdAJ8IcyflMTyz6+v9ZXuqefitD3isdBfvV9ovWeONVH8qpxWexrbntvHcE88lpAXVQBFtaqL22WeJ1NRCJErzxk00vPNuosMadEKhEGeccUbcHtIFb3uSGNnpmad/AT4JCPAWfRsTylPpQT/XnzmeA7XNpAf9ZKZ2XS0fHGzg2bKK9vkl1Xu48ZyJ5HRxBmZMf0hkC6qBIlx5EG3tODZRuGLAPW5puuDlGdSvReROEWlrj10FfBonSdV0u1UCFWSldpucALZXdrxXFYkqHxxs8DqsgaduP9QfTHQUg8K99957xOWUeLWgGihSCvKRQMcfiYHRo3u9ffOOHdS/8QYtO3f2d2iDWktjmLrDzWjUu/t+XjYzv1JEFgJ/F5HfA18HrgcygCu9Oq6X8ru47Dc8M5iASJJUpBXWPg6H3T/0ETNg1pVgXUIds0WLFvHMM8/w+OOPo6pxbUE1UPhSU8m+7FLqXn2VaF09qdOmkn7qqb3atv6tt2l4+213bhWhs88io8j6NjyaPVsOs3tzFRpVUkMBZpw9itT0/k8nnt6DUtWlwMeAHOCvOP3x/YeqHvDyuF6ZOSqb6SOzEAG/TyiamMvoYQOqUwxvVaz7MDkB7N8IB7clLp5BIlEtqAaS4IQJ5H32s+R/8VayLrgA8feuM+fG1at7nDdHamkMU76pqv3Mqbm+lT1bqjw5lpf3oC4H/hkIAz8E/gD8q4h8Cfiuqg64by6/T/j4nFGcP70AnwhpAevRvIOm6iPLmrsoM33S1oJq/fr1cW1BNSR0fqbRZ3/TR9PSFO4w/Ds4ScsLXp5B/RtwGXA18CNVrVLVbwD/ijMuVNJS1R6fp8gIplhy6krBDJCYj5Q/BYaf0P36ptcS0YJqKOh8OS+j6PQERTJwhHJSSe00om7eaG9+NHn5sE81cBUQAva3FarqFpwBCJNONKos37yfst01BFN8zDshnxPH5CQ6rIEjayScdDXsXuX8Eh13JqRZ/fVWS6SFvfV7GZY6jJxUq7d4yDj1VAIjR9K6bx+B0aMJFBYefaMhTnzCjLNGsmdrNS2NYfJGhygYl+XJsbxMUJ8ErsMZduN6D4/Tb9btqea9Xc4lqXBLhOc3VDA21+4x9UneJOdl+qSivoKl25fSEmlBEM4efTanjDgl0WENKt0NuBcYNYrAqFEJjGzgSc0IMOmkfM+P49klPlWtBH4FrAYuEpGrRORMSeJR/vZVdxzWQBUqapoTFI0ZSt7e9zYtEWe8HUU7zJvj1zbgHo1N/OvChez/619p3rIl0WGZo/AsQYnIJcAWoBj4uPu6G9jiLks6YzqdLflEGDXs6MNq1DWH2V3VSMTD5wHM4NYY7tj3YzgatgTVj9oG3LsiJ5vxDY385f/9gpqnn6Fp06ZEh2Z64OUlvl8AF6nq+7GFIjIJeAqY6eGxj8msUdlUNbSybnc1qe49qOy0nnuJeOeDw7yyuZKoKllpKXzy1DHddpNkTHem502ncndl+/yYzDFkBjMTGNHg0TbgXr7PT54/hXA4zObNm3n33Xc5c8J40qZPT3SIphteJqgUoLyL8t1AUvYNJCLMOyGfeSd0vLba3bXrptYIr21xkhNAbVOYN7cfYsFJdj3b9M3JBSeT5k9jR80O8lLzOHnEyYkOadBo6y5KfD4URRBaW1t54YUXOPv66xIdnumBl83MS4CVIvIdEbnefd2B0w/fUZ80FJE0EXlbRN4TkTIRudvDWLvVdu26q6EOGloihDtd1qttsl6UzbGZnjedSydeytxRc0n121l4f2nrLqouGuWdxkYUJRAIcP7HP269RiQ5LxtJ3At8Fqdz2LPdF8Bn3WVH0wxcqKonA6cAl4rIWV7E2pO2a9dw5FAHeaEgI7I7fpFMG+lNc0tjzLGJHXDvzYYGnmhspPGsM7n0l/+Bf9iwRIdneuDpoEequh5oH7taRPLd1n292VaBtt5ZA+4rrq0Q2q5dp+RPJCU7n3BNZftQB209SV95yhjefv8QVQ0tTCnI5KSxw+IZojGmF0pKSnjyySdpbm4mvaCAn/7xj0iKjfmW7LxsxXeZiOwQkVdF5FQRKQPeEpFyEfloL/fhF5HVOA/6Pqeqb3VafquIlIpI6YED/d+93+LFi4kUTCVj2tkERzr/RkdMY/Hixe3rhFJTuGD6CD556lhLTsYkqUQNuGeOj5f3oO7FaVr+LeB54BZVnQJcDPykNztQ1YiqngKMhf/f3p3HR1Wfix//PNmTyUIEAgjIjlRAUGNtiyLV9taKSKVV26pE5V4v9d56e2tbpbdCLP5ardf6+v1qWy91wQWx11JrURS7CFiXFlCRsAjIvsgWQhZIMpl5fn+ckzAJCZmQnDkzk+f9es0r55w5y3O+OXOes36/fFpExrT4fp6qFqtqce/evbs0eHCuXeee1bxqmcDAc6ypA+Or8tpy/rrzryzdvpRdVbv8DidhBHJy+PJ55zE0rISqqlBVgvv20eDBwa3pGl6e44ZVdQOAiBxT1XcBVHWDiHQoMapqhYi8AVwBlHV9qM4Teev3VVJbH2JUv3zOCGRw6623Mv+dnazduBlVSEtLY/TIodbUgfFNbUMtL25+kbqQ8wL51oqtXDPiGvoG+vocWfwbW1fPWcEg1cuWgYCkZ6Du/eWMoUPIv/JK4rgegW7JyzOoChH5VxH5PnBERP5TRPqLSAkn7i21SUR6i0gPtzsb58xroxeBhsLKC6t2sfyjg/x9WzkL3t3BgSqnVomf3z2TlBSnmHIDAR6661+9CMGYqOyo3NGUnMCpdWLzEasRoT2h6mrOajjxhG1w3yccX7u2qb9+6zaCO3f6EVpcKC0tRURO+Vm+fDnLly9vdzwRobS0tEvi8jJBlQDnA0OBxpojluLUbv4vUUzfD3hDRD4EVuLcg3rZi0B3lR/jUPWJt/YbwkrZHqdOvuJhfRlcv4PwrvdZOHcmxcPsSLWlaDbujny6auNORq29vBtIt/sp7QqFmj1iFao8SnDfPmrXr6fhkPPcVjjiFZLuprS0tKkVh2g/ezaV897SHXzw550c3FXV7Luu+g172aLuLiDydONh9xPt9B8C0TWL2UmpLduEwanmqFFeZgrnDynis+efG4twEk5paWm7G+SkSZMAWLZsmefxJLP+uf0ZUTii6aypV3YvRveyJjjak1pQwIG0VIoaQmgwSOhwOaQIoepqQtXVSE4OGYMH+x1mwijfW8PujUea+rd+cJBAjwyyu7iFcXvOEhhQmM2Awmx2H3HqQ8vOSGX8wB6nnKa8pp7D1XX0L8wmJ8OK0cTOFwd9kfOLzicYDtInp4/dN4nS6qwsBjQ0kN6/P1m1tYAS3LMXUoSsUWeTkpPjd4gJo6q8ecXaqFJdXmcJygsiwrTzB7DtUDW1wTDDeueSndF2g4Srd5SzYpNzWSA9VZg6vj8Dz7CN28ROz+yefoeQcMIi7ExPJ3fSJIL791O/dSuhykoAgrv3oKFQ1E3Fd3eBHifXdJJT0LXJCby9B5VQUlOE4UV5jOlfcMrkFAyFeXdreUS/8vbHUb17bIyJA+l9ikg/sx8N5YcBSM3PR4NB6rZs8TmyxNGzf4C+QwtISRXSMlI5a0xPAgVdXz2X52dQItIPpwXdoTgv3P5WVTd5vVyvhMJKMBRuNqw2GG5jbGNMPMocNoyc8eeh4RApmU6TOqGKoz5HlThEhLNG92Tgp85wHtn36DKzp2dQInIHMB/4GPglsBz4mYh8saPvQsWLrPRUhhc1f5JqTP98n6IxxpyOjMGDkazMpuREipA51FqC7ihJEU/vgXp2BiUik4HP4Lxcey3waferV4FZwBAR2evVo+Ne+tLovvQrqOBgVT2De+Uwqq8lKGMSSVphIQVXX83x99+HcJjsceNI86A2GtM5Xl7iuwO4WVVVRIqB4cDrOC/c/h34PfAckHAJKj01hQsGneF3GMaYTsgYMICMAQP8DsOcgpeX2YpUdZ/b/Tngq6r6KPA14BK3VvM+Hi7fGGNMAvMyQVWLSGPTtEeBq0QkA7gKqBKRAFFUeWSMMV0lMxymdtMmGg4f9jsUEwUvE9R84IdudwnweeAP7t8S4LvAQg+Xb4wxTXo1NHDZsWNULX2dI88t5Njq1X6HZNrhdZPvfUXkJ0Ctqn5XVa8EfgT8G04rub/0cPnGGNNkRH2QlIj6+I6tXIkGg21PYHznZV18CnzTrb38JRFJBcI4VTYuBErdcYwxxnMZLXY32hByao9IT/cpItMez1/UVdWngKe8Xo4xxpzKzvQ0zqk70WpB5rChpGRl+RiRaY/VxWeMSXilpaXce++97Y43PCOD1344i0MNDayrrSXUxnhz5syxZl/igCUoY0zCi6bJF5N4LEEZk4BqG2pZc3ANR+uOMqRgCCMKR/gdkjFdLhaVxV4LvKaqVSLyI5xWdu9T1fe8XrYxyWrJtiV8UvMJAFsqthAMBzmn5zk+R2VM14pFha33uMnpYuALwOPAr2OwXGOSUkVtRVNyavRR+Uc+RWOMd2KRoBrvQ04G5qnqK0DXt2xlTDeRkZpxUg3S2WnZPkVjjHdikaD2iMj/ANcDS0QkM0bLNSYp5aTnUNynuKk/MzWT4r7Fp5jCmMQUi4ckrsNpcuO/VbXCbcDw++1NJCIDgadxKpRVnLOv/+tppMYkiAv7XsjwHsM5WneU/rn9SU+1l01N8olFguoHvKKqdSIyCTgXJ/G0pwG4U1XfE5E8YLWI/ElV13sXqjGJozCrkMKsQr/DMMYzsbjUtggIichwYB4wEKcdqFNS1X2NT/qpahWwAejvZaDGGGPiRywSVFhVG4BpwC9U9fs4Z1VRE5HBwHk4DR1GDr9NRFaJyKqDBw+eVnClpaWIyCk/y5cvZ/ny5e2OJyL2sqDpEmENs2L3Ch5b+xgLNiwgVNBWnQfGJK9YJKigiHwDmM6J1nOjvmAuIrk4Z2HfUdXKyO9UdZ6qFqtqce/TbK65tLQUVe2yjyUo0xXWHlpL2aEy6kP1HK07St3gOjTN6lY23UssEtQtwGeB/6Oq20RkCPBMNBOKSDpOclqgqr/3MEZj4sq+mn3NBwiEc8L+BGOMTzxPUO5DDXcBjfeTtqnqA+1NJ86LHo8DG1T1595GabxWU1PDypUrWbdund+hJIS+OX2bD1BIOWZvZ5juJRZVHU0B/hvn5dwhIjIe+LGqXt3OpBOAm4C1IvKBO+yHqrrEq1iNN2pqali7di11dXVMnjyZdevWEQgE/A4rbrRaE7dA3rg8sgZmEa4LU11WTd3eupNe0G2N1cRtkoV43WagiKwGLgOWqep57rAyVR3TlcspLi7WVatWdeUsTRe5/vrreeGFF1BVsrKymDp1Ks8//7zfYRlj4oSIrFbVk942j8lDEqp6tMUwu5jeTTzxxBO88sorNB4I1dbWsnjxYp544gmfIzPGxLtYJKh1IvJNIFVERojIL4C3Y7BcEwdmzZpFTU1Ns2HHjh1j1qxZPkVkjEkUsUhQ3wZGA3XAQqAS+E4MlmviwE9/+tOT7jfl5ORw//33+xSRMSZRxOIpvmOq+l+qeqH7ztJ/qWqt18s18eHWW29l8uTJTTf3s7KymDJlCrfccovPkRlj4l0sHpIYCXwPGEzEU4OqellXLscekohfNTU19OzZk7q6OgYNGmRP8XWBQ8cPse6Q88j+6F6j6ZXdy+eIjDl9bT0kEYvKYl8AHgUe40TbUKYbCQQCjB07lvXr1/PKK69YcuqkyvpKXtz8IsFwEIBNRzZx/ajryc/I9zmyxBaur0fS06N6lN/ERiwSVIOqWgu63VwgEOALnxvP6OGD/A4l4W2t2NqUnACC4SAfV3zMeUXn+RhV4gpV11C1dCnBvXtJycsl7/LLyRg40O+wDLF5SGKxiNwuIv1E5IzGTwyWa+JF/TEm9zvENf0PwTu/hG0r/I4oobXWeq61qHv6at5+i+DevQCEq6qpev11NGQXe+JBLBJUCU4DhW8Dq92P3SzqTnb/g96Z7hG/hmH7W3Cs3N+YEtiwHsPoGzhRFVKfnD4M6zHMx4gSW0OLlhDCx44TbvFqhPGH55f4VHWI18swce74kdaH5diJ9OlIS0njmuHXsLfGOeo/M3Cm3TfphIwBAzhefmIbTS0oICUvz8eITCPPEpSIXKaqfxWRaa19b7WTdyO9RzXvT8+GArvG3xkiQv9ca7+zKwQ++1k0FKJ++w5SCwvJnXiJJfw44eUZ1KXAX4EprXyngCWo7qLoU/ztUAHDc49D0SgYNAHSMvyOyhgAJCODvMu69K0X00U8S1CqOsf9m1BvZB49FuS9XUeoC4YYfWYBA8/I8TukpLClOoct1TncPfoav0MxxiQILy/xffdU38djG0/1DWF+u2onNXXOEzwbP6ni2uKB9O9hT0idlnAYQvWQnuV3JAlh85HNbK/cTo/MHozrPY6MVDvLNN2bl5f4Eu4u447DNU3JCUAVNu6rtAR1Og5tgU2vQl019BhIdmqI46FUv6OKW2WHylix+8Tj93ur9zJ1+FQfIzLGf15e4rtXRFKBO1T1Ya+W05WyM07egbY2zLQj1AAbF0PQrXKxYhcXFFbxt0M9fA0rnm0s39isf0/1HirrK612CNOtefoelKqGgG94uYyuNKAwh5F9Tpz4nRHIYPzAHv4FlKjqKk8kJ9cZGQ0+BZMYstKaXwZNkRQyUuwSn+neYlHV0Vsi8gjwW6Dp7TdVfS8Gy+6wyef2o7iykLpgmAGF2aSk2OOmHZZdCNk94HhF06A9x21neyoX9rmQT2o+oT5UD8AFfS44KWkZ093EIkGNd//+OGKY4jQDH5f65NuOoVNEYMzXYMuf4Xg59BzBBxWv+R1VXOsT6MNN59zE3uq99MjsQWFWod8hGeO7WNQk8Xmvl2HiUG5vGH/i6m5If+JjMIkhMzWTIQVW8YoxjWJxBoWITMZpVbfp1ERVf9z2FCAiTwBXAQdUdYy3ERpjjIk3nlcWKyKPAtfjNP0uwLVANG0uzAeu8C4yExeOH4HKfc4z/cYYEyEWZ1CfU9VzReRD99Hzh4BX25tIVVeIyGDvwzO+2fwn2LPaSU65RTDuG5BhNXcYYxyxaG7juPv3mIicCQSBfl0xYxG5TURWiciqgy2qzPdSKKy88/FhfrtyJ3/ZsJ9j9fYIdYdV7Yfdq06cOVUfgD3WCosx5oRYJKiXRaQH8CDwHrAdWNgVM1bVeaparKrFvXv37opZRuWtLYd4d+th9lbU8uHuo7z84b6YLTtp1FWePKy2lWHGmG4rFk/xzXU7F4nIy0CWqh71erle2nKguln/niPHOV4fslonOqLHIOdyXv2xE8OKPuVfPMaYuOPZGZSI/CCi+1oAVa1T1aMiktDPHBdkpzfrz8lIJSMtFiejSSQtA8bfAH3HQM9hMPoa568xxri83Kt+PaJ7Vovv2n06T0QWAu8AZ4vIbhGZ0ZXBdcYlI3uRl+WcfGakpfD5UUWkWo0THRfoBZ+aAude57QTZYwxEby8xCdtdLfWfxJVjds6/Irysrh1whAO1dRRkJ1OZppd2jPGmK7mZYLSNrpb6084KSlCUZ5ViWSMMV7xMkGNE5FKnLOlbLcbt9/27MYYY07Jy/ag7LqXMcaY02aPnhljjIlLlqBMp5WWliIip/wsX76c5cuXtzueiFBaWur3Khlj4oBoklTSWVxcrKtWWVU5xhiTaERktaoWtxxuZ1DGGGPikiUoY4wxcckSlDHGmLhkCaqFG2+8kX79+pGfn8/IkSN57LHH2hz34Ycfpm/fvuTn53PrrbdSV1fX9N0999zD2LFjSUtL61Y3/R955BGKi4vJzMzk5ptvbnO8mTNnkpub2/TJzMwkLy+v6ftJkyaRlZXV9P3ZZ58dg+jjQ7Tb4Pz580lNTW1WjsuWLWv6vrtugxB9GZaVlfGlL32JXr16IdK8gpu6ujpmzJjBoEGDyMvLY/z48bz6artN2SWFaMuvvd9xeXk511xzDYFAgEGDBvHcc891LBBVTYrPBRdcoF2hrKxMa2trVVV1w4YN2qdPH121atVJ47322mtaVFSkZWVlWl5erpdeeqneddddTd/Pnz9flyxZoldffbXOmTOnS2JLBIsWLdIXX3xRZ86cqSUlJVFPV1JSorfccktT/6WXXqq/+c1vPIgw/kW7DT755JM6YcKENufTXbdB1ejLcOPGjfrYY4/pH/7wB3V2hydUV1frnDlzdNu2bRoKhXTx4sWam5ur27Zti8Uq+Cra8mup5e/461//ul533XVaVVWlb775pubn52tZWdlJ0wGrtJX9up1BtTB69GgyMzMBmh57/vjjj08a76mnnmLGjBmMHj2awsJC7rnnHubPn9/0fUlJCV/+8pebHU10B9OmTeMrX/kKPXv2jHqampoaFi1aRElJiYeRJY5ot8H2dNdtEKIvw7PPPrvpd9xSIBCgtLSUwYMHk5KSwlVXXcWQIUNYvXq15/H77XS2wZa/48b+uXPnkpuby8UXX8zVV1/NM888E3UclqBacfvtt5OTk8OoUaPo168fV1555UnjrFu3jnHjxjX1jxs3jv3793P48OFYhpoUFi1aRO/evZk4cWKz4bNmzaJXr15MmDCh2aWr7iCabRDg/fffp1evXowcOZK5c+fS0GCtOzeKtgyjtX//fjZt2tRqMktGHS2/lr/jTZs2kZaWxsiRI5vGGTduHOvWrYs6BktQrfjVr35FVVUVb775JtOmTWs6kohUXV1NQUFBU39jd1VVVcziTBZPPfUU06dPb3YP4IEHHmDr1q3s2bOH2267jSlTppzWWUSiimYbnDhxImVlZRw4cIBFixaxcOFCHnzwQR+ijU/RlGG0gsEgN9xwAyUlJYwa1T2ahulo+bX8HVdXV5Ofn99snIKCgg7tIy1BtSE1NZWLL76Y3bt38+tf//qk73Nzc6msPNFEeWN3d7yc0hk7d+5k2bJlTJ8+vdnwiy66iLy8PDIzMykpKWHChAksWbLEpyj90d42OHToUIYMGUJKSgpjx45l9uzZ/O53v/Mh0vjVXhlGIxwOc9NNN5GRkcEjjzzSxRHGt2jLr7Xfcct9JDj7yY7sIy1BtaOhoaHVI/fRo0ezZs2apv41a9bQp0+fDt17MfDMM88wYcIEhg4desrxRARNklpPOqqtbbCl7lxG7Ym2DFtSVWbMmMH+/ftZtGgR6enp7U+UhNorv9Z+xyNHjqShoYHNmzc3DVuzZk2HLpFagopw4MABnn/+eaqrqwmFQixdupSFCxdy+eWXnzTu9OnTefzxx1m/fj0VFRXcd999zR6rDgaD1NbWEg6HaWhooLa2llAoFMO18UfkuoZCIWpra095X+Tpp58+6XH0iooKli5d2jTtggULWLFiBVdc0W5DzAmvI9vgq6++yv79+wHYuHEjc+fOZerUqU3fd9dtsCNlqKrU1tZSX18PQG1tbbPXRb71rW+xYcMGFi9eTHZ2dszWwU8dKb9Grf2OA4EA06ZNY/bs2dTU1PDWW2/x0ksvcdNNN0UfTGuP9iXipyseMz9w4IBOnDhRCwoKNC8vT8eMGaPz5s1TVdUdO3ZoIBDQHTt2NI3/0EMPaVFRkebl5enNN9/c9FimqvO4JU7DjE2fJ598stMxxrs5c+actN5z5sxptfzefvttzcnJ0crKymbzOHDggBYXF2tubq4WFBToRRddpK+//nqsV8UXHdkG77zzTi0qKtKcnBwdMmSI3nPPPVpfX980r+66DXakDLdt23ZSGQ0aNEhVVbdv366AZmZmaiAQaPo8++yzfq1aTHR0P9jW71hV9fDhwzp16lTNycnRgQMH6oIFC1pdJm08Zm6VxRpjjPGVVRZrjDEmocR1ghKRK0TkIxHZIiJ3+x2PMcaY2InbBCUiqcAvgS8D5wDfEJFz/I3KGGNMrMRtggI+DWxR1a2qWg88D0xtZxpjjDFJIp4TVH9gV0T/bndYExG5TURWiciqgwcPxjQ4Y4wx3ornBNUuVZ2nqsWqWty7d2+/wzHGGNOF4jlB7QEGRvQPcIcZY4zpBuI5Qa0ERojIEBHJAL4O/NHnmIwxxsRImt8BtEVVG0Tk34GlQCrwhKpGX0+7McaYhBa3CQpAVZcA3asKa2OMMQDJU9WRiBwEdni4iF7AIQ/n3x1YGXaOlV/nWRl2jlflN0hVT3rSLWkSlNdEZFVrdUWZ6FkZdo6VX+dZGXZOrMsvnh+SMMYY041ZgjLGGBOXLEFFb57fASQBK8POsfLrPCvDzolp+dk9KGOMMXHJzqCMMcbEJUtQxhhj4pIlKEBEfiwiX+iC+cwUkeldEVMyE5FlIhL1o6oiMklEXvYyJj+IyB0iskFEatpq60xEBorIGyKyXkTWich/RHz3gIh8KCJPRwy7UUS+E4PwfRFNmbnjPSEiB0SkrMXwM0TkTyKy2f1b6A7/qlu+b4pIT3fYMBH5rbdrFBsi8nYU43xHRHJiEEvUv2dLUICqzlbVP7cc7jaa2JH5PKqqT7c/pjEA3A58EXgBp1HO1jQAd6rqOcBngH8TkXNEpAA4X1XPBepFZKyIZAO34DT0mayiKTOA+cAVrQy/G/iLqo4A/uL2A3wbuBD4H+Cb7rD7gB91PmT/qernohjtO0CHElRH95EdlZQJSkQGi8hGEVngHm39TkRyRGS2iKwUkTIRmSci4o4/X0S+5nZvd49M3wOujZhnkYisdrvHiYiKyFlu/8fu/EtF5HvusDvco94PReR5d1jAPbL7h4i8LyIJ1QDjKcr1cnd91rrrl+mO3+rwFvP8JxF5R0TeE5EXRCTXHX6Fu6z3gGkxXlXPicijwFBgG1ACPCgiH4jIsMjxVHWfqr7ndlcBG3DaRQsD6e42nAMEge8Bv1DVYOzWJHaiLTMAVV0BlLcym6nAU273U8BX3O4wkIlbliJyCfCJqm7u0pXwiYhUu38nuVcwfhfxWxYRuQM4E3hDRN5wx23rt9lsH+n23+uOt1ZERrnjfdqd/n0ReVtEzu5w4KqadB9gMKDABLf/CZwf7xkR4zwDTHG75wNfc7u3Az9oY77rgHzg33FqW78BGAS8435fCnzP7d4LZLrdPdy/PwFubBwGbAICfpdXJ8v1RzgNS450hz2NcySW1dpwt3sZUIxTbcqKxjIA7gJmR0w7AhDgf4GX/V5/D8pzu1sGTdtfFOW/E8h3+38AfAA8BPRLxjLqTJm55VXWYlhFRLc09uOcla0GFgMFwOuR+4tE/wDV7t9JwFGc5otSgHeAiyPL1u1u9bcZMd4PIua9Hfi223078JjbnQ+kud1fABZFxBDVtpqUZ1CuXar6ltv9LHAx8HkR+buIrAUuA0a3MW1b153fBiYAE3GSzUTgEuDNVsb9EFggIjfiXKYB+CfgbhH5AGcnnQWc1YF1igcty/VyYJuqbnKHPYVTLme3MTzSZ3Au07zllkkJTsIf5U67WZ0t+lmvViZRuEevi3CSfCWAqv5MVcer6p3AXGC2iPyziPyviCTFpSkvuduWut1/UtULVHUKzlnWEmCke6bxm1jcm4mhf6jqblUN4xzgDG5lnLZ+m41a7iN/7/5dHTG/AuAFce4DPkzb+9s2JXOCavmClwK/wjnqGgv8BidBtKYGQESedC8hNNaovgInIQ0CXgLG4SS+1hLUZJx7AecDK0UkDeeI7avuTmW8qp6lqhtOew390bJcKzoxLwH+FFEe56jqjE7ML+GJ81DEB+5npjssHSc5LVDV37cyzXk4ZfkRcK2qXgcME5ERsYzdL62V2SnsF5F+7nT9gAMt5pUD3Izz270XZ8f8N5yrJcmiLqI7ROutWrT326xpY56R85sLvKGqY4AptL2/bVMyJ6izROSzbvc3cTYygEPu0ejX2puBqt7i/nOudAe9CdwIbHaPPsqBKyPmDYCIpAADVfUNnFPjAiAXp22rb0fc+zqvMyvok5blugoYLCLD3WE3ActxdpatDY/0LjChcRz3Ht1IYKM7beO9hW94sypxowrIA1DVXRE7hUfdbeVxYIOq/ryN6ecC9wDpOG2ngXNPJZmO+ltqs8zame6POEkH9+9LLb7/PvD/1LmPl41zQJbsZdmoqUxp+7fZEQWcaAX95tMJKJkT1Ec4TzxtAAqBX+OcNZXhJIqVHZ2hqm7HObJY4Q76G8417CMtRk0FnnUvJb6Ps8FX4OxI0oEPRWSd259oWpbrwzhPjr3grm8YeFRVa1sbHjkjVT2Is+EuFJEPca6Hj3KnvQ14xb0R2+woNwk9D3zfvZnc8ob/BJzkflnEWULjARMi8hVglarudbexD9zyzlLVNTGK3w+nKjNEZCHO9nS2iOwWkcaj//uBL4rIZpz7IvdHTHMm8GlV/YM76Bc4+4mZwHOerUn8mAe8JiJvtPXb7OD8fgb8VETe5zTbHkzKqo5EZDDOTbgxfseSTKxcjTGxlMxnUMYYYxJYUp5BGWOMSXx2BmWMMSYuWYIyxhgTlyxBGWOMiUuWoIzxkYj0EJHbT3Pa7SLSq6tjMiZeWIIyxl89cOovM8a0YAnKGH/dj1Mt0Qci8rCI/CWiVuip0PQW/ysiskacmvivj5yBiGSLyKsi8i++rIExHjmtt3uNMV3mbmCMqo5362vMUdVK99LduyLyR5x2jfaq6mQAcdqCapSLU6vC02ptkZkkY2dQxsQPAX7iVi3zZ5x2n/oAa3Gq53lARC5R1aMR07wEPGnJySQjS1DGxI8bgN7ABao6HtiPU6feJpxa8dcC94nI7Ihp3gKuaKyA2JhkYgnKGH9F1iBdABxQ1aCIfB63/R23EtNjqvos8CBOsmo0GzhCcjfzbropS1DG+EhVD+M0ClcGjAeK3drIp+M0OwIwFviH23DcHOC+FrP5DyBbRH4Wk6CNiRGri88YY0xcsjMoY4wxcckSlDHGmLhkCcoYY0xcsgRljDEmLlmCMsYYE5csQRljjIlLlqCMMcbEpf8PBe4KsqH3HQ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.figure(figsize=(10,12))\n",
    "plt.figure(figsize=(6,10))\n",
    "ax1 = plt.subplot(311)\n",
    "n_steps = 5 # 8\n",
    "x_pos = np.arange(n_steps) #7\n",
    "#aucs = [auc_pw_mean, auc_po_mean, auc_ft2_mean, auc_base2_mean, auc_ft10_mean, auc_base10_mean, auc_int_mean]\n",
    "#stds = [auc_pw_std, auc_po_std, auc_ft2_std, auc_base2_std, auc_ft10_std, auc_base10_std, auc_int_std]\n",
    "#names = ['pair-wise', 'pooled', 'ft-2%', 'int-2%', 'ft-10%', 'ft-10%-pooled', 'int-10%', 'internal']\n",
    "names = ['pair-wise', 'pooled', 'ft-2%', 'ft-10%', 'internal']\n",
    "ax1.errorbar(x_pos, aucs_m, yerr=aucs_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "sns.stripplot(x=\"task\", y=\"auc_mean\", data=df_tot2, jitter=0.1, ax=ax1,alpha=0.5)\n",
    "\n",
    "ax1.set_ylim(0.60,0.93)\n",
    "\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(names)\n",
    "#ax[0].tight_layout()\n",
    "ax1.set_ylabel(f'AUC')\n",
    "\n",
    "ax2 = plt.subplot(312, sharex= ax1)\n",
    "\n",
    "#plt.plot((2,3), (auc_ft2_mean, auc_base2_mean), '--', color='black')\n",
    "#plt.plot((4,5), (auc_ft10_mean, auc_base10_mean), '--', color='black')\n",
    "#plt.hlines(auc_int_mean, 0,6, color='black', linestyle='dotted')\n",
    "sns.stripplot(x=\"task\", y=\"precision_mean\", data=df_tot2, jitter=0.1, ax=ax2, alpha=0.5)\n",
    "ax2.errorbar(x_pos, precision_m, yerr=precision_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "ax2.set_ylabel(f'PPV @ 80% Sensitivity')\n",
    "ax2.set_ylim(0.15,0.55)\n",
    "\n",
    "#ax[1].set_xticklabels(names)\n",
    "#ax[1].tight_layout()\n",
    "\n",
    "ax3 = plt.subplot(313, sharex= ax1)\n",
    "sns.stripplot(x=\"task\", y=\"earliness_mean\", data=df_tot2, jitter=0.1, ax=ax3, alpha=0.5)\n",
    "ax3.errorbar(x_pos, earliness_m, yerr=earliness_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "ax3.set_ylabel(f'Earliness @ 80% Sensitivity')\n",
    "#ax[2].set_xticklabels(names)\n",
    "ax3.set_ylim(-0.2,6.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "x1 = np.arange(n_steps)\n",
    "y1 = np.array([0.13] * n_steps)\n",
    "for i,j, n in zip(x1,y1, aucs_m):\n",
    "    ax1.annotate(f'{n:.2f}',xy=(i-0.1,j+0.5), size=12)\n",
    "y2 = np.array([0.18] * n_steps)\n",
    "\n",
    "for i,j, n in zip(x1,y2, precision_m):\n",
    "    ax2.annotate(f'{n:.2f}',xy=(i-0.1,j), size=12)\n",
    "\n",
    "y3 = np.array([0.2] * n_steps)\n",
    "for i,j, n in zip(x1,y3, earliness_m):\n",
    "    ax3.annotate(f'{n:.2f}',xy=(i-0.1,j), size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b399fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1243ef9a0>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys_sorted = ['ft10', 'int10', 'int']\n",
    "aucs_m = [auc_means[k] for k in keys_sorted]\n",
    "aucs_s = [auc_stds[k] for k in keys_sorted]\n",
    "earliness_m = [earliness_means[k] for k in keys_sorted]\n",
    "earliness_s = [earliness_stds[k] for k in keys_sorted]\n",
    "precision_m = [precision_means[k] for k in keys_sorted]\n",
    "precision_s = [precision_stds[k] for k in keys_sorted]\n",
    "\n",
    "df_tot3 = pd.concat([df_ft10, df_base10, df_int])\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "ax1 = plt.subplot(311)\n",
    "n_steps = 3\n",
    "x_pos = np.arange(n_steps) #7\n",
    "#aucs = [auc_pw_mean, auc_po_mean, auc_ft2_mean, auc_base2_mean, auc_ft10_mean, auc_base10_mean, auc_int_mean]\n",
    "#stds = [auc_pw_std, auc_po_std, auc_ft2_std, auc_base2_std, auc_ft10_std, auc_base10_std, auc_int_std]\n",
    "names = ['ft-10%', 'int-10%', 'internal-100%']\n",
    "ax1.errorbar(x_pos, aucs_m, yerr=aucs_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "sns.stripplot(x=\"task\", y=\"auc_mean\", data=df_tot3, jitter=0.1, ax=ax1,alpha=0.5)\n",
    "\n",
    "ax1.set_ylim(0.70,0.93)\n",
    "\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(names)\n",
    "#ax[0].tight_layout()\n",
    "ax1.set_ylabel(f'AUROC')\n",
    "\n",
    "# ax2 = plt.subplot(312, sharex= ax1)\n",
    "\n",
    "# #plt.plot((2,3), (auc_ft2_mean, auc_base2_mean), '--', color='black')\n",
    "# #plt.plot((4,5), (auc_ft10_mean, auc_base10_mean), '--', color='black')\n",
    "# #plt.hlines(auc_int_mean, 0,6, color='black', linestyle='dotted')\n",
    "# sns.stripplot(x=\"task\", y=\"precision_mean\", data=df_tot2, jitter=0.1, ax=ax2, alpha=0.5)\n",
    "# ax2.errorbar(x_pos, precision_m, yerr=precision_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "# ax2.set_ylabel(f'PPV @ 80% Sensitivity')\n",
    "# ax2.set_ylim(0.15,0.55)\n",
    "\n",
    "# #ax[1].set_xticklabels(names)\n",
    "# #ax[1].tight_layout()\n",
    "\n",
    "# ax3 = plt.subplot(313, sharex= ax1)\n",
    "# sns.stripplot(x=\"task\", y=\"earliness_mean\", data=df_tot2, jitter=0.1, ax=ax3, alpha=0.5)\n",
    "# ax3.errorbar(x_pos, earliness_m, yerr=earliness_s, ecolor='black', capsize=10, fmt='d', color='black')\n",
    "# ax3.set_ylabel(f'Earliness @ 80% Sensitivity')\n",
    "# #ax[2].set_xticklabels(names)\n",
    "# ax3.set_ylim(-0.2,6.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "x1 = np.arange(n_steps)\n",
    "y1 = np.array([0.13] * n_steps)\n",
    "for i,j, n in zip(x1,y1, aucs_m):\n",
    "    ax1.annotate(f'{n:.2f}',xy=(i-0.1,j+0.58), size=12)\n",
    "y2 = np.array([0.18] * n_steps)\n",
    "\n",
    "# for i,j, n in zip(x1,y2, precision_m):\n",
    "#     ax2.annotate(f'{n:.2f}',xy=(i-0.1,j), size=12)\n",
    "\n",
    "# y3 = np.array([0.2] * n_steps)\n",
    "# for i,j, n in zip(x1,y3, earliness_m):\n",
    "#     ax3.annotate(f'{n:.2f}',xy=(i-0.1,j), size=12)\n",
    "    \n",
    "plt.plot((0,1), (auc_ft10_mean, auc_base10_mean), '--', color='black')\n",
    "#plt.plot((4,5), (auc_ft10_mean, auc_base10_mean), '--', color='black')\n",
    "#plt.hlines(auc_int_mean, 0,2, color='black', linestyle='dotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdde590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
